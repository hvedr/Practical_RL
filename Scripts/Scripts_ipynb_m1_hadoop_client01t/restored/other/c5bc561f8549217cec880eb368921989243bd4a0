{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Построение Look-alike модели для целевой аудитории раздела веб-сайта\n",
    "\n",
    "Используются различные подходы:\n",
    "- Логистическая регрессия\n",
    "- Naive Bayes\n",
    "- текущий подход, рассмотренный в Wiki[https://wiki.tcsbank.ru/pages/viewpage.action?pageId=176096365].\n",
    "\n",
    "Сравнение методов производится по метрике AUC ROC.\n",
    "\n",
    "** Модификация - все через Hive, в Spark только само обучение и вывод результатов. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "start = datetime.datetime.now()\n",
    "target_urls =['raiffeisen.ru/retail/cards/credit']\n",
    "exclude_urls = target_urls + ['raiffeisen.ru']\n",
    "\n",
    "source_table_name = 'user_kposminin.access_log_sample4'\n",
    "train_start_date, train_end_date = '2016-06-14', '2016-06-14'\n",
    "test_date = '2016-06-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyhs2\n",
    "\n",
    "user = 'kposminin'\n",
    "\n",
    "connection = pyhs2.connect(host='ds-hadoop-cs01p.tcsbank.ru',\n",
    "                           port=10000,\n",
    "                           authMechanism='PLAIN',\n",
    "                           user=user,\n",
    "                           password='')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, '2016-08-18', 'Calculation name', 'Additional description. id must be unique but it is not automatically guaranteed', 'Calc params in format var1:value, var2:value ...']]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SELECT * FROM  user_kposminin.calcs')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hive queries\n",
    "\n",
    "#based on urlp column\n",
    "\n",
    "calc_id = 18 #int(hc.sql('select max(id) from user_kposminin.calcs').collect()[0][0]) + 1\n",
    "\n",
    "target_expression = '('+'or'.join(' url like \"%' + u + '%\" ' for u in target_urls)+')'\n",
    "exclude_expression = 'not ('+'or'.join(' urlp like \"%' + u + '%\" ' for u in exclude_urls)+')'\n",
    "\n",
    "update_calcs_table_query = '''\n",
    "insert into user_kposminin.calcs values(\n",
    "    {calc_id},\n",
    "    {date},\n",
    "    \"Look alike model comparison\",\"Comparison of NaiveBayes, LogisticRegression and Current approach\",\n",
    "    \"train_start_date: {train_start_date}, train_end_date:train_end_date,\n",
    "        test_date: {test_date}, source_table_name: {source_table_name},\n",
    "        target_urls: {target_urls}, exclude_urls: {exclude_urls}\"\n",
    "    )\n",
    "'''.format(\n",
    "    calc_id = calc_id, \n",
    "    train_start_date = train_start_date, \n",
    "    train_end_date = train_end_date, \n",
    "    test_date = test_date,\n",
    "    source_table_name = source_table_name,\n",
    "    target_urls = str(target_urls),    \n",
    "    exclude_urls = str(exclude_urls),\n",
    "    date = str(datetime.datetime.now().date())\n",
    ")\n",
    "\n",
    "\n",
    "create_tables_in_hive_query = '''\n",
    "drop table if exists user_kposminin.urls_w_levels_train{calc_id};\n",
    "\n",
    "create table user_kposminin.urls_w_levels_train{calc_id} as\n",
    "select\n",
    "    a.id as cookie\n",
    "    ,concat(a.id, \"-\", a.ymd) as object_id\n",
    "    ,a.ymd\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
    "    ,a.url\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)', 1) as ref_domain\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) ref_lev0\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) ref_lev1\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) ref_lev2\n",
    "    ,a.referrer\n",
    "    ,a.timestamp\n",
    "    from \n",
    "    (\n",
    "       select b.*\n",
    "        from\n",
    "           (select\n",
    "              al.*,\n",
    "              count(*) over (partition by url) as url_count\n",
    "           from {source_table_name} al\n",
    "              where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "           ) b\n",
    "       where b.url_count > 50\n",
    "    ) a\n",
    ";\n",
    "\n",
    "drop table if exists user_kposminin.urls_w_levels_test{calc_id} ;\n",
    "\n",
    "create table user_kposminin.urls_w_levels_test{calc_id} as\n",
    "select\n",
    "    a.id as cookie\n",
    "    ,concat(a.id, \"-\", a.ymd) as object_id\n",
    "    ,a.ymd\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
    "    ,a.url\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)', 1) as ref_domain\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) ref_lev0\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) ref_lev1\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) ref_lev2\n",
    "    ,a.referrer\n",
    "    ,a.timestamp\n",
    "    from \n",
    "    (\n",
    "       select b.*\n",
    "        from\n",
    "           (select\n",
    "              al.*,\n",
    "              count(*) over (partition by url) as url_count\n",
    "           from {source_table_name} al\n",
    "              where ymd = \"{test_date}\"\n",
    "           ) b\n",
    "       where b.url_count > 50\n",
    "    ) a\n",
    ";\n",
    "\n",
    "drop table if exists user_kposminin.user_urlp_train{calc_id};\n",
    "\n",
    "create table user_kposminin.user_urlp_train{calc_id} as \n",
    "   select \n",
    "        concat(cookie, 0) as cookie,\n",
    "        domain as urlp    \n",
    "    from user_kposminin.urls_w_levels_train{calc_id}\n",
    "    union all\n",
    "    select\n",
    "        concat(cookie, 0) as cookie,\n",
    "        concat(domain,'[0]',lev0) as urlp\n",
    "     from user_kposminin.urls_w_levels_train{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 0) as cookie,\n",
    "        concat(domain,'[1]',lev1) as urlp\n",
    "    from user_kposminin.urls_w_levels_train{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 0) as cookie,\n",
    "        concat(domain,'[2]',lev2) as urlp\n",
    "    from user_kposminin.urls_w_levels_train{calc_id}\n",
    ";\n",
    "\n",
    "drop table if exists user_kposminin.user_urlp_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_urlp_test{calc_id} as \n",
    "   select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        domain as urlp    \n",
    "    from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select\n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[0]',lev0) as urlp\n",
    "     from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[1]',lev1) as urlp\n",
    "    from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[2]',lev2) as urlp\n",
    "    from user_kposminin.urls_w_levels_test{calc_id}   \n",
    ";\n",
    "\n",
    "drop table if exists user_kposminin.user_train{calc_id};\n",
    "\n",
    "create table user_kposminin.user_train{calc_id} as\n",
    "select\n",
    "   concat(id, 0)  as cookie,\n",
    "   max(case when {target_expression} then 1 else 0 end) as label\n",
    "from \n",
    "   {source_table_name} al\n",
    "   where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "group by id;\n",
    "\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.user_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_test{calc_id} as\n",
    "select\n",
    "   concat(id, 1) as cookie,\n",
    "   max(case when {target_expression} then 1 else 0 end) as label\n",
    "from \n",
    "   {source_table_name} al\n",
    "   where ymd = \"{test_date}\"\n",
    "group by id;\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.urlp_score_train{calc_id};\n",
    "\n",
    "create table user_kposminin.urlp_score_train{calc_id} as\n",
    "select\n",
    "    urlp,\n",
    "    log((positives + 0.5) / (total - positives + 0.5)) as score\n",
    "from\n",
    "    (select\n",
    "        urlp,\n",
    "        sum(label) as positives,\n",
    "        count(cookie) as total\n",
    "    from\n",
    "        (select distinct\n",
    "            a.urlp,\n",
    "            a.cookie,\n",
    "            b.label\n",
    "        from \n",
    "            (select * \n",
    "             from user_kposminin.user_urlp_train{calc_id}\n",
    "             where {exclude_expression}\n",
    "            ) a\n",
    "        left join user_kposminin.user_train{calc_id} b \n",
    "        on a.cookie = b.cookie\n",
    "        ) c\n",
    "    group by urlp\n",
    "    ) d\n",
    "where\n",
    "    total > 20\n",
    "    or positives > 1\n",
    ";\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.user_score_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_score_test{calc_id} as\n",
    "select\n",
    "    cs.cookie,\n",
    "    cs.score,\n",
    "    i.label\n",
    "from\n",
    "    (select \n",
    "        u.cookie,\n",
    "        max(s.score) as score\n",
    "    from \n",
    "        user_kposminin.user_urlp_test{calc_id} u\n",
    "    join user_kposminin.urlp_score_train{calc_id} s\n",
    "    on u.urlp = s.urlp\n",
    "    group by u.cookie) cs\n",
    "join user_kposminin.user_test{calc_id} i\n",
    "on i.cookie = cs.cookie\n",
    "'''.format(\n",
    "    calc_id = calc_id, \n",
    "    train_start_date = train_start_date, \n",
    "    train_end_date = train_end_date, \n",
    "    test_date = test_date,\n",
    "    source_table_name = source_table_name,\n",
    "    target_expression = target_expression,\n",
    "    exclude_expression = exclude_expression\n",
    ")\n",
    "\n",
    "train_labeledpoint_query = '''\n",
    "select\n",
    "    u.label,\n",
    "    cu.url_list\n",
    "from\n",
    "   (select\n",
    "      cookie,\n",
    "      collect_list(urlp) as url_list\n",
    "   from \n",
    "      user_kposminin.user_urlp_train{calc_id}\n",
    "   where {exclude_expression}\n",
    "   group by cookie) cu\n",
    "join user_kposminin.user_train{calc_id} u\n",
    "on cu.cookie = u.cookie\n",
    "'''.format(calc_id = calc_id, exclude_expression = exclude_expression)\n",
    "\n",
    "test_labeledpoint_query = '''\n",
    "select\n",
    "    u.label,\n",
    "    cu.url_list\n",
    "from\n",
    "   (select\n",
    "      cookie,\n",
    "      collect_list(urlp) as url_list\n",
    "   from \n",
    "      user_kposminin.user_urlp_test{calc_id}\n",
    "   where {exclude_expression}\n",
    "   group by cookie) cu\n",
    "join user_kposminin.user_test{calc_id} u\n",
    "on cu.cookie = u.cookie\n",
    "'''.format(calc_id = calc_id, exclude_expression = exclude_expression)\n",
    "\n",
    "current_approach_results_query = '''\n",
    "select\n",
    "    score,\n",
    "    label\n",
    "from \n",
    "    user_kposminin.user_score_test{calc_id}\n",
    "'''.format(calc_id = calc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyhs2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e69d460b9267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyhs2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#app_name = 'healthcare_visits'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#mode = '\"PHONE\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'kposminin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyhs2"
     ]
    }
   ],
   "source": [
    "import pyhs2\n",
    "\n",
    "user = 'kposminin'\n",
    "\n",
    "connection = pyhs2.connect(host= 'ds-hadoop-cs01p.tcsbank.ru',\n",
    "                           port= 10000,\n",
    "                           authMechanism= 'PLAIN',\n",
    "                           user= user,\n",
    "                           password= '')\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('select * from user_kposminin.calcs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: date, name: string, description: string, params: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make calculations and create tables in Hive\n",
    "\n",
    "#hc.sql(update_calcs_table_query)\n",
    "#for q in create_tables_in_hive_query.split(';'):\n",
    "#    print(create_tables_in_hive_query.split(';').index(q))\n",
    "#    hc.sql(q)\n",
    "#print('\\nHive operations successfully completed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Table not found: `user_kposminin`.`user_urlp_train18`; line 9 pos 21'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4fa5b83aa45a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#transform urls (as Bag of Words) into features and form features with labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labeledpoint_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \"\"\"\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u'Table not found: `user_kposminin`.`user_urlp_train18`; line 9 pos 21'"
     ]
    }
   ],
   "source": [
    "#Load train and test data to Spark\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "\n",
    "tf = HashingTF(numFeatures = 10 ** 5)\n",
    "\n",
    "#transform urls (as Bag of Words) into features and form features with labels\n",
    "train_data = hc.sql(train_labeledpoint_query).rdd.map(lambda r: LabeledPoint(r.label,tf.transform(r.url_list)))\n",
    "train_data.cache()\n",
    "\n",
    "test_data = hc.sql(test_labeledpoint_query).rdd.map(lambda r: LabeledPoint(r.label,tf.transform(r.url_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train NaiveBayes model\n",
    "\n",
    "modelNB = NaiveBayes.train(train_data)\n",
    "\n",
    "def predict_proba_NB(f,model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability. f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Function selects winning class with it probability.\n",
    "    Output: tuple with model selected class number as first element (type int) and it probability as second (type float).\n",
    "    '''\n",
    "    logp = [[i,f.dot(model.theta[i]) + model.pi[i]] for i in range(len(model.theta))] # classes with log probabilities\n",
    "    wi = sorted(logp, key = lambda e:  - e[1])[0][0] #winning index\n",
    "    prob = 1./sum([np.exp(e[1] - logp[wi][1]) for e in logp]) #winning class probability\n",
    "    return wi, prob\n",
    "\n",
    "def predict_proba_NB_2(f, model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability for 2-class classification.\n",
    "    f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Output: probability of class 1 (type float).\n",
    "    '''\n",
    "    if len(model.theta) != 2:\n",
    "        print('Model is NOT a 2-class classifier')\n",
    "        return None\n",
    "    logp = [f.dot(model.theta[i]) + model.pi[i] for i in range(2)]    \n",
    "    return 1./(1. + np.exp(logp[0] - logp[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LogisticRegression model\n",
    "\n",
    "modelLR = LogisticRegressionWithSGD.train(train_data)\n",
    "modelLR.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Current approach (results only). All calculations in Hive\n",
    "\n",
    "ca_res = hc.sql(current_approach_results_query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing result\n",
    "\n",
    "df_test = test_data.map( lambda lp: pyspark.sql.Row(\n",
    "        Label = lp.label,\n",
    "        NaiveBayes = float(predict_proba_NB_2(lp.features, modelNB)),\n",
    "        LogisticRegression = float(modelLR.predict(lp.features))\n",
    "    )).toDF().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build AUCROC metric and print results\n",
    "\n",
    "import sklearn\n",
    "AUCROC = {}\n",
    "for c in df_test.columns:\n",
    "    if c!= 'Label':\n",
    "        AUCROC[c] = sklearn.metrics.roc_auc_score(df_test['Label'],df_test[c])\n",
    "AUCROC['CurrApproach'] = sklearn.metrics.roc_auc_score(ca_res['label'], ca_res['score'])\n",
    "        \n",
    "print('Methods AUCROC performance on test sample ({0:.0f} samples with {1:.0f} positives):\\n'.format(\n",
    "        df_test.size,df_test['Label'].sum()) +'\\n'.join(['{0:<30}{1:.5f}'.format(k,v) for (k,v) in AUCROC.items()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Time of work {0}'.format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "! which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-plugin-ud.ipynb\r\n",
      "data\r\n",
      "Delivery_results_analysis.ipynb\r\n",
      "derby.log\r\n",
      "First_classification_excercise.ipynb\r\n",
      "First.ipynb\r\n",
      "home\r\n",
      "Look_alike_model_comparison_v2-Copy1.ipynb\r\n",
      "Look_alike_model_comparison_v2-Copy2.ipynb\r\n",
      "Look_alike_model_comparison_v2.ipynb\r\n",
      "nohup.out\r\n",
      "test.txt\r\n",
      "Untitled1.ipynb\r\n",
      "Untitled2.ipynb\r\n",
      "Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
