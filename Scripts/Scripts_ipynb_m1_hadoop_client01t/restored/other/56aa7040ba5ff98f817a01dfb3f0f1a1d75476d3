{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.setCheckpointDir('/user/kposminin/checkpointdir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def repart(filename):\n",
    "    starttime = datetime.datetime.now()\n",
    "    sc.textFile(filename).repartition(32*8).saveAsTextFile('.'.join(filename.split('.')[:-1]))\n",
    "    print('End. Time of work {0}.'.format(datetime.datetime.now() - starttime))\n",
    "#repart(\"/user/kposminin/la_app_20160817_1.txt\")\n",
    "#repart(\"/user/kposminin/la_app_20160818_1.txt\")\n",
    "#repart(\"/user/kposminin/la_app_20160824_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(table):\n",
    "    si = 15 #score start index\n",
    "    def top_avg_score(slist): \n",
    "        return [sum(slist[:i])/i for i in [2,3,4,5,7,10]]\n",
    "    return [r + top_avg_score(r[si:si+11]) for r in table] \n",
    "\n",
    "def add_feature_rdd(row):\n",
    "    si = 15 #score start index\n",
    "    def top_avg_score(slist): \n",
    "        return [sum(slist[:i])/i for i in [2,3,4,5,7,10]]\n",
    "    r = row\n",
    "    return r + top_avg_score(r[si:si+11])\n",
    "    \n",
    "    \n",
    "# Load and parse the data file.\n",
    "# Load and parse the data file.\n",
    "train = sc.textFile(\"/user/kposminin/la_20160817_3.txt\") \\\n",
    "  .filter(lambda s: (s[0] == '1') or (hash(s) % 1000 == 0)) \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \\\n",
    "  .collect()\n",
    "\n",
    "test = sc.textFile(\"/user/kposminin/la_20160824_3.txt\") \\\n",
    "  .filter(lambda s: (s[0] == '1') or (hash(s) % 1000 == 17)) \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \\\n",
    "  .collect()\n",
    "\n",
    "test_rdd = sc.textFile(\"/user/kposminin/la_20160824_3.txt\") \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \n",
    "\n",
    "test_rdd2 = sc.textFile(\"/user/kposminin/la_20160818_3.txt\") \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2 = sc.textFile(\"/user/kposminin/la_20160818_3.txt\") \\\n",
    "  .filter(lambda s: (s[0] == '1') or (hash(s) % 300 == 87)) \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = '''smax ,savg ,ssum ,smedian ,sstd ,cntrepeat ,cntuniq \n",
    ",duration , has_scores, mobile ,emailru ,vkru ,okru ,social_other , s1 ,s2 ,s3 ,s4 ,s5 ,s6 ,s7 ,s8 ,s9 ,s10 , \n",
    "sm1 ,sm2 ,sm3 ,sm4 ,sm5, avg2, avg3,avg4,avg5,avg7,avg10'''.replace(' ','').replace('\\n','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score  AUCROC on sampled test data 0.818371529014\n"
     ]
    }
   ],
   "source": [
    "aucroc_smax = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[1] for e in test]\n",
    "    )\n",
    "print('Max score  AUCROC on sampled test data {0}'.format(aucroc_smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test AUC ROC 0.843090200848\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(test_rdd.map(lambda r: (float(r[1]),float(r[0]))))\n",
    "print('Full test AUC ROC {0}'.format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test 20160818 AUC ROC 0.843819806579\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(test_rdd2.map(lambda r: (float(r[1]),float(r[0]))))\n",
    "print('Full test 20160818 AUC ROC {0}'.format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Варьируем размер семплирования  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5023 ['test on 5023-model', 0.87585529174876009]\n",
      "8845 ['test on 8845-model', 0.87230379889219778]\n",
      "16264 ['test on 16264-model', 0.8698999084238872]\n",
      "31226 ['test on 31226-model', 0.86535683842561495]\n",
      "76957 ['test on 76957-model', 0.87027077578850187]\n",
      "152483 ['test on 152483-model', 0.86609921153177005]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelGBT = {}\n",
    "AUCROC=[]\n",
    "\n",
    "for f in [40,20,10,5,2,1]:\n",
    "    train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*f) == 0]\n",
    "    s = len(train1)\n",
    "    modelGBT[s] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=2000, learning_rate=0.04,\n",
    "       max_depth=3, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append(['GBT test on {0}-model'.format(s),sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    #AUCROC.append(['train '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "    #    y_true = [e[0] for e in train1], \n",
    "    #    y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in train1])]\n",
    "    #)])    \n",
    "    print('{0} {1}'.format(s,AUCROC[-1]))\n",
    "\n",
    "AUCROC.append(['smax '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[1] for e in test]\n",
    ")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем меньше объем выборки, тем лучше обучение. Достигается улучшение по сравнению с текущим подходом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5048 ['test on 5048-model', 0.87488486723995595]\n",
      "8853 ['test on 8853-model', 0.87578267300647705]\n",
      "16479 ['test on 16479-model', 0.85510116496516331]\n"
     ]
    }
   ],
   "source": [
    "modelRF = {}\n",
    "\n",
    "\n",
    "for f in [40,20,10,2]:\n",
    "    train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*f) == 0]\n",
    "    s = len(train1)\n",
    "    modelRF[s] = sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, \n",
    "                min_samples_split=20, min_samples_leaf=10, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "                warm_start=False, class_weight='auto') \\\n",
    "    .fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    \n",
    "    AUCROC.append(['RF test on {0}-model'.format(s),sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelRF[s].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    #AUCROC.append(['train '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "    #    y_true = [e[0] for e in train1], \n",
    "    #    y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in train1])]\n",
    "    #)])    \n",
    "    print('{0} {1}'.format(s,AUCROC[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сравнимо с градиентным бустингом. Достигается улучшение по сравнению с текущим подходом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5125 ['LR test on 5125-model', 0.83345360139130609]\n",
      "8780 ['LR test on 8780-model', 0.82907160978351158]\n",
      "16295 ['LR test on 16295-model', 0.83297771967560108]\n",
      "152483 ['LR test on 152483-model', 0.83209338177671432]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "modelLR = {}\n",
    "AUCROC = []\n",
    "for f in [40,20,10,1]:\n",
    "    train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*f) == 0]\n",
    "    s = len(train1)\n",
    "    \n",
    "    modelRF[s] = sklearn.linear_model.LogisticRegression(penalty='l1', class_weight = 'auto') \\\n",
    "        .fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append(['LR test on {0}-model'.format(s),sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelRF[s].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    print('{0} {1}'.format(s,AUCROC[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия хуже текущего подхода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*20) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_table_to_file(table, filename):\n",
    "    f = open(filename,'w+')\n",
    "    #f.write('label,' + ','.join(columns)+'\\n')\n",
    "    f.write('\\n'.join([','.join([str(e) for e in r]) for r in table]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Варьируем параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT 30 trees ['GBT 30 trees', 0.84589361214189684]\n",
      "GBT 50 trees ['GBT 50 trees', 0.84528972103790279]\n",
      "GBT 100 trees ['GBT 100 trees', 0.85903091365852413]\n",
      "GBT 200 trees ['GBT 200 trees', 0.86655869477184377]\n",
      "GBT 500 trees ['GBT 500 trees', 0.87359304284435757]\n",
      "GBT 1000 trees ['GBT 1000 trees', 0.87196810959784354]\n",
      "GBT 2000 trees ['GBT 2000 trees', 0.86931065419843812]\n"
     ]
    }
   ],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "import sklearn.ensemble\n",
    "import sklearn\n",
    "modelGBT = {}\n",
    "AUCROC1={}\n",
    "\n",
    "for n in [30,50,100,200,500,1000,2000]:\n",
    "    key = 'GBT {0} trees'.format(n)\n",
    "    modelGBT[key] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=n, learning_rate=0.06,\n",
    "       max_depth=3, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append([key,sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[key].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    print('{0} {1}'.format(key,AUCROC[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT 1 depth ['GBT 1 depth', 0.8879349885558141]\n",
      "GBT 2 depth ['GBT 2 depth', 0.87755841674118451]\n",
      "GBT 3 depth ['GBT 3 depth', 0.86931065419843812]\n",
      "GBT 5 depth ['GBT 5 depth', 0.86385182037894159]\n",
      "GBT 7 depth ['GBT 7 depth', 0.83871529256437416]\n",
      "GBT 9 depth ['GBT 9 depth', 0.85280441138388219]\n",
      "GBT 13 depth ['GBT 13 depth', 0.84844868481154767]\n"
     ]
    }
   ],
   "source": [
    "# Вариация глубины\n",
    "import sklearn.ensemble\n",
    "import sklearn\n",
    "\n",
    "\n",
    "for d in [1,2,3,5,7,9,13]:\n",
    "    key = 'GBT {0} depth'.format(d)\n",
    "    modelGBT[key] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=2000, learning_rate=0.06,\n",
    "       max_depth = d, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append([key,sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[key].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    print('{0} {1}'.format(key,AUCROC[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT 200-trees 1-depth 0.03-learn rate ['GBT 200-trees 1-depth 0.03-learn rate', 0.84971364889193668]\n",
      "GBT 200-trees 1-depth 0.05-learn rate ['GBT 200-trees 1-depth 0.05-learn rate', 0.86601748312381921]\n",
      "GBT 200-trees 1-depth 0.07-learn rate ['GBT 200-trees 1-depth 0.07-learn rate', 0.87431954272171908]\n",
      "GBT 200-trees 1-depth 0.1-learn rate ['GBT 200-trees 1-depth 0.1-learn rate', 0.8805847381800741]\n",
      "GBT 200-trees 3-depth 0.03-learn rate ['GBT 200-trees 3-depth 0.03-learn rate', 0.85819464527279732]\n",
      "GBT 200-trees 3-depth 0.05-learn rate ['GBT 200-trees 3-depth 0.05-learn rate', 0.86671247896521098]\n",
      "GBT 200-trees 3-depth 0.07-learn rate ['GBT 200-trees 3-depth 0.07-learn rate', 0.86651242187222643]\n",
      "GBT 200-trees 3-depth 0.1-learn rate ['GBT 200-trees 3-depth 0.1-learn rate', 0.86498075630265225]\n",
      "GBT 200-trees 5-depth 0.03-learn rate ['GBT 200-trees 5-depth 0.03-learn rate', 0.85553795215469985]\n",
      "GBT 200-trees 5-depth 0.05-learn rate ['GBT 200-trees 5-depth 0.05-learn rate', 0.86698257435703985]\n",
      "GBT 200-trees 5-depth 0.07-learn rate ['GBT 200-trees 5-depth 0.07-learn rate', 0.86887557742809096]\n",
      "GBT 200-trees 5-depth 0.1-learn rate ['GBT 200-trees 5-depth 0.1-learn rate', 0.86897149019423403]\n",
      "GBT 200-trees 7-depth 0.03-learn rate ['GBT 200-trees 7-depth 0.03-learn rate', 0.85015613568305604]\n",
      "GBT 200-trees 7-depth 0.05-learn rate ['GBT 200-trees 7-depth 0.05-learn rate', 0.86172814505820028]\n",
      "GBT 200-trees 7-depth 0.07-learn rate ['GBT 200-trees 7-depth 0.07-learn rate', 0.86827678256520968]\n",
      "GBT 200-trees 7-depth 0.1-learn rate ['GBT 200-trees 7-depth 0.1-learn rate', 0.8702950772088609]\n",
      "GBT 500-trees 1-depth 0.03-learn rate ['GBT 500-trees 1-depth 0.03-learn rate', 0.8768722315787989]\n",
      "GBT 500-trees 1-depth 0.05-learn rate ['GBT 500-trees 1-depth 0.05-learn rate', 0.88408045251604284]\n",
      "GBT 500-trees 1-depth 0.07-learn rate ['GBT 500-trees 1-depth 0.07-learn rate', 0.88822259217556754]\n",
      "GBT 500-trees 1-depth 0.1-learn rate ['GBT 500-trees 1-depth 0.1-learn rate', 0.88948290445447564]\n",
      "GBT 500-trees 3-depth 0.03-learn rate ['GBT 500-trees 3-depth 0.03-learn rate', 0.86924134047087209]\n",
      "GBT 500-trees 3-depth 0.05-learn rate ['GBT 500-trees 3-depth 0.05-learn rate', 0.87256546878603491]\n",
      "GBT 500-trees 3-depth 0.07-learn rate ['GBT 500-trees 3-depth 0.07-learn rate', 0.87190616548744704]\n",
      "GBT 500-trees 3-depth 0.1-learn rate ['GBT 500-trees 3-depth 0.1-learn rate', 0.86977532256755108]\n",
      "GBT 500-trees 5-depth 0.03-learn rate ['GBT 500-trees 5-depth 0.03-learn rate', 0.87139475014977619]\n",
      "GBT 500-trees 5-depth 0.05-learn rate ['GBT 500-trees 5-depth 0.05-learn rate', 0.86899637612004854]\n",
      "GBT 500-trees 5-depth 0.07-learn rate ['GBT 500-trees 5-depth 0.07-learn rate', 0.86958848014628898]\n",
      "GBT 500-trees 5-depth 0.1-learn rate ['GBT 500-trees 5-depth 0.1-learn rate', 0.87272255799411957]\n",
      "GBT 500-trees 7-depth 0.03-learn rate ['GBT 500-trees 7-depth 0.03-learn rate', 0.87025225208335788]\n",
      "GBT 500-trees 7-depth 0.05-learn rate ['GBT 500-trees 7-depth 0.05-learn rate', 0.87053494262499842]\n",
      "GBT 500-trees 7-depth 0.07-learn rate ['GBT 500-trees 7-depth 0.07-learn rate', 0.87212787083086685]\n",
      "GBT 500-trees 7-depth 0.1-learn rate ['GBT 500-trees 7-depth 0.1-learn rate', 0.86937650129687483]\n",
      "GBT 2000-trees 1-depth 0.03-learn rate ['GBT 2000-trees 1-depth 0.03-learn rate', 0.88986008555543727]\n",
      "GBT 2000-trees 1-depth 0.05-learn rate ['GBT 2000-trees 1-depth 0.05-learn rate', 0.88869697626439181]\n",
      "GBT 2000-trees 1-depth 0.07-learn rate ['GBT 2000-trees 1-depth 0.07-learn rate', 0.8880931471125888]\n",
      "GBT 2000-trees 1-depth 0.1-learn rate ['GBT 2000-trees 1-depth 0.1-learn rate', 0.8874717343128028]\n",
      "GBT 2000-trees 3-depth 0.03-learn rate ['GBT 2000-trees 3-depth 0.03-learn rate', 0.87246653652396833]\n",
      "GBT 2000-trees 3-depth 0.05-learn rate ['GBT 2000-trees 3-depth 0.05-learn rate', 0.86890544646492973]\n",
      "GBT 2000-trees 3-depth 0.07-learn rate ['GBT 2000-trees 3-depth 0.07-learn rate', 0.87100048371193406]\n",
      "GBT 2000-trees 3-depth 0.1-learn rate ['GBT 2000-trees 3-depth 0.1-learn rate', 0.86714911800820371]\n",
      "GBT 2000-trees 5-depth 0.03-learn rate ['GBT 2000-trees 5-depth 0.03-learn rate', 0.86648360063548946]\n",
      "GBT 2000-trees 5-depth 0.05-learn rate ['GBT 2000-trees 5-depth 0.05-learn rate', 0.86598457573604171]\n",
      "GBT 2000-trees 5-depth 0.07-learn rate ['GBT 2000-trees 5-depth 0.07-learn rate', 0.86139301602713947]\n",
      "GBT 2000-trees 5-depth 0.1-learn rate ['GBT 2000-trees 5-depth 0.1-learn rate', 0.84257344876696549]\n",
      "GBT 2000-trees 7-depth 0.03-learn rate ['GBT 2000-trees 7-depth 0.03-learn rate', 0.86390569993019883]\n",
      "GBT 2000-trees 7-depth 0.05-learn rate ['GBT 2000-trees 7-depth 0.05-learn rate', 0.84543328581330934]\n",
      "GBT 2000-trees 7-depth 0.07-learn rate ['GBT 2000-trees 7-depth 0.07-learn rate', 0.84140361092924998]\n",
      "GBT 2000-trees 7-depth 0.1-learn rate ['GBT 2000-trees 7-depth 0.1-learn rate', 0.84704606296249663]\n",
      "GBT 4000-trees 1-depth 0.03-learn rate ['GBT 4000-trees 1-depth 0.03-learn rate', 0.88872701768993634]\n",
      "GBT 4000-trees 1-depth 0.05-learn rate ['GBT 4000-trees 1-depth 0.05-learn rate', 0.88728879487958123]\n",
      "GBT 4000-trees 1-depth 0.07-learn rate ['GBT 4000-trees 1-depth 0.07-learn rate', 0.88661720888571127]\n",
      "GBT 4000-trees 1-depth 0.1-learn rate ['GBT 4000-trees 1-depth 0.1-learn rate', 0.8859020193235887]\n",
      "GBT 4000-trees 3-depth 0.03-learn rate ['GBT 4000-trees 3-depth 0.03-learn rate', 0.86880634181415739]\n",
      "GBT 4000-trees 3-depth 0.05-learn rate ['GBT 4000-trees 3-depth 0.05-learn rate', 0.86902663572504879]\n",
      "GBT 4000-trees 3-depth 0.07-learn rate ['GBT 4000-trees 3-depth 0.07-learn rate', 0.868610667165301]\n",
      "GBT 4000-trees 3-depth 0.1-learn rate ['GBT 4000-trees 3-depth 0.1-learn rate', 0.85926579057648977]\n",
      "GBT 4000-trees 5-depth 0.03-learn rate ['GBT 4000-trees 5-depth 0.03-learn rate', 0.86076368142761128]\n",
      "GBT 4000-trees 5-depth 0.05-learn rate ['GBT 4000-trees 5-depth 0.05-learn rate', 0.84714024645443564]\n",
      "GBT 4000-trees 5-depth 0.07-learn rate ['GBT 4000-trees 5-depth 0.07-learn rate', 0.83595683085128614]\n",
      "GBT 4000-trees 5-depth 0.1-learn rate ['GBT 4000-trees 5-depth 0.1-learn rate', 0.83935397655761412]\n",
      "GBT 4000-trees 7-depth 0.03-learn rate ['GBT 4000-trees 7-depth 0.03-learn rate', 0.84389401628566874]\n",
      "GBT 4000-trees 7-depth 0.05-learn rate ['GBT 4000-trees 7-depth 0.05-learn rate', 0.84538437051806192]\n",
      "GBT 4000-trees 7-depth 0.07-learn rate ['GBT 4000-trees 7-depth 0.07-learn rate', 0.84140361092924998]\n",
      "GBT 4000-trees 7-depth 0.1-learn rate ['GBT 4000-trees 7-depth 0.1-learn rate', 0.84704606296249663]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for n,d,lr in itertools.product([200,500,2000,4000],[1,3,5,7],[0.03,0.05,0.07,0.1]):\n",
    "    key = 'GBT {0}-trees {1}-depth {2}-learn rate'.format(n,d,lr)\n",
    "    modelGBT[key] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=n, learning_rate=lr,\n",
    "       max_depth = d, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append([key,sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[key].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    print('{0} {1}'.format(key,AUCROC[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 500-trees 15-depth 10-min leaf ['RF 500-trees 15-depth 10-min leaf', 0.88782539243615455]\n",
      "RF 500-trees 15-depth 20-min leaf ['RF 500-trees 15-depth 20-min leaf', 0.88768306670457076]\n",
      "RF 500-trees 15-depth 40-min leaf ['RF 500-trees 15-depth 40-min leaf', 0.88563079801802558]\n",
      "RF 500-trees 18-depth 10-min leaf ['RF 500-trees 18-depth 10-min leaf', 0.88731923764758158]\n",
      "RF 500-trees 18-depth 20-min leaf ['RF 500-trees 18-depth 20-min leaf', 0.88838619713801348]\n",
      "RF 500-trees 18-depth 40-min leaf ['RF 500-trees 18-depth 40-min leaf', 0.88465178370324449]\n",
      "RF 1000-trees 15-depth 10-min leaf ['RF 1000-trees 15-depth 10-min leaf', 0.88869619782164255]\n",
      "RF 1000-trees 15-depth 20-min leaf ['RF 1000-trees 15-depth 20-min leaf', 0.88775841134326294]\n",
      "RF 1000-trees 15-depth 40-min leaf ['RF 1000-trees 15-depth 40-min leaf', 0.88593394086345512]\n",
      "RF 1000-trees 18-depth 10-min leaf ['RF 1000-trees 18-depth 10-min leaf', 0.88859274843677116]\n",
      "RF 1000-trees 18-depth 20-min leaf ['RF 1000-trees 18-depth 20-min leaf', 0.88827000176321325]\n",
      "RF 1000-trees 18-depth 40-min leaf ['RF 1000-trees 18-depth 40-min leaf', 0.88510650201174923]\n",
      "RF 2000-trees 15-depth 10-min leaf ['RF 2000-trees 15-depth 10-min leaf', 0.88762299462777527]\n",
      "RF 2000-trees 15-depth 20-min leaf ['RF 2000-trees 15-depth 20-min leaf', 0.88808596873914003]\n",
      "RF 2000-trees 15-depth 40-min leaf ['RF 2000-trees 15-depth 40-min leaf', 0.88479691075129629]\n",
      "RF 2000-trees 18-depth 10-min leaf ['RF 2000-trees 18-depth 10-min leaf', 0.88813313321159459]\n",
      "RF 2000-trees 18-depth 20-min leaf ['RF 2000-trees 18-depth 20-min leaf', 0.88829314494695577]\n",
      "RF 2000-trees 18-depth 40-min leaf ['RF 2000-trees 18-depth 40-min leaf', 0.88566723398712277]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "model = {}\n",
    "for n,d,ls in itertools.product([500,1000,2000],[15,18],[10,20,40]):\n",
    "    key = 'RF {0}-trees {1}-depth {2}-min leaf'.format(n,d,ls)\n",
    "    model[key] = sklearn.ensemble.RandomForestClassifier(n_estimators = n, criterion='gini', max_depth = d, \n",
    "                min_samples_split=20, min_samples_leaf = ls, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                max_leaf_nodes=None, bootstrap=True, oob_score=False, random_state=None, verbose=0, \n",
    "                warm_start=False, class_weight='auto') \\\n",
    "    .fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append([key,sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in model[key].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    print('{0} {1}'.format(key,AUCROC[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0060000000000000001, 'social_other'),\n",
       " (0.0001619221317144149, 0.021999999999999999, 'emailru'),\n",
       " (0.00066509547187676009, 0.0, 'okru'),\n",
       " (0.0017624640643627231, 0.0, 'vkru'),\n",
       " (0.0023055360894703157, 0.040000000000000001, 'mobile'),\n",
       " (0.0028449690172619615, 0.016, 's10'),\n",
       " (0.0040044522704579307, 0.0060000000000000001, 's9'),\n",
       " (0.0046076953433872242, 0.0, 's8'),\n",
       " (0.0047726843288324536, 0.0, 's7'),\n",
       " (0.0069552274972296962, 0.040000000000000001, 'duration'),\n",
       " (0.0074199581358142504, 0.0, 's6'),\n",
       " (0.0085602519137338504, 0.032000000000000001, 's5'),\n",
       " (0.0085941953517960357, 0.051999999999999998, 'cntrepeat'),\n",
       " (0.009044052066678996, 0.0040000000000000001, 'sstd'),\n",
       " (0.0094969168531821527, 0.106, 'sm5'),\n",
       " (0.015386514597758038, 0.01, 's4'),\n",
       " (0.015813316296610598, 0.056000000000000001, 'smedian'),\n",
       " (0.018193779567013152, 0.002, 's3'),\n",
       " (0.021363422025643501, 0.002, 'has_scores'),\n",
       " (0.021843157622157251, 0.050000000000000003, 'cntuniq'),\n",
       " (0.021952372918380401, 0.043999999999999997, 'sm4'),\n",
       " (0.023047902082895255, 0.017999999999999999, 'sm3'),\n",
       " (0.025712399539846546, 0.01, 'avg10'),\n",
       " (0.02783317186689702, 0.01, 'sm2'),\n",
       " (0.030677678918302701, 0.071999999999999995, 'ssum'),\n",
       " (0.036676705562510042, 0.012, 'avg7'),\n",
       " (0.037224377732379188, 0.01, 's2'),\n",
       " (0.045326891083536887, 0.002, 'avg5'),\n",
       " (0.057534995037898481, 0.021999999999999999, 'avg4'),\n",
       " (0.061563676634920184, 0.056000000000000001, 's1'),\n",
       " (0.074434157051601182, 0.01, 'avg3'),\n",
       " (0.077231492574275265, 0.032000000000000001, 'smax'),\n",
       " (0.079123472540086989, 0.035999999999999997, 'avg2'),\n",
       " (0.10660263037847878, 0.10199999999999999, 'sm1'),\n",
       " (0.13126246543300982, 0.12, 'savg')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(zip(modelGBT['GBT 500-trees 1-depth 0.1-learn rate'].feature_importances_,columns))\n",
    "sorted(zip(model['RF 1000-trees 18-depth 20-min leaf'].feature_importances_,\n",
    "           modelGBT['GBT 500-trees 1-depth 0.1-learn rate'].feature_importances_,columns)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelGBT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-53c78c966458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     sklearn.metrics.roc_auc_score(\n\u001b[0;32m      7\u001b[0m         \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodelGBT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GBT 500-trees 1-depth 0.1-learn rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     ),\n\u001b[0;32m     10\u001b[0m     sklearn.metrics.roc_auc_score(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelGBT' is not defined"
     ]
    }
   ],
   "source": [
    "print('AUCROC on 20160818 sampled test data:\\nsmax {0},\\nGBT  {1},\\nRF   {2}.'.format(\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "           y_true = [e[0] for e in test2], \n",
    "           y_score = [e[columns.index('s1')+1] for e in test2]\n",
    "    ),\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test2], \n",
    "        y_score = [r[1] for r in modelGBT['GBT 500-trees 1-depth 0.1-learn rate'].predict_proba([e[1:] for e in test2])]\n",
    "    ),\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test2], \n",
    "        y_score = [r[1] for r in model['RF 1000-trees 18-depth 20-min leaf'].predict_proba([e[1:] for e in test2])]\n",
    "    )\n",
    "))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобранные модели градиентного бустинга и случайного леса показывают лучшую результативность на тестовой семплированной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = sklearn.ensemble.GradientBoostingClassifier(n_estimators=2000, learning_rate = 0.03,\n",
    "       max_depth = 1, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "\n",
    "m2 = sklearn.ensemble.RandomForestClassifier(n_estimators = 1000, criterion='gini', max_depth = 18, \n",
    "                min_samples_split=20, min_samples_leaf = 20, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                max_leaf_nodes=None, bootstrap=True, oob_score=False, random_state=None, verbose=0, \n",
    "                warm_start=False, class_weight='auto') \\\n",
    "    .fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(m1,open('la_GBT.pck','w'))\n",
    "pickle.dump(m2,open('la_RF.pck','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC on 20160818 sampled test data:\n",
      "smax 0.835034570738,\n",
      "GBT  0.90117992489,\n",
      "RF   0.900539035448.\n"
     ]
    }
   ],
   "source": [
    "print('AUCROC on 20160818 sampled test data:\\nsmax {0},\\nGBT  {1},\\nRF   {2}.'.format(\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "           y_true = [e[0] for e in test2], \n",
    "           y_score = [e[columns.index('s1')+1] for e in test2]\n",
    "    ),\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test2], \n",
    "        y_score = [r[1] for r in m1.predict_proba([e[1:] for e in test2])]\n",
    "    ),\n",
    "    sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test2], \n",
    "        y_score = [r[1] for r in m2.predict_proba([e[1:] for e in test2])]\n",
    "    )\n",
    "))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC on full test data 20160818. smax: 0.843819806579.\n",
      "AUC ROC on full test data 20160818.  GBT: 0.903283929051.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o434.areaUnderROC.\n: org.apache.spark.SparkException: Job 42 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1731)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1730)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:147)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:264)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:126)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:153)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:144)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:222)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:85)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0b4e5b4285b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m print('AUC ROC on full test data 20160818.   RF: {0}.'.format(\n\u001b[1;32m---> 17\u001b[1;33m         BinaryClassificationMetrics(test_rdd2.map(lambda r: (\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/evaluation.pyc\u001b[0m in \u001b[0;36mareaUnderROC\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mROC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mcurve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"areaUnderROC\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o434.areaUnderROC.\n: org.apache.spark.SparkException: Job 42 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1731)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1730)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:147)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:264)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:126)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:153)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:144)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:222)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:85)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "print('AUC ROC on full test data 20160818. smax: {0}.'.format(\n",
    "        BinaryClassificationMetrics(test_rdd2.map(lambda r: (\n",
    "        float(r[1]),\n",
    "        float(r[0])\n",
    "       ))).areaUnderROC\n",
    "))\n",
    "print('AUC ROC on full test data 20160818.  GBT: {0}.'.format(\n",
    "        BinaryClassificationMetrics(test_rdd2.map(lambda r: (\n",
    "        float(m1.predict_proba(r[1:])[0][1]),\n",
    "        float(r[0])\n",
    "       ))).areaUnderROC\n",
    "))\n",
    "\n",
    "print('AUC ROC on full test data 20160818.   RF: {0}.'.format(\n",
    "        BinaryClassificationMetrics(test_rdd2.map(lambda r: (\n",
    "        float(m2.predict_proba(r[1:])[0][1]),\n",
    "        float(r[0])\n",
    "       ))).areaUnderROC\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC on full test data 20160824. smax: 0.843090200848.\n",
      "AUC ROC on full test data 20160824.  GBT: 0.891054128883.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "print('AUC ROC on full test data 20160824. smax: {0}.'.format(\n",
    "        BinaryClassificationMetrics(test_rdd.map(lambda r: (\n",
    "        float(r[1]),\n",
    "        float(r[0])\n",
    "       ))).areaUnderROC\n",
    "))\n",
    "print('AUC ROC on full test data 20160824.  GBT: {0}.'.format(\n",
    "        BinaryClassificationMetrics(test_rdd.map(lambda r: (\n",
    "        float(m1.predict_proba(r[1:])[0][1]),\n",
    "        float(r[0])\n",
    "       ))).areaUnderROC\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances.\n",
      "GBT, RF, feature\n",
      "(0.0, 4.5278955517342106e-06, 'social_other')\n",
      "(0.0, 0.00062820099223011088, 'okru')\n",
      "(0.0, 0.0015131659364007815, 'vkru')\n",
      "(0.0, 0.0043353347851704518, 's7')\n",
      "(0.0, 0.021986015866317231, 'has_scores')\n",
      "(0.00050000000000000001, 0.063582507120006113, 'avg5')\n",
      "(0.002, 0.0061769785939705581, 's8')\n",
      "(0.0060000000000000001, 0.052016535286286816, 'avg4')\n",
      "(0.0060000000000000001, 0.073818657105537686, 'avg3')\n",
      "(0.0085000000000000006, 0.072714365172140513, 'avg2')\n",
      "(0.010500000000000001, 0.012626373624494825, 's5')\n",
      "(0.010999999999999999, 0.022202280361539647, 'sm3')\n",
      "(0.0115, 0.013091467354738562, 's4')\n",
      "(0.014, 0.02698260821878478, 'sm2')\n",
      "(0.017500000000000002, 0.0035269727734909741, 's10')\n",
      "(0.021999999999999999, 0.033306443299158134, 'avg10')\n",
      "(0.022499999999999999, 0.005319320657070861, 's6')\n",
      "(0.022499999999999999, 0.042862800082745088, 'avg7')\n",
      "(0.024500000000000001, 0.010040854254971827, 'sstd')\n",
      "(0.025499999999999998, 0.022476213617825924, 's3')\n",
      "(0.029499999999999998, 0.00017423323325409029, 'emailru')\n",
      "(0.033000000000000002, 0.00728891448982775, 'sm5')\n",
      "(0.035000000000000003, 0.030047406197691626, 's2')\n",
      "(0.037999999999999999, 0.0028819424123700064, 'mobile')\n",
      "(0.042000000000000003, 0.0034609754528306747, 's9')\n",
      "(0.045499999999999999, 0.01022910810463248, 'cntrepeat')\n",
      "(0.049000000000000002, 0.016604932651751059, 'smedian')\n",
      "(0.049500000000000002, 0.019666752655045142, 'sm4')\n",
      "(0.051999999999999998, 0.025303509481994751, 'cntuniq')\n",
      "(0.056000000000000001, 0.082039129724013338, 'smax')\n",
      "(0.057500000000000002, 0.0069001860861257194, 'duration')\n",
      "(0.058000000000000003, 0.030785603404245077, 'ssum')\n",
      "(0.0625, 0.066301918567377854, 's1')\n",
      "(0.0935, 0.11286562218507422, 'savg')\n",
      "(0.094500000000000001, 0.096238142355333761, 'sm1')\n"
     ]
    }
   ],
   "source": [
    "print('Feature importances.\\nGBT, RF, feature\\n'+'\\n'.join([str(r) for r in sorted(zip(m1.feature_importances_,\n",
    "           m2.feature_importances_,columns))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
