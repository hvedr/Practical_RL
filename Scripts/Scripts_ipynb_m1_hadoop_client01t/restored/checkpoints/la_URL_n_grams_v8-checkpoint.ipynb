{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текстовый анализ URL в задаче lookalike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "\n",
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 33)\n",
    "        .set(\"spark.driver.maxResultSize\", \"16g\")\n",
    "        .set('spark.driver.memory','16g')\n",
    "        .set(\"spark.executor.memory\", '20g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 2048)        \n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "n_list = [1,2,3,4,5]\n",
    "tf_size = 2 ** 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_queries = '''\n",
    "\n",
    "CREATE FUNCTION md5 as 'onemd5.Md5';\n",
    "\n",
    "CREATE TABLE `user_kposminin.phone_id_train`(\n",
    "  `phone_num` string, \n",
    "  `id` string, \n",
    "  `approve_label` int, \n",
    "  `full_app_label` int, \n",
    "  `full_app_first_day` int, \n",
    "  `sampled` int, \n",
    "  `strong_sampled` int)\n",
    "PARTITIONED BY (ymd string)\n",
    ";\n",
    "\n",
    "truncate table user_kposminin.phone_id_train;\n",
    "\n",
    "CREATE TABLE `user_kposminin.url_text_train`(\n",
    "  `phone_num` string, \n",
    "  `approve_label` int, \n",
    "  `full_app_label` int, \n",
    "  `full_app_first_day` int, \n",
    "  `sampled` int, \n",
    "  `strong_sampled` int, \n",
    "  `urls_str` string)\n",
    "  partitioned by (`ymd` string)\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE `user_kposminin.phone_id_test`(\n",
    "  `phone_num` string, \n",
    "  `id` string, \n",
    "  `approve_label` int, \n",
    "  `full_app_label` int, \n",
    "  `full_app_first_day` int, \n",
    "  `sampled` int, \n",
    "  `strong_sampled` int)\n",
    "PARTITIONED BY (ymd string)\n",
    ";\n",
    "\n",
    "truncate table user_kposminin.phone_id_test;\n",
    "\n",
    "CREATE TABLE `user_kposminin.url_text_test`(\n",
    "  `phone_num` string, \n",
    "  `approve_label` int, \n",
    "  `full_app_label` int, \n",
    "  `full_app_first_day` int, \n",
    "  `sampled` int, \n",
    "  `strong_sampled` int, \n",
    "  `urls_str` string)\n",
    "  partitioned by (`ymd` string)\n",
    ";\n",
    "\n",
    "'''\n",
    "\n",
    "another_train_prepare_query = '''\n",
    "insert overwrite table user_kposminin.phone_id_train partition (ymd) \n",
    "  select     \n",
    "    m.phone_num,\n",
    "    m.id,\n",
    "    if(ua.id is Null,0,1) as approve_label,\n",
    "    if(uf.id is Null,0,1) as full_app_label,\n",
    "    nvl(uf.first_day,0) as full_app_first_day,\n",
    "    m.sampled,\n",
    "    if(substr(md5(concat(phone_num,'aa')),1,1) = '0', 1, 0) as strong_sampled,\n",
    "    '#ymd0' as ymd\n",
    "  from\n",
    "    (select \n",
    "       uid_str as id,\n",
    "       property_value as phone_num,\n",
    "       if(substr(md5(property_value),1,3) = '000', 1, 0) as sampled\n",
    "     from\n",
    "       prod_dds.md_uid_property \n",
    "     where\n",
    "       property_cd = 'PHONE' and\n",
    "       load_src = 'LI.02'\n",
    "    ) m\n",
    "    left join(\n",
    "      select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "      from prod_features_liveinternet.user_action\n",
    "      where action_type = 'tinkoff_platinum_approved_application'\n",
    "        and ymd between '#ymd1' and '#ymd3'\n",
    "    ) ua on ua.id = m.id\n",
    "    left join(\n",
    "      select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "      from prod_features_liveinternet.user_action\n",
    "      where action_type = 'tinkoff_platinum_complete_application'\n",
    "        and ymd between '#ymd1' and '#ymd3'\n",
    "    ) uf on uf.id = m.id\n",
    "    where\n",
    "      (sampled = 1 or (not ua.id is Null) or (not uf.id is Null))\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "insert overwrite table user_kposminin.url_text_train partition (ymd) \n",
    "select \n",
    "  u.phone_num,\n",
    "  max(u.approve_label) as approve_label,\n",
    "  max(u.full_app_label) as full_app_label,\n",
    "  max(u.full_app_first_day) as full_app_first_day,\n",
    "  max(u.sampled) as sampled,\n",
    "  max(u.strong_sampled) as strong_sampled,\n",
    "  concat_ws(' ',collect_list(url)) as up,\n",
    "  '#ymd0' as ymd\n",
    "from \n",
    "  user_kposminin.phone_id_train u\n",
    "  inner join prod_raw_liveinternet.access_log v on u.id = v.id \n",
    "where\n",
    "  u.ymd = '#ymd0' and \n",
    "  v.ymd = '#ymd0'\n",
    "group by \n",
    "  u.phone_num\n",
    ";\n",
    "\n",
    "'''\n",
    "\n",
    "another_test_prepare_query = '''\n",
    "insert overwrite table user_kposminin.phone_id_test partition (ymd) \n",
    "  select     \n",
    "    m.phone_num,\n",
    "    m.id,\n",
    "    if(ua.id is Null,0,1) as approve_label,\n",
    "    if(uf.id is Null,0,1) as full_app_label,\n",
    "    nvl(uf.first_day,0) as full_app_first_day,\n",
    "    m.sampled,\n",
    "    if(substr(md5(concat(phone_num,'aa')),1,1) = '0', 1, 0) as strong_sampled,\n",
    "    '#ymd0' as ymd\n",
    "  from\n",
    "    (select \n",
    "       uid_str as id,\n",
    "       property_value as phone_num,\n",
    "       if(substr(md5(property_value),1,1) = '0', 1, 0) as sampled\n",
    "     from\n",
    "       prod_dds.md_uid_property \n",
    "     where\n",
    "       property_cd = 'PHONE' and\n",
    "       load_src = 'LI.02' and\n",
    "       md5(property_value),1,1) = '0'\n",
    "    ) m\n",
    "    left semi join prod_raw_liveinternet.access_log v on m.id = v.id and v.ymd = '#ymd0'\n",
    "    left join(\n",
    "      select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "      from prod_features_liveinternet.user_action\n",
    "      where action_type = 'tinkoff_platinum_approved_application'\n",
    "        and ymd between '#ymd1' and '#ymd3'\n",
    "    ) ua on ua.id = m.id\n",
    "    left join(\n",
    "      select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "      from prod_features_liveinternet.user_action\n",
    "      where action_type = 'tinkoff_platinum_complete_application'\n",
    "        and ymd between '#ymd1' and '#ymd3'\n",
    "    ) uf on uf.id = m.id\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "insert overwrite table user_kposminin.url_text_test partition (ymd) \n",
    "select \n",
    "  u.phone_num,\n",
    "  max(u.approve_label) as approve_label,\n",
    "  max(u.full_app_label) as full_app_label,\n",
    "  max(u.full_app_first_day) as full_app_first_day,\n",
    "  max(u.sampled) as sampled,\n",
    "  max(u.strong_sampled) as strong_sampled,\n",
    "  concat_ws(' ',collect_list(url)) as up,\n",
    "  '#ymd0' as ymd\n",
    "from \n",
    "  user_kposminin.phone_id_test u\n",
    "  inner join prod_raw_liveinternet.access_log v on u.id = v.id \n",
    "where\n",
    "  u.ymd = '#ymd0' and \n",
    "  v.ymd = '#ymd0'\n",
    "group by \n",
    "  u.phone_num\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    \n",
    "train_query = 'select * from user_kposminin.url_text_train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used alias to avoid confusion with the mllib library\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.ml.feature import HashingTF as MLHashingTF\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_date = datetime.datetime(2016,12,23).date()\n",
    "qq = ''\n",
    "for _ in range(30):\n",
    "    query = another_train_prepare_query.replace('#ymd0',str(train_date)) \\\n",
    "      .replace('#ymd1',str(train_date + datetime.timedelta(days = 1))) \\\n",
    "      .replace('#ymd3',str(train_date + datetime.timedelta(days = 3))) \\\n",
    "      .replace('#ind',str(train_date).replace('-',''))\n",
    "    qq += query\n",
    "    train_date = train_date - datetime.timedelta(days = 8)\n",
    "#print(qq)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_date = datetime.datetime(2017,1,17).date()\n",
    "qq = ''\n",
    "for _ in range(1):\n",
    "    query = another_test_prepare_query.replace('#ymd0',str(test_date)) \\\n",
    "      .replace('#ymd1',str(test_date + datetime.timedelta(days = 1))) \\\n",
    "      .replace('#ymd3',str(test_date + datetime.timedelta(days = 3))) \\\n",
    "      .replace('#ind',str(test_date).replace('-',''))\n",
    "    qq += query\n",
    "    #test_date = test_date - datetime.timedelta(days = 2)\n",
    "#print(qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translate ru to eng, Transform text to n_gram list, Get n_gram index\n",
    "\n",
    "#abc = list(set(''.join([e[0] for e in hc.sql('select url from prod_raw_liveinternet.access_log v where ymd = \"2017-01-10\" limit 100000').collect()])))\n",
    "\n",
    "def n_gram(s, n):\n",
    "    '''Returns n-gram list from string s.'''\n",
    "    return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "def n_gram_index(ngr,abc):\n",
    "    '''Returns index of n-gram ngr. ngr chars must be from abc list'''\n",
    "    N = tr_abc_len\n",
    "    ind = 0\n",
    "    \n",
    "    for i in range(len(ngr)):\n",
    "        try:\n",
    "            j = abc.index(ngr[i].lower())\n",
    "            if j > N:\n",
    "                j = abc.index(ngr[i].lower().translate(transl))\n",
    "            ind += (N ** i) * j\n",
    "        except ValueError:\n",
    "            ind += (N ** i) * (N - 1)\n",
    "    return ind\n",
    "\n",
    "#abc = list(u'abcdefghijklmnopqrstuvwxyz0123456789 _абвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
    "#tr_abc_len = abc.index(u'а') - 1\n",
    "\n",
    "symbols = (u\"абвгдеёжзийклмнопрстуфхцчшщъыьэюя&-?%#!/\\=_.-~$\",\n",
    "           u\"abvgdeejzijklmnoprstufhzcss_y_eua           \")\n",
    "transl = {ord(a):ord(b) for a, b in zip(*symbols)}\n",
    "\n",
    "def handle_str(s,transl,n):\n",
    "    '''Translate ru-> eng by letter and lower string'''\n",
    "    return re.sub('[ ]+',' '*(n-1),re.sub('[^a-z0-9]+',' ',urllib.unquote(s.encode('UTF-8','ignore')).decode('UTF-8','ignore').lower().translate(transl)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = train_sample.collect()\n",
    "#for i in range(len(a)):\n",
    "#    print i\n",
    "#    b = handle_str(a[i].up,transl)\n",
    "s= u'''ok.ru/feed glistof.net/board/statusy_pro_ulybku/32 ok.ru/dk?st.cmd=anonymMain mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki\n",
    "mail.ru/?from=odnoklassniki ok.ru/ mail.ru/?from=odnoklassniki ok.ru/game/candyvalley2 ok.ru/dk?st.cmd=anonymMain\n",
    "ok.ru/game/candyvalley2 ok.ru/gifts ok.ru/feed ok.ru/ ok.ru/feed ok.ru/game/candyvalley2 ok.ru/game/candyvalley\n",
    "ok.ru/dk?st.cmd=anonymMain ok.ru/ my-hit.org/film/19945/ news.mail.ru/politics/27714036/?frommail=1 my-hit.org/film/19945/ news.mail.ru/politics/27714036/?frommail=1 filmogo.co/5902-odnazhdy-v-odesse-2-sezon-vse-seriyiiiiji.html \n",
    "ok.ru/game/candyvalley allserials.net/serial-3560-zapovednik-straha-1-sezon.html fast-torrent.ru/film/mezhdu-zhiznyu-i-smertyu.html ok.ru/feed bigcinema.to/series/severnyy-veter-serial.html\n",
    "go.mail.ru/search?rf=1011&fm=1&q=%D1%87%D1%82%D0%BE%20%D1%82%D0%B0%D0%BA%D0%BE%D0%B5%D0%B1%D1%80%D1%8E%D0%BA%D0%B8%20%D0%BA%D1%8E%D0%BB%D0%BE%D1%82%D1%8B&sbmt=1478611919453 bigcinema.to/series/severnyy-veter-serial.html \n",
    "bolshoyvopros.ru/questions/1828070-chto-takoe-brjuki-kjuloty-kak-oni-vygljadjat-kto-ih-avtor.html go.mail.ru/search?fm=1&rf=1011&q=ex.ua ok.ru/?_erv=vaywlyirbwpynedplup ok.ru/game/vegamix ok.ru/game/piratetreasures mail.ru/?from=odnoklassniki\n",
    "ok.ru/?_erv=vaywlyirbwpynedplup mail.ru/?from=odnoklassniki filmogo.co/5902-odnazhdy-v-odesse-2-sezon-vse-seriyiiiiji.html mail.ru/?from=odnoklassniki allserials.net/serial-3560-zapovednik-straha-1-sezon.html \n",
    "mail.ru/?from=odnoklassniki news.mail.ru/politics/27714037/?frommail=1 ok.ru/dk?st.cmd=anonymMain news.mail.ru/politics/27714037/?frommail=1 mail.ru/?from=odnoklassniki go.mail.ru/search?fm=1&rf=1011&q=cnfnecs ghj ek%2Cre \n",
    "mail.ru/?from=odnoklassniki statusas.ru/ulibka/33-ulibka.html mail.ru/?from=odnoklassniki go.mail.ru/search?fm=1&rf=1011&q=%D0%B1%D1%80%D1%8E%D0%BA%D0%B8 %D0%BA%D1%8E%D0%BB%D0%BE%D1%82%D1%8B mail.ru/?from=odnoklassniki\n",
    "mail.ru/?from=odnoklassniki ok.ru/feed mail.ru/?from=odnoklassniki ok.ru/profile/330316254834 mail.ru/?from=odnoklassniki pogoda.mail.ru/prognoz/kiev/ mail.ru/?from=odnoklassniki pogoda.mail.ru/prognoz/kiev/\n",
    "news.mail.ru/politics/27714036/?frommail=1 mail.ru/?from=odnoklassniki news.mail.ru/politics/27714036/?frommail=1 mail.ru/?from=odnoklassniki glistof.net/board/statusy_pro_ulybku/32 ok.ru/?_erv=viewlyirbwpynedra\n",
    "mail.ru/?from=odnoklassniki ok.ru/ mail.ru/?from=odnoklassniki ok.ru/ bigcinema.to/series/podzemnyy-perehod-serial.html bigcinema.to/series/podzemnyy-perehod-serial.html fast-torrent.ru/film/mezhdu-zhiznyu-i-smertyu.html \n",
    "bigcinema.to/series/severnyy-veter-serial.html bigcinema.to/series/severnyy-veter-serial.html mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki pogoda.mail.ru/prognoz/kiev/\n",
    "pogoda.mail.ru/prognoz/kiev/ mail.ru/?from=odnoklassniki mail.ru/?from=odnoklassniki news.mail.ru/incident/27707964/?frommail=1 news.mail.ru/incident/27707964/?frommail=1'''\n",
    "\n",
    "#print(handle_str(s,transl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sdf = hc.sql('select phone_num,label,first_day,up from user_kposminin.url_text_20161108_2 limit 100') #\n",
    "#sdf.show()\n",
    "\n",
    "def generate_ngram_stat_manual(sdf,n_list,tf_size, idf = None):\n",
    "    \n",
    "    #cols = sdf.columns\n",
    "    df_ngrams = (sdf\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [handle_str(r[-1],transl,n = max(n_list))])\n",
    "               .map(lambda r: r[:-1] + [[[e for e in n_gram(r[-1],n) if not ' ' in e] for n in n_list]] )\n",
    "               .map(lambda r: r + [[len(l) for l in r[-1]]])\n",
    "            )\n",
    "    \n",
    "    df_tf = (df_ngrams\n",
    "         .map(lambda r: r[:-2] + \n",
    "                [[\n",
    "                    reduce(\n",
    "                       lambda a,b:a+b,\n",
    "                       [\n",
    "                        [(int(hashlib.md5(k).hexdigest(),16) % tf_size,float(v)/r[-1][n-1] ),\n",
    "                        (int(hashlib.md5(k + ':;Z').hexdigest(),16) % tf_size,float(v)/r[-1][n-1])]\n",
    "                          for k,v in Counter(r[-2][n-1]).iteritems()  \n",
    "                       ],\n",
    "                       []\n",
    "                     )      for n in n_list\n",
    "                ]]\n",
    "             )\n",
    "         )\n",
    "\n",
    "    df_tf.cache()\n",
    "    \n",
    "    \n",
    "    if not idf:\n",
    "        id_num = float(df_tf.count())\n",
    "        idf = (df_tf\n",
    "           .flatMap(lambda r: reduce(lambda a,b:a+b,[[((n,k),1) for k,_ in r[-1][n-1]] for n in n_list],[]) )\n",
    "           .reduceByKey(lambda v1,v2: v1 + v2)\n",
    "           .map(lambda (k,v):(k,np.log((id_num + 1)/(v + 1))))\n",
    "         )\n",
    "\n",
    "    df_tfidf = (df_tf\n",
    "            .flatMap(lambda r: reduce(lambda a,b:a+b,[[((n,k),r[:-1] + [v]) for k,v in r[-1][n-1]] for n in n_list]))\n",
    "            .join(idf)\n",
    "            .map(lambda ((n,k),(v,i)): (tuple(v[:-1]),(k,n,v[-1],v[-1] * i)))\n",
    "            .groupByKey()\n",
    "            .map(lambda (k,v):(k,sorted(v)))            \n",
    "           )\n",
    "    \n",
    "    return df_tfidf, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_ngram_tf_manual(sdf,n_list,tf_size):\n",
    "    \n",
    "    #cols = sdf.columns\n",
    "    df_ngrams = (sdf\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [handle_str(r[-1],transl,n = max(n_list))])\n",
    "               .map(lambda r: r[:-1] + [[[e for e in n_gram(r[-1],n) if not ' ' in e] for n in n_list]] )\n",
    "               .map(lambda r: r + [[len(l) for l in r[-1]]])\n",
    "            )\n",
    "    \n",
    "    df_tf = (df_ngrams\n",
    "         .map(lambda r: r[:-2] + \n",
    "                [[\n",
    "                    reduce(\n",
    "                       lambda a,b:a+b,\n",
    "                       [\n",
    "                        [(int(hashlib.md5(k).hexdigest(),16) % tf_size,float(v)/r[-1][n-1] ),\n",
    "                        (int(hashlib.md5(k + ':;Z').hexdigest(),16) % tf_size,float(v)/r[-1][n-1])]\n",
    "                          for k,v in Counter(r[-2][n-1]).iteritems()  \n",
    "                       ],\n",
    "                       []\n",
    "                     )      for n in n_list\n",
    "                ]]\n",
    "             )\n",
    "         )\n",
    "    \n",
    "    return df_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/30 14:40:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://nameservice1/user/k.p.osminin/url_text_tf_train_dir' to trash at: hdfs://nameservice1/user/k.p.osminin/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! hadoop fs -rm -r hdfs://nameservice1/user/k.p.osminin/url_text_tf_train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train was handled in 1:13:41.867371.\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "train_select_query = '''\n",
    "select ymd,phone_num,approve_label,full_app_label,full_app_first_day,sampled,strong_sampled,urls_str \n",
    "from user_kposminin.url_text_train\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "df1 = hc.sql(train_select_query)\n",
    "# df2, idf = generate_ngram_stat_manual(df1,n_list,tf_size) # Не проработал за выходные \n",
    "df2 = generate_ngram_tf_manual(df1, n_list, tf_size)\n",
    "\n",
    "\n",
    "\n",
    "(df2\n",
    ".saveAsTextFile('url_text_tf_train_dir')\n",
    " )\n",
    "\n",
    "#.map(lambda (k,v):','.join([str(e) for e in k]) + ',' + ';'.join([' '.join([str(ee) for ee in e]) for e in v]) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('train was handled in {}.'.format(datetime.datetime.now()  - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "test_select_query = '''\n",
    "select ymd,phone_num,approve_label,full_app_label,full_app_first_day,sampled,strong_sampled,urls_str \n",
    "from user_kposminin.url_text_test\n",
    "'''\n",
    "\n",
    "\n",
    "df1t = hc.sql(test_select_query)\n",
    "df2t,idf = generate_ngram_stat_manual(df1t,n_list,tf_size)\n",
    "df2t.map(lambda (k,v):','.join([str(e) for e in k]) + ',' + ';'.join([' '.join([str(ee) for ee in e]) for e in v]) + '\\n')\n",
    "(df2t\n",
    ".map(lambda (k,v):','.join([str(e) for e in k]) + ',' + ';'.join([' '.join([str(ee) for ee in e]) for e in v]) + '\\n')\n",
    ".saveAsTextFile('./external_hdfs/url_text_test_dir')\n",
    " )\n",
    "\n",
    "print('{}.test was handled in {}.'.format(datetime.datetime.now(),datetime.datetime.now()  - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#a = df2.take(5)\n",
    "#len(a[0][1])\n",
    "import os\n",
    "(df2\n",
    ".map(lambda (k,v):','.join([str(e) for e in k]) + ',' + ';'.join([' '.join([str(ee) for ee in e]) for e in v]) + '\\n')\n",
    ".saveAsTextFile('url_text_train_dir_tst')\n",
    " )\n",
    "#df2.saveAsSequenceFile('external_hdfs/url_text_trains_seq_tst')\n",
    "\n",
    "#os.popen('cat ./external_hdfs/url_text_train_dir_tst/* > url_text_train.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! du -h external_hdfs/url_text_train.txt\n",
    "#! du --help\n",
    "19./200 * 1500000/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#htf_method = MLHashingTF(numFeatures = tf_size, inputCol=\"ngram_list\", outputCol=\"tf\")\n",
    "#df_tf = htf_method.transform(df_ngrams) \n",
    "#df_tf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_ngram_stat(sdf,n_list,tf_size,idf_method = None, minDocFreq = 1):\n",
    "    '''\n",
    "    Generates n-gram statistics of sdf.\n",
    "    \n",
    "    Input:\n",
    "      sdf -PySpark DataFrame.sdf last column contains text data to analyse (type string)\n",
    "      n - size of n-grams\n",
    "      tf_size -  dimension of a  space ngram projected to;\n",
    "      \n",
    "    Returns DataFrame with all sdf columns except last + columns:\n",
    "        tf_size - dimension of a  space ngram projected to;\n",
    "        tf_index - list of  indexes of n-gram found in sdf text columns;\n",
    "        tf_values - list of corresponding TF values (n-gram counts);\n",
    "        idf_values - list of corresponding TFIDF values.\n",
    "    '''\n",
    "    cols = sdf.columns\n",
    "    df_ngrams = (sdf\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [handle_str(r[-1],transl,max(n_list))])\n",
    "               .map(lambda r: list(r[:-1]) + [reduce(lambda a,b:a+b,n_gram(r[-1],n)  for b in n_list)])\n",
    "               .toDF()\n",
    "               .withColumnRenamed(\"_{}\".format(len(cols)),\"ngram_list\"))    \n",
    "    htf_method = MLHashingTF(numFeatures = tf_size, inputCol=\"ngram_list\", outputCol=\"tf\")\n",
    "    df_tf = htf_method.transform(df_ngrams)    \n",
    "    if not idf_method:\n",
    "        print('Fitting idf_method')\n",
    "        idf_method = MLIDF(inputCol=\"tf\", outputCol=\"idf\", minDocFreq = minDocFreq).fit(df_tf)        \n",
    "    df_tfidf = idf_method.transform(df_tf)\n",
    "    df_data = (df_tfidf\n",
    "             .rdd\n",
    "             .map(lambda r:\n",
    "                  list(r[:-3]) + \n",
    "                  [r.tf.indices.tolist(),\n",
    "                  r.tf.values.tolist(),\n",
    "                  r.idf.values.tolist()]\n",
    "                 )\n",
    "             .toDF()\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)), \"tf_index\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+1), \"tf_values\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+2), \"idf_values\")\n",
    "             )\n",
    "    for i in range(len(cols)):\n",
    "        df_data = df_data.withColumnRenamed(\"_{}\".format(i+1),cols[i])\n",
    "    return df_data, idf_method\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_ngram_stat(sdf,n_list,tf_size,idf_method = None, minDocFreq = 1):\n",
    "    '''\n",
    "    Generates n-gram statistics of sdf.\n",
    "    \n",
    "    Input:\n",
    "      sdf -PySpark DataFrame.sdf last column contains text data to analyse (type string)\n",
    "      n - size of n-grams\n",
    "      tf_size -  dimension of a  space ngram projected to;\n",
    "      \n",
    "    Returns DataFrame with all sdf columns except last + columns:\n",
    "        tf_size - dimension of a  space ngram projected to;\n",
    "        tf_index - list of  indexes of n-gram found in sdf text columns;\n",
    "        tf_values - list of corresponding TF values (n-gram counts);\n",
    "        idf_values - list of corresponding TFIDF values.\n",
    "    '''\n",
    "    cols = sdf.columns\n",
    "    df_ngrams = (sdf\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [handle_str(r[-1],transl,max(n_list))])\n",
    "               .map(lambda r: list(r[:-1]) + [n_gram(r[-1],n)  for b in n_list])\n",
    "               .toDF()\n",
    "               .withColumnRenamed(\"_{}\".format(len(cols)),\"ngram_list\"))    \n",
    "    htf_method = MLHashingTF(numFeatures = tf_size, inputCol=\"ngram_list\", outputCol=\"tf\")\n",
    "    df_tf = htf_method.transform(df_ngrams)    \n",
    "    if not idf_method:\n",
    "        print('Fitting idf_method')\n",
    "        idf_method = MLIDF(inputCol=\"tf\", outputCol=\"idf\", minDocFreq = minDocFreq).fit(df_tf)        \n",
    "    df_tfidf = idf_method.transform(df_tf)\n",
    "    df_data = (df_tfidf\n",
    "             .rdd\n",
    "             .map(lambda r:\n",
    "                  list(r[:-3]) + \n",
    "                  [r.tf.indices.tolist(),\n",
    "                  r.tf.values.tolist(),\n",
    "                  r.idf.values.tolist()]\n",
    "                 )\n",
    "             .toDF()\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)), \"tf_index\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+1), \"tf_values\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+2), \"idf_values\")\n",
    "             )\n",
    "    for i in range(len(cols)):\n",
    "        df_data = df_data.withColumnRenamed(\"_{}\".format(i+1),cols[i])\n",
    "    return df_data, idf_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sample = hc.sql('select phone_num,label,first_day,up from user_kposminin.url_text_20161108_2')\n",
    "train_data,idf_method = generate_ngram_stat(train_sample, n = n, tf_size = tf_size)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.write.saveAsTable(\"user_kposminin.url_text_feat_20161108_7\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sample = hc.sql('select phone_num,label,first_day,up from user_kposminin.url_text_20161115 where (substr(md5(phone_num),1,1) in (\"0\",\"1\") or label = 1)')\n",
    "\n",
    "from pyspark.ml.feature import IDFModel\n",
    "idfm = MLIDF._new_java_obj(\"org.apache.spark.ml.feature.IDFModel.load\", \"idf_model\")\n",
    "idf_method = IDFModel(idfm)\n",
    "\n",
    "test_data, _ = generate_ngram_stat(test_sample, n = n, tf_size = tf_size, idf_method = idf_method)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.write.saveAsTable(\"user_kposminin.url_text_feat_20161115_12\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save IDF model\n",
    "writer = idf_method._call_java(\"write\")\n",
    "writer.save(\"idf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load IDF model\n",
    "from pyspark.ml.feature import IDFModel\n",
    "idfm = MLIDF._new_java_obj(\"org.apache.spark.ml.feature.IDFModel.load\", \"idf_model\")\n",
    "idf_method1 = IDFModel(idfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = (hc.sql('select label,first_day,tf_index from user_kposminin.url_text_feat_20161108_7')\n",
    "         .map(lambda r:LabeledPoint(r.label,SparseVector(tf_size,{e:1 for e in r.tf_index})))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train NaiveBayes model\n",
    "train_data.cache()\n",
    "modelNB = NaiveBayes.train(train_data)\n",
    "\n",
    "def predict_proba_NB(f,model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability. f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Function selects winning class with it probability.\n",
    "    Output: tuple with model selected class number as first element (type int) and it probability as second (type float).\n",
    "    '''\n",
    "    logp = [[i,f.dot(model.theta[i]) + model.pi[i]] for i in range(len(model.theta))] # classes with log probabilities\n",
    "    wi = sorted(logp, key = lambda e:  - e[1])[0][0] #winning index\n",
    "    prob = 1./sum([np.exp(e[1] - logp[wi][1]) for e in logp]) #winning class probability\n",
    "    return wi, prob\n",
    "\n",
    "def predict_proba_NB_2(f, model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability for 2-class classification.\n",
    "    f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Output: probability of class 1 (type float).\n",
    "    '''\n",
    "    if len(model.theta) != 2:\n",
    "        print('Model is NOT a 2-class classifier')\n",
    "        return None\n",
    "    logp = [f.dot(model.theta[i]) + model.pi[i] for i in range(2)]    \n",
    "    return 1./(1. + np.exp(logp[0] - logp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LogisticRegression model\n",
    "modelLR = LogisticRegressionWithSGD.train(train_data)\n",
    "modelLR.clearThreshold()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = (hc.sql('select label,first_day,tf_index from user_kposminin.url_text_feat_20161115_12')\n",
    "         .map(lambda r:LabeledPoint(r.label,SparseVector(tf_size,{e:1 for e in r.tf_index})))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = test_data.map( lambda lp: pyspark.sql.Row(\n",
    "        Label = lp.label,\n",
    "        NaiveBayes = float(predict_proba_NB_2(lp.features, modelNB)),\n",
    "        LogisticRegression = float(modelLR.predict(lp.features))\n",
    "    )).toDF().toPandas()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build AUCROC metric and print results\n",
    "import sklearn\n",
    "AUCROC = {}\n",
    "for c in df_test.columns:\n",
    "    if c!= 'Label':\n",
    "        AUCROC[c] = sklearn.metrics.roc_auc_score(df_test['Label'],df_test[c])\n",
    "        \n",
    "print('Methods AUCROC performance on test sample ({0:.0f} samples with {1:.0f} positives):\\n'.format(len(df_test),df_test['Label'].sum()) +\n",
    "     '\\n'.join(['{0:<30}{1:.5f}'.format(k,v) for (k,v) in AUCROC.items()]))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_conf_str = '''\n",
    "# task type, support train and predict\n",
    "task = train\n",
    "\n",
    "# boosting type, support gbdt for now, alias: boosting, boost\n",
    "boosting_type = gbdt\n",
    "\n",
    "# application type, support following application\n",
    "# regression , regression task\n",
    "# binary , binary classification task\n",
    "# lambdarank , lambdarank task\n",
    "# alias: application, app\n",
    "objective = binary\n",
    "\n",
    "# eval metrics, support multi metric, delimite by ',' , support following metrics\n",
    "# l1 \n",
    "# l2 , default metric for regression\n",
    "# ndcg , default metric for lambdarank\n",
    "# auc \n",
    "# binary_logloss , default metric for binary\n",
    "# binary_error\n",
    "metric = auc,binary_logloss\n",
    "\n",
    "# frequence for metric output\n",
    "metric_freq = 1\n",
    "\n",
    "# true if need output metric for training data, alias: tranining_metric, train_metric\n",
    "is_training_metric = true\n",
    "\n",
    "# number of bins for feature bucket, 255 is a recommend setting, it can save memories, and also has good accuracy. \n",
    "max_bin = 255\n",
    "\n",
    "# training data\n",
    "# if exsting weight file, should name to \"binary.train.weight\"\n",
    "# alias: train_data, train\n",
    "#data = binary.train\n",
    "\n",
    "# validation data, support multi validation data, separated by ','\n",
    "# if exsting weight file, should name to \"binary.test.weight\"\n",
    "# alias: valid, test, test_data, \n",
    "#valid_data = binary.test\n",
    "\n",
    "# number of trees(iterations), alias: num_tree, num_iteration, num_iterations, num_round, num_rounds\n",
    "num_trees = 100\n",
    "\n",
    "# shrinkage rate , alias: shrinkage_rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# number of leaves for one tree, alias: num_leaf\n",
    "num_leaves = 63\n",
    "\n",
    "# type of tree learner, support following types:\n",
    "# serial , single machine version\n",
    "# feature , use feature parallel to train\n",
    "# data , use data parallel to train\n",
    "# voting , use voting based parallel to train\n",
    "# alias: tree\n",
    "tree_learner = data\n",
    "\n",
    "# number of threads for multi-threading. One thread will use one CPU, defalut is setted to #cpu. \n",
    "# num_threads = 8\n",
    "\n",
    "# feature sub-sample, will random select 80% feature to train on each iteration \n",
    "# alias: sub_feature\n",
    "feature_fraction = 0.8\n",
    "\n",
    "#classes are unbalanced\n",
    "is_unbalance = true\n",
    "\n",
    "# Support bagging (data sub-sample), will perform bagging every 5 iterations\n",
    "bagging_freq = 5\n",
    "\n",
    "# Bagging farction, will random select 80% data on bagging\n",
    "# alias: sub_row\n",
    "bagging_fraction = 0.8\n",
    "\n",
    "# minimal number data for one leaf, use this to deal with over-fit\n",
    "# alias : min_data_per_leaf, min_data\n",
    "min_data_in_leaf = 5\n",
    "\n",
    "# minimal sum hessians for one leaf, use this to deal with over-fit\n",
    "#min_sum_hessian_in_leaf = 5.0\n",
    "\n",
    "# save memory and faster speed for sparse feature, alias: is_sparse\n",
    "is_enable_sparse = true\n",
    "\n",
    "# when data is bigger than memory size, set this to true. otherwise set false will have faster speed\n",
    "# alias: two_round_loading, two_round\n",
    "use_two_round_loading = true\n",
    "\n",
    "# true if need to save data to binary file and application will auto load data from binary file next time\n",
    "# alias: is_save_binary, save_binary\n",
    "is_save_binary_file = false\n",
    "\n",
    "# output model file\n",
    "output_model = lgbm.model\n",
    "\n",
    "'''\n",
    "\n",
    "lgbm_dir = './lgbm'\n",
    "import os\n",
    "os.popen('mkdir -p '+lgbm_dir).read()\n",
    "open(lgbm_dir + '/train.conf','w').write(train_conf_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prapare train data for LightGBM. Write to file.\n",
    "'''\n",
    "with open(lgbm_dir + '/train_data.txt','w') as f:\n",
    "    for r in train_data.map(lambda lp: ' '.join([str(int(lp.label))] + [str(i) +':1' for i in lp.features.indices])).collect():\n",
    "        f.write(r+'\\n')\n",
    "'''\n",
    "print(datetime.datetime.now())\n",
    "# prapare test data for LightGBM. Write to file.\n",
    "test = (hc.sql('select label,tf_index,idf_values from user_kposminin.url_text_feat_20161108_7')\n",
    "        .map(lambda r: ' '.join([str(int(r.label))] + [str(i) +':' + str(v) for i,v in zip(r.tf_index,r.idf_values)]))\n",
    "        .saveAsTextFile('url_text_train_data1')\n",
    "       )\n",
    "#with open(lgbm_dir + '/test_data.txt','w') as f:\n",
    "#    for r in test.collect():\n",
    "#        f.write(r+'\\n')\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "# prapare test data for LightGBM. Write to file.\n",
    "test = (hc.sql('select label,tf_index from user_kposminin.url_text_feat_20161115_12 where substr(md5(phone_num),1,1) in (\"0\",\"1\")')\n",
    "        .map(lambda r: ' '.join([str(int(r.label))] + [str(i) +':1'  for i in r.tf_index]))\n",
    "        .saveAsTextFile('url_text_test_data_sampled')\n",
    "       )\n",
    "#with open(lgbm_dir + '/test_data.txt','w') as f:\n",
    "#    for r in test.collect():\n",
    "#        f.write(r+'\\n')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_ngram_stat(sdf,n,tf_size,idf_method = None, minDocFreq = 1):\n",
    "    '''\n",
    "    Generates n-gram statistics of sdf.\n",
    "    \n",
    "    Input:\n",
    "      sdf -PySpark DataFrame.sdf last column contains text data to analyse (type string)\n",
    "      n - size of n-grams\n",
    "      tf_size -  dimension of a  space ngram projected to;\n",
    "      \n",
    "    Returns DataFrame with all sdf columns except last + columns:\n",
    "        tf_size - dimension of a  space ngram projected to;\n",
    "        tf_index - list of  indexes of n-gram found in sdf text columns;\n",
    "        tf_values - list of corresponding TF values (n-gram counts);\n",
    "        idf_values - list of corresponding TFIDF values.\n",
    "    '''\n",
    "    cols = sdf.columns\n",
    "    df_ngrams = (sdf\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [n_gram(handle_str(r[-1],transl),n)])\n",
    "               .toDF()\n",
    "               .withColumnRenamed(\"_{}\".format(len(cols)),\"ngram_list\"))    \n",
    "    htf_method = MLHashingTF(numFeatures = tf_size, inputCol=\"ngram_list\", outputCol=\"tf\")\n",
    "    df_tf = htf_method.transform(df_ngrams)    \n",
    "    if not idf_method:\n",
    "        print('Fitting idf_method')\n",
    "        idf_method = MLIDF(inputCol=\"tf\", outputCol=\"idf\", minDocFreq = minDocFreq).fit(df_tf)        \n",
    "    df_tfidf = idf_method.transform(df_tf)\n",
    "    df_data = (df_tfidf\n",
    "             .rdd\n",
    "             .map(lambda r:\n",
    "                  list(r[:-3]) + \n",
    "                  [r.tf.indices.tolist(),\n",
    "                  r.tf.values.tolist(),\n",
    "                  r.idf.values.tolist()]\n",
    "                 )\n",
    "             .toDF()\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)),\"tf_index\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+1),\"tf_values\")\n",
    "             .withColumnRenamed(\"_{}\".format(len(cols)+2),\"idf_values\")\n",
    "             )\n",
    "    for i in range(len(cols)):\n",
    "        df_data = df_data.withColumnRenamed(\"_{}\".format(i+1),cols[i])\n",
    "    return df_data, idf_method\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 1)\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set('spark.driver.memory','4g')\n",
    "       # .set(\"spark.executor.memory\", '8g')\n",
    "       # .set(\"spark.yarn.executor.memoryOverhead\", 2048)        \n",
    "       )\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = '''We have been helping grow the San Francisco data science community since 2014. and we are finally ready\n",
    "to bring our fellowship program to New York.  We wanted to give aspiring data scientists in the tri-state area. the \n",
    "opportunity to hone their skills by building real world machine learning systems for finance and startups'''\n",
    "\n",
    "df = sc.parallelize([[0,0,e] for e in text.split('.')]).toDF()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "cols = df.columns\n",
    "df_ngrams = (df\n",
    "               .rdd\n",
    "               .map(lambda r: list(r[:-1]) + [n_gram(handle_str(r[-1],transl),n)])\n",
    "               .toDF()\n",
    "               .withColumnRenamed(\"_{}\".format(len(cols)),\"ngram_list\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ngrams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_size = 2**20\n",
    "htf_method = MLHashingTF(numFeatures = tf_size, inputCol=\"ngram_list\", outputCol=\"tf\")\n",
    "df_tf = htf_method.transform(df_ngrams)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(df_tf.select('tf').collect()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
