{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Кредитный скоринг\n",
    "### Разметка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print('Now is {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 10)\n",
    "        .set(\"spark.driver.maxResultSize\", \"26g\")\n",
    "        .set('spark.driver.memory','26g')\n",
    "        .set(\"spark.executor.memory\", '6g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)      \n",
    "        .set('spark.akka.frameSize',2040)\n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case sparkcontext doesn't work, use following (remove --driver-memory 4g string):\n",
    "#import os\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '''\n",
    "#--master yarn --deploy-mode client --num-executors 2 --executor-memory 8g --executor-cores 1 --conf spark.yarn.queue=kposminin\n",
    "#  pyspark-shell\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем таблицу визитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обсчитываем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_features_query = '''\n",
    "\n",
    "-- -- user_kposminin.cred_app_visits -- --\n",
    "\n",
    "-- select ymd,count(*) from cred_app_visits group by ymd order by ymd;\n",
    "-- user_kposminin.ccall_visits_aza_test_20170309\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_1 as\n",
    "select \n",
    "  phone_mobile, \n",
    "  call_ymd,\n",
    "  (unix_timestamp(max(ymd), 'yyyy-MM-dd') - unix_timestamp(min(ymd), 'yyyy-MM-dd'))/60/60/24 as ymd_range,\n",
    "  stddev(unix_timestamp(ymd, 'yyyy-MM-dd')/60/60 + avg_hour) as time_std,\n",
    "  count(distinct ymd) as ymd_cnt,\n",
    "  count(distinct id) as id_cnt,\n",
    "  avg(avg_hour) as avg_hour,\n",
    "  percentile_approx(avg_hour,0.1) as avg_hour_q10,\n",
    "  percentile_approx(avg_hour,0.9) as avg_hour_q90,\n",
    "  urlfr,\n",
    "  count(*) as cnt,\n",
    "  sum(cnt) as hits,\n",
    "  avg(duration) as avg_duration\n",
    "from \n",
    "  user_kposminin.ccall_visits_aza_test_20170309  v\n",
    "where\n",
    "  call_ymd > ymd and call_ymd < date_add(ymd,180)\n",
    "group by\n",
    "  phone_mobile, \n",
    "  call_ymd,\n",
    "  urlfr\n",
    ";\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_2 as \n",
    "  select \n",
    "     v.phone_mobile,     \n",
    "     v.call_ymd,\n",
    "     v.urlfr,\n",
    "     log((t1.cnt_positive + 1)/(t1.cnt_total - t1.cnt_positive + 1)) as score1,\n",
    "     t2.score as score2,\n",
    "     t3.score as score3,\n",
    "     v.cnt,\n",
    "     v.hits,\n",
    "     v.avg_duration,\n",
    "     v.time_std, \n",
    "     v.ymd_range, \n",
    "     v.avg_hour,\n",
    "     v.avg_hour_q10, \n",
    "     v.avg_hour_q90, \n",
    "     v.ymd_cnt,\n",
    "     substr(y.section_ind, 0, 6) as yaca_ind   \n",
    "  from\n",
    "     user_kposminin.ccall_sc_aza_20170309_1 v\n",
    "     left join user_kposminin.urlfr_tgt_cnt_ccall_20161201 t1 on t1.urlfr = v.urlfr\n",
    "     left join (\n",
    "       select urlfr,score from prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
    "       where ymd = '2017-01-15' and target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "       and (cnt_total > 30000 or cnt_positive > 10)) t2 on t2.urlfr = v.urlfr\n",
    "     left join (\n",
    "       select urlfr,score from prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
    "       where ymd = '2017-01-15' and target = 'tinkoff_platinum_complete_application03@tinkoff_action'\n",
    "       and (cnt_total > 30000 or cnt_positive > 10)) t3 on t3.urlfr = v.urlfr\n",
    "     left join user_kposminin.yaca_urlfr y on y.urlfr = v.urlfr\n",
    "\n",
    ";\n",
    "\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_3 as \n",
    "select\n",
    "  phone_mobile                   as phone_mobile,\n",
    "  call_ymd                       as call_ymd, \n",
    "  yaca_ind                       as yaca_ind,\n",
    "  sum(cnt)                       as visits_cnt\n",
    "from user_kposminin.ccall_sc_aza_20170309_2 a\n",
    "group by\n",
    "  phone_mobile, call_ymd, yaca_ind \n",
    ";\n",
    "\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_4 as \n",
    "select\n",
    "  phone_mobile                   as phone_mobile,\n",
    "  call_ymd                       as call_ymd, \n",
    "  sum(cnt)                       as visits_cnt\n",
    "from user_kposminin.ccall_sc_aza_20170309_2 a\n",
    "group by\n",
    "  phone_mobile, call_ymd\n",
    ";\n",
    "\n",
    "\n",
    "#Здесь не хочет считать. Помогает, если таблицу user_kposminin.cc_sc_tr_2_t  хранить в формате bzip2 и выставить настройки :\n",
    "\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_5 as \n",
    "select \n",
    "  phone_mobile as phone_mobile, \n",
    "  call_ymd as call_ymd, \n",
    "  count(*) as cnt, \n",
    "  sum(cnt) as visits_cnt, \n",
    "  sum(hits) as hits, \n",
    "  avg(avg_duration) as avg_duration, \n",
    "  avg(time_std) as avg_time_std, \n",
    "  avg(ymd_range) as avg_ymd_range, \n",
    "  avg(ymd_cnt) as avg_ymd_cnt, \n",
    "  avg(avg_hour) as avg_hour, \n",
    "  avg(avg_hour_q10) as avg_hour_q10, \n",
    "  avg(avg_hour_q90) as avg_hour_q90, \n",
    "  max(score1) as max_score1, \n",
    "  avg(score1) as avg_score1, \n",
    "  percentile_approx(score1,array(0.95,0.9,0.7,0.5,0.3)) as q_arr_score1, \n",
    "  max(score2) as max_score2, \n",
    "  avg(score2) as avg_score2, \n",
    "  percentile_approx(score2,array(0.95,0.9,0.7,0.5,0.3)) as q_arr_score2, \n",
    "  max(score3) as max_score3,\n",
    "  avg(score3) as avg_score3, \n",
    "  percentile_approx(score3,array(0.95,0.9,0.7,0.5,0.3)) as q_arr_score3 \n",
    "from user_kposminin.ccall_sc_aza_20170309_2 a \n",
    "group by a.phone_mobile, a.call_ymd\n",
    ";\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_5_part2 as \n",
    "select \n",
    "  phone_mobile as phone_mobile, \n",
    "  call_ymd as call_ymd, \n",
    "  sum(if(urlfr like 'e.mail.ru%',1,0)) as emailru,\n",
    "  sum(if(urlfr like 'm.%',1,0))/sum(1) as mobile_share,\n",
    "  sum(if(urlfr rlike '^(m\\\\.)?vk.com%', 1, 0))/sum(1) as vk_share,\n",
    "  sum(if(urlfr like 'vk.com%' or urlfr rlike '^(m\\\\.)?ok\\\\.ru' or urlfr like 'm.odnoklassniki.ru%' or urlfr rlike '^(m\\\\.)?my.mail.ru',1,0))/sum(1) as social_share,\n",
    "\n",
    "  sum(if(avg_hour >= 9 and avg_hour <= 20,cnt,0))/sum(1) as work_hours_hits_share,\n",
    "  stddev(avg_hour) as hour_std,  \n",
    "  count( if(score1 > 1, urlfr,Null))/sum(1) as good_urlfr_share_score1,\n",
    "  count( if(score2 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score2,\n",
    "  count( if(score3 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score3,\n",
    "  avg( if(score1 > 1, time_std ,Null)) as good_urlfr_timestd_score1,\n",
    "  max(\n",
    "             named_struct(\n",
    "             'score1', score1,\n",
    "             'time_std', time_std\n",
    "             )           \n",
    "     ).time_std as max_urlfr_time_std_1\n",
    "from user_kposminin.ccall_sc_aza_20170309_2 a \n",
    "group by a.phone_mobile, a.call_ymd\n",
    ";\n",
    "\n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_6 as \n",
    "select\n",
    "  b.phone_mobile                 as phone_mobile,\n",
    "  b.call_ymd                     as call_ymd, \n",
    "  concat_ws(\" \",sort_array(collect_list(concat(b.yaca_ind,\":\",format_number(b.visits_cnt/greatest(c.visits_cnt,cast(1 as bigint)),5))))) as yaca_str\n",
    "  \n",
    "from user_kposminin.ccall_sc_aza_20170309_3 b \n",
    "  left join user_kposminin.ccall_sc_aza_20170309_5 c on c.phone_mobile = b.phone_mobile and c.call_ymd = b.call_ymd\n",
    "group by\n",
    "  b.phone_mobile, b.call_ymd ;\n",
    " \n",
    "\n",
    "create table user_kposminin.ccall_sc_aza_20170309_scoring as\n",
    "select\n",
    "  a.*,\n",
    "  c.emailru, \n",
    "  c.mobile_share, \n",
    "  c.vk_share, \n",
    "  c.social_share,\n",
    "  c.work_hours_hits_share, \n",
    "  c.hour_std, \n",
    "  c.good_urlfr_share_score1, \n",
    "  c.good_urlfr_share_score2, \n",
    "  c.good_urlfr_share_score3, \n",
    "  c.good_urlfr_timestd_score1, \n",
    "  c.max_urlfr_time_std_1, \n",
    "  b.yaca_str\n",
    "from\n",
    "  user_kposminin.ccall_sc_aza_20170309_5 a\n",
    "  left join ccall_sc_aza_20170309_6 b on b.phone_mobile = a.phone_mobile and b.call_ymd = a.call_ymd\n",
    "  left join ccall_sc_aza_20170309_5_part2 c on c.phone_mobile = a.phone_mobile and c.call_ymd = a.call_ymd\n",
    ";\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузить и обработать таблицу с признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [u'phone_mobile', u'call_ymd', u'cnt', u'visits_cnt',\n",
    "       u'hits', u'avg_duration', u'avg_time_std', u'avg_ymd_range',\n",
    "       u'avg_ymd_cnt', u'avg_hour', u'avg_hour_q10', u'avg_hour_q90',\n",
    "       u'max_score1', u'avg_score1', u'q95_score1', u'q90_score1',\n",
    "       u'q70_score1', u'q50_score1', u'q30_score1', u'max_score2',\n",
    "       u'avg_score2', u'q95_score2', u'q90_score2', u'q70_score2',\n",
    "       u'q50_score2', u'q30_score2', u'max_score3', u'avg_score3',\n",
    "       u'q95_score3', u'q90_score3', u'q70_score3', u'q50_score3',\n",
    "       u'q30_score3', u'emailru', u'mobile_share', u'vk_share',\n",
    "       u'social_share', u'work_hours_hits_share', u'hour_std',\n",
    "       u'good_urlfr_share_score1', u'good_urlfr_share_score2',\n",
    "       u'good_urlfr_share_score3', u'good_urlfr_timestd_score1',\n",
    "       u'max_urlfr_time_std_1', u'yaca_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### ND таблица считается так сложно, т.к. поле default_flg зачастую нулевое и выбрасывает ошибку, что не может определить тип поля по первым 100 строкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o258.collectToPython.\n: org.apache.spark.SparkException: Job 8 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1731)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1730)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:147)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply$mcI$sp(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.collectToPython(DataFrame.scala:1777)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d0b908e71d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'default_flg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         .map(lambda r: [r[0] if r[0] else None] + list(r[1:16]) + (r[16] if r[16] else []) + list(r[17:19]) + (r[19] if r[19] else []) \n\u001b[0m\u001b[0;32m      9\u001b[0m                + list(r[20:22]) + (r[22] if r[22] else [])  + list(r[26:35]) + list(r[36:]))\n\u001b[0;32m     10\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \"\"\"\n\u001b[0;32m   1377\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m     \u001b[1;31m##########################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o258.collectToPython.\n: org.apache.spark.SparkException: Job 8 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1731)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1730)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:147)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply$mcI$sp(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.collectToPython(DataFrame.scala:1777)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#raw_cols  = hc.sql('select * from  user_kposminin.cred_app_scoring_2 limit 1').collect()[0].__fields__\n",
    "#hc.sql('set hive.support.quoted.identifiers=none')\n",
    "df_all = (hc.sql('select * from user_kposminin.cred_app_scoring_2 a')\n",
    "        .repartition(20)\n",
    "        .fillna(-1, subset = ['default_flg'])\n",
    "        .rdd\n",
    "        .map(lambda r: [r[0] if r[0] else None] + list(r[1:16]) + (r[16] if r[16] else []) + list(r[17:19]) + (r[19] if r[19] else []) \n",
    "               + list(r[20:22]) + (r[22] if r[22] else [])  + list(r[26:35]) + list(r[36:]))\n",
    "        .toDF()\n",
    "        .toPandas()\n",
    "         )\n",
    "#hc.sql('set hive.support.quoted.identifiers=column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def try_float(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_all = (sc.textFile('/user/hive/warehouse/user_kposminin.db/cred_app_scoring_2')\n",
    "        .map(lambda r: r.split('\\x01'))\n",
    "        .map(lambda r: [r[0] if r[0] else None] + list(r[1:16]) + (r[16].split('\\x02') if r[16] else []) + list(r[17:19]) \n",
    "             + (r[19].split('\\x02') if r[19] else []) + list(r[20:22]) + (r[22].split('\\x02') if r[22] else [])  + list(r[26:35]) + list(r[36:]))\n",
    "        .map(lambda r: list(r[:4]) + [try_float(e) for e in r[4:-1]] + [r[-1]])\n",
    "        .collect()\n",
    "         )\n",
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_cols  = hc.sql('select * from  user_kposminin.cred_app_scoring_2 limit 1').collect()[0].__fields__\n",
    "df_all = (hc.sql('select * from user_kposminin.cred_app_scoring_2')        \n",
    "        .map(lambda r: [r[0] if r[0] else None] + list(r[1:16]) + (r[16] if r[16] else []) + list(r[17:19]) + (r[19] if r[19] else []) \n",
    "               + list(r[20:22]) + (r[22] if r[22] else [])  + list(r[26:35]) + list(r[36:]))\n",
    "        .toDF()\n",
    "        .toPandas()\n",
    "         )\n",
    "df_all.columns = ['default_flg','financial_product_type_cd'] + cols \n",
    "feat_cols = df_all.columns[4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_all = (hc.sql('select * from user_kposminin.cred_app_scoring_2').rdd)\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_cols = df_all.columns[4:-1]\n",
    "#label     = 'approve'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Факторы Я.каталога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# cPickle.dump(v,open('data/ccall_scoring_dict_vectorizer','w'))\n",
    "v1 = cPickle.load(open('data/ccall_scoring_dict_vectorizer','r'))\n",
    "type(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train_all_yaca_dense = v1.fit_transform(df_train_all['yaca_str'].map(lambda s: { kv.split(':')[0]:float(kv.split(':')[1])  for kv in s.split(' ')} if s else {}))\n",
    "df_all_yaca_dense = v1.transform(df_all['yaca_str'].map(lambda s: { kv.split(':')[0]:float(kv.split(':')[1])  for kv in s.split(' ')} if s else {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yaca_cols = ['yaca_{}'.format(i) for i in range(df_all_yaca_dense.shape[1])]\n",
    "for i in range(df_all_yaca_dense.shape[1]):\n",
    "    #df_train_all.loc[:,'yaca_{}'.format(i)] = df_train_all_yaca_dense[:,i]\n",
    "    df_all.loc[:,'yaca_{}'.format(i)] = df_all_yaca_dense[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "feat_cols_w_yaca = feat_cols.tolist() + yaca_cols\n",
    "#dtrain_all_yaca = xgb.DMatrix( df_train_all[feat_cols_w_yaca], label=df_train_all['approve'], missing = np.nan)\n",
    "dall_yaca = xgb.DMatrix( df_all[feat_cols_w_yaca], missing = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "bst1 = cPickle.load(open('data/ccall_scoring_xgb.model','r'))\n",
    "type(bst1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.loc[:,'pred'] = bst1.predict(dall_yaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('Prediction of default. All products')\n",
    "print('AUC ROC on all  data: {:.5f}'.format(sklearn.metrics.roc_auc_score(\n",
    "            y_true = df_all['default_flg'],\n",
    "            y_score = - df_all['pred'])\n",
    "))\n",
    "\n",
    "print('AUC ROC on 2015 data: {:.5f}'.format(sklearn.metrics.roc_auc_score(\n",
    "            y_true = df_all[df_all['call_ymd'] < '2016-01-01']['default_flg'],\n",
    "            y_score = - df_all.loc[df_all['call_ymd'] < '2016-01-01','pred'])\n",
    "))\n",
    "\n",
    "print('AUC ROC on 2016 data: {:.5f}'.format(sklearn.metrics.roc_auc_score(\n",
    "            y_true = df_all[df_all['call_ymd'] >= '2016-01-01']['default_flg'],\n",
    "            y_score = - df_all.loc[df_all['call_ymd'] >= '2016-01-01','pred'])\n",
    "))\n",
    "\n",
    "print('PR ROC on all  data: {:.5f}'.format(sklearn.metrics.average_precision_score(\n",
    "            y_true = df_all['default_flg'],\n",
    "            y_score = - df_all['pred'])\n",
    "))\n",
    "\n",
    "print('PR ROC on 2015 data: {:.5f}'.format(sklearn.metrics.average_precision_score(\n",
    "            y_true = df_all[df_all['call_ymd'] < '2016-01-01']['default_flg'],\n",
    "            y_score = - df_all.loc[df_all['call_ymd'] < '2016-01-01','pred'])\n",
    "))\n",
    "\n",
    "print('PR ROC on 2016 data: {:.5f}'.format(sklearn.metrics.average_precision_score(\n",
    "            y_true = df_all[df_all['call_ymd'] >= '2016-01-01']['default_flg'],\n",
    "            y_score = - df_all.loc[df_all['call_ymd'] >= '2016-01-01','pred'])\n",
    "))\n",
    "print('Avg default rate on all  data: {:.5f}'.format(df_all['default_flg'].mean()))\n",
    "print('Avg default rate on 2015 data: {:.5f}'.format(df_all[df_all['call_ymd'] < '2016-01-01']['default_flg'].mean()))\n",
    "print('Avg default rate on 2016 data: {:.5f}'.format(df_all[df_all['call_ymd'] >= '2016-01-01']['default_flg'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Prediction of default. Per product. All periods.')\n",
    "print('AUC ROC on CCR all years: {:.5f}'.format(sklearn.metrics.roc_auc_score(\n",
    "            y_true = df_all[df_all['financial_product_type_cd'] == 'CCR']['default_flg'],\n",
    "            y_score = - df_all[df_all['financial_product_type_cd'] == 'CCR']['pred'])\n",
    "))\n",
    "print('PR ROC on CCR all years: {:.5f}'.format(sklearn.metrics.average_precision_score(\n",
    "            y_true = df_all[df_all['financial_product_type_cd'] == 'CCR']['default_flg'],\n",
    "            y_score = - df_all[df_all['financial_product_type_cd'] == 'CCR']['pred'])\n",
    "))\n",
    "print('Avg default rate on CCR all years: {:.5f}'.format(df_all[df_all['financial_product_type_cd'] == 'CCR']['default_flg'].mean()))\n",
    "\n",
    "print('AUC ROC on LON all years: {:.5f}'.format(sklearn.metrics.roc_auc_score(\n",
    "            y_true = df_all[df_all['financial_product_type_cd'] == 'LON']['default_flg'],\n",
    "            y_score = - df_all[df_all['financial_product_type_cd'] == 'LON']['pred'])\n",
    "))\n",
    "print('PR ROC on LON all years: {:.5f}'.format(sklearn.metrics.average_precision_score(\n",
    "            y_true = df_all[df_all['financial_product_type_cd'] == 'LON']['default_flg'],\n",
    "            y_score = - df_all[df_all['financial_product_type_cd'] == 'LON']['pred'])\n",
    "))\n",
    "print('Avg default rate on LON all years: {:.5f}'.format(df_all[df_all['financial_product_type_cd'] == 'LON']['default_flg'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.default_flg.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.registerDataFrameAsTable(hc.createDataFrame(df_all), 'df_all')\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'user_kposminin.default_az_cred_scor_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hc.registerDataFrameAsTable(hc.createDataFrame(df_all), 'df_all')\n",
    "hc.sql('drop table if exists big_data_science.default_az_cred_scor_result')\n",
    "hc.sql('create table big_data_science.default_az_cred_scor_result as select * from df_all')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_all.to_csv('data/default_az_cred_scor_result.csv',index = False,columns = False,header=False)\n",
    "df_all.default_flg.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Почему-то спарк неограниченно долго работает, но по факту таблица записывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
