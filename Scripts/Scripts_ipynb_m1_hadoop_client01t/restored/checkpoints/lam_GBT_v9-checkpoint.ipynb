{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.setCheckpointDir('/user/kposminin/checkpointdir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def repart(filename):\n",
    "    starttime = datetime.datetime.now()\n",
    "    sc.textFile(filename).repartition(32*8).saveAsTextFile('.'.join(filename.split('.')[:-1]))\n",
    "    print('End. Time of work {0}.'.format(datetime.datetime.now() - starttime))\n",
    "#repart(\"/user/kposminin/la_app_20160817_1.txt\")\n",
    "#repart(\"/user/kposminin/la_app_20160818_1.txt\")\n",
    "#repart(\"/user/kposminin/la_app_20160824_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(table):\n",
    "    si = 15 #score start index\n",
    "    def top_avg_score(slist): \n",
    "        return [sum(slist[:i])/i for i in [2,3,4,5,7,10]]\n",
    "    return [r + top_avg_score(r[si:si+11]) for r in table] \n",
    "\n",
    "def add_feature_rdd(row):\n",
    "    si = 15 #score start index\n",
    "    def top_avg_score(slist): \n",
    "        return [sum(slist[:i])/i for i in [2,3,4,5,7,10]]\n",
    "    r = row\n",
    "    return r + top_avg_score(r[si:si+11])\n",
    "    \n",
    "    \n",
    "# Load and parse the data file.\n",
    "# Load and parse the data file.\n",
    "train = sc.textFile(\"/user/kposminin/la_20160817_3.txt\") \\\n",
    "  .filter(lambda s: (s[0] == '1') or (hash(s) % 500 == 0)) \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \\\n",
    "  .collect()\n",
    "\n",
    "test = sc.textFile(\"/user/kposminin/la_20160824_3.txt\") \\\n",
    "  .filter(lambda s: (s[0] == '1') or (hash(s) % 500 == 16)) \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \\\n",
    "  .collect()\n",
    "\n",
    "test_rdd = sc.textFile(\"/user/kposminin/la_20160824_3.txt\") \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \n",
    "\n",
    "test_rdd2 = sc.textFile(\"/user/kposminin/la_20160818_3.txt\") \\\n",
    "  .map(lambda r:r.split('\\t')) \\\n",
    "  .map(lambda r:[int(e) for e in r]) \\\n",
    "  .filter(lambda r: len(r) == 30) \\\n",
    "  .map(add_feature_rdd) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = '''smax ,savg ,ssum ,smedian ,sstd ,cntrepeat ,cntuniq \n",
    ",duration , has_scores, mobile ,emailru ,vkru ,okru ,social_other , s1 ,s2 ,s3 ,s4 ,s5 ,s6 ,s7 ,s8 ,s9 ,s10 , \n",
    "sm1 ,sm2 ,sm3 ,sm4 ,sm5, avg2, avg3,avg4,avg5,avg7,avg10'''.replace(' ','').replace('\\n','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score  AUCROC on sampled test data 0.572428257731\n"
     ]
    }
   ],
   "source": [
    "aucroc_smax = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[5] for e in test]\n",
    "    )\n",
    "print('Max score  AUCROC on sampled test data {0}'.format(aucroc_smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test AUC ROC 0.83329218359\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(test_rdd.map(lambda r: (float(r[7]),float(r[0]))))\n",
    "print('Full test AUC ROC {0}'.format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test 20160818 AUC ROC 0.843819806579\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(test_rdd2.map(lambda r: (float(r[1]),float(r[0]))))\n",
    "print('Full test 20160818 AUC ROC {0}'.format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Варьируем размер семплирования  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9625 ['test on 9625-model', 0.88390849940370542]\n",
      "17909 ['test on 17909-model', 0.87325036756241925]\n",
      "34928 ['test on 34928-model', 0.88015456731374841]\n",
      "68443 ['test on 68443-model', 0.87293689138753006]\n",
      "169333 ['test on 169333-model', 0.87918543359528312]\n",
      "337253 ['test on 337253-model', 0.87202237496549706]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelGBT = {}\n",
    "AUCROC=[]\n",
    "\n",
    "for f in [40,20,10,5,2,1]:\n",
    "    train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*f) == 0]\n",
    "    s = len(train1)\n",
    "    modelGBT[s] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=2000, learning_rate=0.04,\n",
    "       max_depth=3, random_state=0).fit(X = [e[1:] for e in train1], y = [e[0] for e in train1])\n",
    "    AUCROC.append(['test on {0}-model'.format(s),sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    #AUCROC.append(['train '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "    #    y_true = [e[0] for e in train1], \n",
    "    #    y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in train1])]\n",
    "    #)])    \n",
    "    print('{0} {1}'.format(s,AUCROC[-1]))\n",
    "\n",
    "AUCROC.append(['smax '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[1] for e in test]\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-709e9563b4a0>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-709e9563b4a0>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    (n_estimators=200, learning_rate=0.04,\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "modelGBT = {}\n",
    "AUCROC=[]\n",
    "\n",
    "for f in [40,20,10,5,2,1]:\n",
    "    train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*f) == 0]\n",
    "    s = len(train1)\n",
    "    modelGBT[s] = sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, \n",
    "                                oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "    AUCROC.append(['test on {0}-model'.format(s),sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in test])]\n",
    "    )])\n",
    "    #AUCROC.append(['train '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "    #    y_true = [e[0] for e in train1], \n",
    "    #    y_score = [r[1] for r in modelGBT[s].predict_proba([e[1:] for e in train1])]\n",
    "    #)])    \n",
    "    print('{0} {1}'.format(s,AUCROC[-1]))\n",
    "\n",
    "AUCROC.append(['smax '+ str(s), sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[1] for e in test]\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860640750085\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "m = sklearn.linear_model.LogisticRegression(penalty='l1', class_weight = {0:0.015,1:0.985}) \\\n",
    "    .fit(X = [e[1:] for e in train], y = [e[0] for e in train])\n",
    "aucroc = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in m.predict_proba([e[1:] for e in test])]\n",
    "    )\n",
    "print(aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print('AUCROC score from sample number:\\n' + '\\n'.join(['{0} {1:.5f}'.format(k,v) for (k,v) in sorted(AUCROC.items())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUCROC score from sample number:\\n' + '\\n'.join(['{0} {1:.5f}'.format(*e) for e in AUCROC])) #800 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1 = [r for r in train if (r[0] == 1) or int(np.random.rand()*5) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 26.0 failed 4 times, most recent failure: Lost task 4.3 in stage 26.0 (TID 1178, m1-hadoop-wk10t.tcsbank.ru): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-49-aec1d63755bd>\", line 1, in <lambda>\nKeyError: 77856\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-49-aec1d63755bd>\", line 1, in <lambda>\nKeyError: 77856\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-aec1d63755bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_rdd2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodelGBT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m77856\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodelGBT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5042\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mAUCROC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_smax2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_full\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_full\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUCROC_full_smax {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUCROC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_smax2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 26.0 failed 4 times, most recent failure: Lost task 4.3 in stage 26.0 (TID 1178, m1-hadoop-wk10t.tcsbank.ru): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-49-aec1d63755bd>\", line 1, in <lambda>\nKeyError: 77856\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/disk12/yarn/nm/usercache/k.p.osminin/appcache/application_1470830020606_3255/container_1470830020606_3255_01_000031/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-49-aec1d63755bd>\", line 1, in <lambda>\nKeyError: 77856\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "res_full = test_rdd2.map(lambda r:(r[0],int(10**4 * modelGBT[77856].predict_proba(r[1:])[0][1]),int(10**4 * modelGBT[5042].predict_proba(r[1:])[0][1]),r[1])).collect()\n",
    "AUCROC['full_smax2'] = sklearn.metrics.roc_auc_score(y_true = [e[0] for e in res_full], y_score = [e[3] for e in res_full])\n",
    "print('AUCROC_full_smax {0}'.format(AUCROC['full_smax2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AUCROC['full_GBT12'] = sklearn.metrics.roc_auc_score(y_true = [e[0] for e in res_full], y_score = [e[1] for e in res_full])\n",
    "print('AUCROC_full_GBT1 {0}'.format(AUCROC['full_GBT12']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AUCROC['full_GBT22'] = sklearn.metrics.roc_auc_score(y_true = [e[0] for e in res_full], y_score = [e[2] for e in res_full])\n",
    "print('AUCROC_full_GBT2 {0}'.format(AUCROC['full_GBT22']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#open('res_aucroc.txt','w').write(str(AUCROC))\n",
    "#sorted(zip(modelGBT[5042].feature_importances_,columns))\n",
    "print('AUCROC score:\\n' + '\\n'.join(['{0} {1:.5f}'.format(k,v) for (k,v) in sorted(AUCROC.items())]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(modelGBT[5042],open('la_modelGBT5042.pckl','w'))\n",
    "#pickle.dump(modelGBT[77856],open('la_modelGBT77856.pckl','w'))\n",
    "#m = pickle.load(open('la_modelGBT5042.pckl','r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_table_to_file(table, filename):\n",
    "    f = open(filename,'w+')\n",
    "    #f.write('label,' + ','.join(columns)+'\\n')\n",
    "    f.write('\\n'.join([','.join([str(e) for e in r]) for r in table]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1 = train\n",
    "train = [r for r in train if (r[0] == 1) or int(np.random.rand() * 5) == 0]\n",
    "test1 = [r for r in test if (r[0] == 1) or int(np.random.rand()*5) == 0]\n",
    "len(train1),len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = add_features(train)\n",
    "test1 = test\n",
    "test = add_features([r for r in test if (r[0] == 1) or int(np.random.rand()*5) == 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Варьируем количество деревьев "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "import sklearn.ensemble\n",
    "import sklearn\n",
    "modelGBT = {}\n",
    "AUCROC1={}\n",
    "for n in [30,50,80,100,200,500,1000]:\n",
    "    print(n)\n",
    "    modelGBT[n] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=n, learning_rate=0.1,\n",
    "       max_depth=2, random_state=0).fit(X = [e[1:] for e in train], y = [e[0] for e in train])\n",
    "    AUCROC1[n] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[n].predict_proba([e[1:] for e in test])]\n",
    "    )\n",
    "    AUCROC1['train '+str(n)] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in train], \n",
    "        y_score = [r[1] for r in modelGBT[n].predict_proba([e[1:] for e in train])]\n",
    "    )\n",
    "    print(AUCROC1[n])\n",
    "AUCROC1['smax'] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [e[1] for e in test]\n",
    ")\n",
    "AUCROC1['train smax'] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in train], \n",
    "        y_score = [e[1] for e in train]\n",
    ")\n",
    "#print(AUCROC)\n",
    "# Evaluate model on test instances and compute test error\n",
    "##predictions = model.predict(test.map(lambda x: x.features))\n",
    "#predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUCROC test sample score from GBT trees number:\\n' + '\\n'.join(['{0} {1:.5f}'.format(k,v) for (k,v) in sorted(AUCROC1.items())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Вариация глубины дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelGBT = sklearn.ensemble.GradientBoostingClassifier(n_estimators=50, learning_rate=0.1,\n",
    "       max_depth=6, random_state=0).fit(X = [e[1:] for e in train], y = [e[0] for e in train])\n",
    "print(sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT.predict_proba([e[1:] for e in test])]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelGBT = {}\n",
    "AUCROC1 = {'smax':AUCROC1['smax'], 'train smax':AUCROC1['train smax']}\n",
    "for m in [1,2,3,4,6]:    \n",
    "    modelGBT[m] = sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "       max_depth=m, random_state=0).fit(X = [e[1:] for e in train], y = [e[0] for e in train])\n",
    "    AUCROC1[m] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in test], \n",
    "        y_score = [r[1] for r in modelGBT[m].predict_proba([e[1:] for e in test])]\n",
    "    )\n",
    "    AUCROC1['train '+str(m)] = sklearn.metrics.roc_auc_score(\n",
    "        y_true = [e[0] for e in train], \n",
    "        y_score = [r[1] for r in modelGBT[m].predict_proba([e[1:] for e in train])]\n",
    "    )\n",
    "    print('{0} {1}'.format(m,AUCROC1[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUCROC test sample score from GBT tree depth:\\n' + '\\n'.join(['{0} {1:.5f}'.format(k,v) for (k,v) in sorted(AUCROC1.items())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### До глубины 7 разница несущественна, далее начинается переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = test_rdd.map(lambda r:(float(r[0]),modelGBT.predict_proba(r[1:])[0][1])).collect()\n",
    "ar = sklearn.metrics.roc_auc_score(y_true = [e[0] for e in res1], y_score = [e[1] for e in res1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open('test.txt','w').write(str(AUCROC)+','+str(ar)+'\\n'+str(sorted(zip(columns,modelGBT.feature_importances_), key = lambda r:-r[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification GBT model:')\n",
    "print(model.toDebugString())\n",
    "print('AUCROC: {0}'.format(sklearn.metrics.roc_auc_score(\n",
    "            labelsAndPredictions.map(lambda r:r[0]).collect(),\n",
    "            labelsAndPredictions.map(lambda r:r[1]).collect()\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUCROC smax: {0}'.format(sklearn.metrics.roc_auc_score(\n",
    "            test.map(lambda lp: lp.label).collect(),\n",
    "            test.map(lambda lp: lp.features[0]).collect()\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(['{0} {1:.5f}'.format(k,v) for (k,v) in sorted(zip(columns,modelGBT[5].feature_importances_), key = lambda r:-r[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in range(200)])\n",
    "#p1(0,k,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4927027656760324, 0.36342215660643273)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(0,23,365),p(1,23,365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4927027656760144, 0.3634221566065063)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1(0,23,365),p1(1,23,365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_coinc_prob(2500000,37000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P >= 2: 0.157836820959\n",
      "P >= 3: 4.67614449378e-05\n",
      "P >= 4: 1.21569420086e-08\n"
     ]
    }
   ],
   "source": [
    "p=1500./180000000\n",
    "n = 100\n",
    "N = 2500000*20/n\n",
    "print('P >= 2: {0}'.format(1 - ((1-p)**n + n*p*(1-p)**(n-1))**N))\n",
    "print('P >= 3: {0}'.format(1 - ((1-p)**n + n*p*(1-p)**(n-1) + n*(n-1)/2*(p**2)*(1-p)**(n-2))**N))\n",
    "print('P >= 4: {0}'.format(1 - ((1-p)**n + n*p*(1-p)**(n-1) + n*(n-1)/2*(p**2)*(1-p)**(n-2) + n*(n-1)*(n-2)/6*(p**3)*(1-p)**(n-3))**N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.516735506621864e-08"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
