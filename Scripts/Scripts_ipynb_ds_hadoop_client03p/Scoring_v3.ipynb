{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Кредитный скоринг\".\n",
    "#### Подход Андрея. Мешок слов из триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "\n",
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 20)\n",
    "        .set(\"spark.driver.maxResultSize\", \"16g\")\n",
    "        .set('spark.driver.memory','16g')\n",
    "        .set(\"spark.executor.memory\", '8g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)        \n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def transliterate(string):\n",
    "    tr = {u'а': u'a',\n",
    "           u'б': u'b',\n",
    "           u'в': u'v',\n",
    "           u'г': u'g',\n",
    "           u'д': u'd',\n",
    "           u'е': u'e',\n",
    "           u'ё': u'e',\n",
    "           u'ж': u'zh',\n",
    "           u'з': u'z',\n",
    "           u'и': u'i',\n",
    "           u'й': u'y',\n",
    "           u'к': u'k',\n",
    "           u'л': u'l',\n",
    "           u'м': u'm',\n",
    "           u'н': u'n',\n",
    "           u'о': u'o',\n",
    "           u'п': u'p',\n",
    "           u'р': u'r',\n",
    "           u'с': u's',\n",
    "           u'т': u't',\n",
    "           u'у': u'u',\n",
    "           u'ф': u'f',\n",
    "           u'х': u'h',\n",
    "           u'ц': u'ts',\n",
    "           u'ч': u'ch',\n",
    "           u'ш': u'sh',\n",
    "           u'щ': u'sch',\n",
    "           u'ъ': u'',\n",
    "           u'ы': u'y',\n",
    "           u'ь': u'',\n",
    "           u'э': u'e',\n",
    "           u'ю': u'yu',\n",
    "           u'я': u'ya',}\n",
    "\n",
    "    for k in tr:\n",
    "        string = string.replace(k, tr[k])\n",
    "        \n",
    "    return string\n",
    "\n",
    "def tri(s, cnt,d):\n",
    "    s = transliterate(s.lower())\n",
    "    prefix = 'D'\n",
    "    for i in range(len(s)-2):\n",
    "        if s[i] == '#':\n",
    "            prefix = 'U'\n",
    "            continue\n",
    "        k = prefix + s[i:i+3]\n",
    "        #if not k.isalnum():\n",
    "        #if not k.isalpha():\n",
    "        #    continue\n",
    "        d[k] = d[k] + cnt if k in d else cnt\n",
    "    return d\n",
    "\n",
    "\n",
    "abc = u\"abcdefghijklmnopqrstuvwxyz0123456789 !#%&'*+,-.:=_|\"\n",
    "\n",
    "def ngram_index(ngr,abc):\n",
    "    n = len(abc) + 1\n",
    "    return sum((abc.index(ngr[i]) if ngr[i] in abc else n) * ((n+1) ** (i-1)) for i in range(1,len(ngr))) + ((n+1) ** (len(ngr)-1)) * (ngr[0] == 'U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `scoring_train_andrey': No such file or directory\n",
      "rm: `scoring_test_andrey': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#user_kposminin.ccall_scoring_train \n",
    "#user_kposminin.ccall_scoring_train \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "table_create_query = '''\n",
    "create table ccall_scoring_train as\n",
    "select \n",
    "  phone_mobile, \n",
    "  call_ymd,\n",
    "  max(approve) as approve,\n",
    "  (unix_timestamp(max(ymd), 'yyyy-MM-dd') - unix_timestamp(min(ymd), 'yyyy-MM-dd'))/60/60/24 as ymd_range,\n",
    "  stddev(unix_timestamp(ymd, 'yyyy-MM-dd')/60/60 + avg_hour) as time_std,\n",
    "  count(distinct ymd) as ymd_cnt,\n",
    "  count(distinct id) as id_cnt,\n",
    "  avg(avg_hour) as avg_hour,\n",
    "  percentile_approx(avg_hour,0.1) as avg_hour_q10,\n",
    "  percentile_approx(avg_hour,0.9) as avg_hour_q90,\n",
    "  urlfr,\n",
    "  count(*) as cnt,\n",
    "  sum(cnt) as hits,\n",
    "  avg(duration) as avg_duration\n",
    "\n",
    "from \n",
    "  user_kposminin.ccall_visits v\n",
    "where\n",
    "  call_ymd > ymd and call_ymd < date_add(ymd,180)\n",
    "group by\n",
    "  phone_mobile, \n",
    "  call_ymd,\n",
    "  urlfr\n",
    "'''\n",
    "\n",
    "select_train_query = '''\n",
    "select phone_mobile, call_ymd, approve, urlfr, cnt\n",
    "from user_kposminin.ccall_scoring_train\n",
    "'''\n",
    "\n",
    "! hadoop fs -rm -r scoring_train_andrey\n",
    "! hadoop fs -rm -r scoring_test_andrey\n",
    "\n",
    "#Vowpal wabbit format\n",
    "\n",
    "df_train = (hc.sql(select_train_query)\n",
    "            .rdd\n",
    "            .filter(lambda row: (row['call_ymd'] < '2016-12-01'))\n",
    "            .map(lambda row: (row[:3],Counter(tri(row['urlfr'],row['cnt'],{}))))\n",
    "            .reduceByKey(lambda v1,v2: v1 + v2)\n",
    "            .map(lambda (k,v): (k,sorted([(ngram_index(vk,abc),vv) for vk,vv in v.items()])))\n",
    "            .map(lambda (k,v): ('1' if k[2] == 1 else '-1') + ' | ' + ' '.join('{}:{}'.format(*e) for e in v))\n",
    "            .saveAsTextFile('scoring_train_andrey')\n",
    "            )\n",
    "\n",
    "df_test = (hc.sql(select_train_query)\n",
    "            .rdd\n",
    "            .filter(lambda row: (row['call_ymd'] >= '2016-12-01'))\n",
    "            .map(lambda row: (row[:3],Counter(tri(row['urlfr'],row['cnt'],{}))))\n",
    "            .reduceByKey(lambda v1,v2: v1 + v2)\n",
    "            .map(lambda (k,v): (k,sorted([(ngram_index(vk,abc),vv) for vk,vv in v.items()])))\n",
    "            .map(lambda (k,v): ('1' if k[2] == 1 else '-1') + ' | ' + ' '.join('{}:{}'.format(*e) for e in v))\n",
    "            .saveAsTextFile('scoring_test_andrey')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Дальше в LightGBM\n",
    "Лучший результат: [LightGBM] [Info] Iteration: 5, training AUC : 0.608894; test : AUC : 0.628938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Данные:\n",
    "* обучающая выборка ds-clickhouse01t:/data1/share/kosm/data/scoring_train_andrey_libsvm.txt\n",
    "* тестирующая выборка ds-clickhouse01t:/data1/share/kosm/data/scoring_test_andrey_libsvm.txt\n",
    "* конфиг файл LightGBM ds-clickhouse01t:/home/k.osminin/scoring_andrey_lgbm_files/train.conf (брался из расчета Андрея без изменений)\n",
    "* Результат расчета ds-clickhouse01t:/home/k.osminin/scoring_andrey_lgbm_files/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
