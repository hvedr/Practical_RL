{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текстовый анализ URL в задаче lookalike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"8g\").set('spark.driver.memory','8g')\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "train_date = datetime.date(2016,11,8)\n",
    "n = 4\n",
    "\n",
    "test_date = train_date + datetime.timedelta(days = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hive queries\n",
    "\n",
    "    \n",
    "train_sample_query = '''\n",
    "\n",
    "CREATE FUNCTION md5 as 'onemd5.Md5';\n",
    "\n",
    "create table if not exists user_kposminin.url_text_#ind as\n",
    "select \n",
    "  m.phone_num,\n",
    "  max(if(u.id is Null,0,1)) as label,\n",
    "  max(nvl(u.first_day,0)) as first_day,\n",
    "  split(concat_ws(' ',collect_list(url)),'[ /\\\\\\\\-=_\\\\\\\\?\\\\\\\\.]') as up_bow,\n",
    "  concat_ws(' ',collect_list(url)) as up\n",
    "from \n",
    "  (select \n",
    "       uid_str as id,\n",
    "       property_value as phone_num\n",
    "     from\n",
    "       prod_dds.md_uid_property \n",
    "     where\n",
    "       property_cd = 'PHONE' and\n",
    "       load_src = 'LI.02'\n",
    "  ) m\n",
    "  inner join prod_raw_liveinternet.access_log v on m.id = v.id\n",
    "  left join(\n",
    "    select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "    from prod_features_liveinternet.user_action\n",
    "    where action_type = 'tinkoff_platinum_approved_application'\n",
    "      and ymd between '#ymd1' and '#ymd3'\n",
    "  ) u on u.id = v.id\n",
    "where\n",
    "  v.ymd = '#ymd0' and \n",
    "  (substr(md5(v.id),1,2) = '00' or not u.id is Null)\n",
    "group by \n",
    "  m.phone_num\n",
    ";\n",
    "select * from user_kposminin.url_text_#ind\n",
    "\n",
    "'''.replace('#ymd0',str(train_date)) \\\n",
    "   .replace('#ymd1',str(train_date + datetime.timedelta(days = 1))) \\\n",
    "   .replace('#ymd3',str(train_date + datetime.timedelta(days = 3))) \\\n",
    "   .replace('#ind',str(train_date).replace('-',''))\n",
    "\n",
    "    \n",
    "test_full_query = '''\n",
    "create table if not exists user_kposminin.url_text_#ind as\n",
    "select \n",
    "  m.phone_num,\n",
    "  max(if(u.id is Null,0,1)) as label,\n",
    "  max(nvl(u.first_day,0)) as first_day,\n",
    "  split(concat_ws(' ',collect_list(url)),'[ /\\\\\\\\-=_\\\\\\\\?\\\\\\\\.]') as up_bow,\n",
    "  concat_ws(' ',collect_list(url)) as up\n",
    "from \n",
    "  (select \n",
    "       uid_str as id,\n",
    "       property_value as phone_num\n",
    "     from\n",
    "       prod_dds.md_uid_property \n",
    "     where\n",
    "       property_cd = 'PHONE' and\n",
    "       load_src = 'LI.02'\n",
    "  ) m\n",
    "  inner join prod_raw_liveinternet.access_log v on m.id = v.id\n",
    "  left join(\n",
    "    select distinct id, if(ymd = '#ymd1',1,0) as first_day\n",
    "    from prod_features_liveinternet.user_action\n",
    "    where action_type = 'tinkoff_platinum_approved_application'\n",
    "      and ymd between '#ymd1' and '#ymd3'\n",
    "  ) u on u.id = v.id\n",
    "where\n",
    "  v.ymd = '#ymd0' \n",
    "group by \n",
    "  m.phone_num\n",
    ";\n",
    "select * from user_kposminin.url_text_#ind\n",
    "\n",
    "'''.replace('#ymd0',str(test_date)) \\\n",
    "   .replace('#ymd1',str(test_date + datetime.timedelta(days = 1))) \\\n",
    "   .replace('#ymd3',str(test_date + datetime.timedelta(days = 3))) \\\n",
    "   .replace('#ind',str(test_date).replace('-',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translate ru to eng, Transform text to n_gram list, Get n_gram index\n",
    "\n",
    "#abc = list(set(''.join([e[0] for e in hc.sql('select url from prod_raw_liveinternet.access_log v where ymd = \"2017-01-10\" limit 100000').collect()])))\n",
    "abc = list(u'abcdefghijklmnopqrstuvwxyz0123456789 %&-?_абвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
    "tr_abc_len = abc.index(u'а') - 1\n",
    "\n",
    "symbols = (u\"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\",\n",
    "           u\"abvgdeejzijklmnoprstufhzcss_y_euaABVGDEEJZIJKLMNOPRSTUFHZCSS_Y_EUA\")\n",
    "\n",
    "transl = {ord(a):ord(b) for a, b in zip(*symbols)}\n",
    "\n",
    "def n_gram(s, n):\n",
    "    '''Returns n-gram list from string s.'''\n",
    "    return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "def n_gram_index(ngr,abc):\n",
    "    '''Returns index of n-gram ngr. ngr chars must be from abc list'''\n",
    "    N = tr_abc_len\n",
    "    ind = 0\n",
    "    \n",
    "    for i in range(len(ngr)):\n",
    "        try:\n",
    "            j = abc.index(ngr[i].lower())\n",
    "            if j > N:\n",
    "                j = abc.index(ngr[i].lower().translate(transl))\n",
    "            ind += (N ** i) * j\n",
    "        except ValueError:\n",
    "            ind += (N ** i) * (N - 1)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calc in Spark\n",
    "for q in train_sample_query.split(';')[:-1] + test_full_query.split(';')[:-1]:\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train = hc.sql(train_sample_query.split(';')[-1]) \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: [r.label, r.first_day, reduce(lambda a,b:a+b,[n_gram(e,n) for e in r.up_bow])]) \\\n",
    "        .map(lambda r: LabeledPoint(float(r[0]),SparseVector(tr_abc_len ** n,{n_gram_index(e,abc):1.0 for e in list(set(r[2]))})))\n",
    "\n",
    "test_full  = hc.sql(test_full_query.split(';')[-1]) \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: [r.label, r.first_day, reduce(lambda a,b:a+b,[n_gram(e,n) for e in r.up_bow])]) \\\n",
    "        .map(lambda r: LabeledPoint(float(r[0]),SparseVector(tr_abc_len ** n,{n_gram_index(e,abc):1.0 for e in list(set(r[2]))})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calc local\n",
    "'''\n",
    "for q in train_sample_query.split(';')[:-1] + test_sample_query.split(';')[:-1]:\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train = hc.sql(train_sample_query.split(';')[-1]) \\\n",
    "        .collect()\n",
    "\n",
    "test  = hc.sql(test_sample_query.split(';')[-1]) \\\n",
    "        .collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = hc.sql('select * from user_kposminin.url_text_20161108_2') \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: [r.label, r.first_day, reduce(lambda a,b:a+b,[n_gram(e,n) for e in r.up_bow])]) \\\n",
    "        .map(lambda r: (r[0],SparseVector(tr_abc_len ** n,{n_gram_index(e,abc):1.0 for e in set(r[2])}))) \\\n",
    "        .toDF() \\\n",
    "        .write.saveAsTable(\"user_kposminin.url_text_20161108_6\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LogisticRegression model\n",
    "modelLR = LogisticRegressionWithSGD.train(train,iterations = 10)\n",
    "modelLR.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreAndLabels = test_full \\\n",
    "                 .map(lambda lp: (modelLR.predict(lp.features),lp.label))\n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The AUC ROC score on full test data for {0}-grams is ): {1}\".format(n,metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = test \\\n",
    "                     .map(lambda r: (float(modelLR.predict(r.features)),float(r.label))) \\\n",
    "                     .toDF() \\\n",
    "                     .toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "print('AUC ROC {0}'.format(            \n",
    "            sk.metrics.roc_auc_score(y_true = df_test.iloc[:,1].astype('int'), y_score = df_test.iloc[:,0])\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
