{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Построение Look-alike модели для целевой аудитории раздела веб-сайта\n",
    "\n",
    "Используются различные подходы:\n",
    "- Логистическая регрессия\n",
    "- Naive Bayes\n",
    "- текущий подход, рассмотренный в Wiki[https://wiki.tcsbank.ru/pages/viewpage.action?pageId=176096365].\n",
    "\n",
    "Сравнение методов производится по метрике AUC ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)\n",
    "table_name = 'user_kposminin.urls_w_levels3'\n",
    "train_start_date, train_end_date  = '2016-06-01', '2016-06-02'\n",
    "test_date = '2016-06-30'\n",
    "\n",
    "create_query = '''\n",
    "34 gd \n",
    "drop table if exists  {0};\n",
    "\n",
    "create table {0} as\n",
    "    select\n",
    "    a.id as cookie\n",
    "    ,concat(a.id, \"-\", a.ymd) as object_id\n",
    "    ,a.iymd\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
    "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
    "    ,a.url\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)', 1) as ref_domain\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) ref_lev0\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) ref_lev1\n",
    "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) ref_lev2\n",
    "    ,a.referrer\n",
    "    ,a.timestamp\n",
    "    from prod_raw_liveinternet.access_log  a\n",
    "    where \n",
    "      (ymd between {1} and {2} or ymd = {3})\n",
    ";\n",
    "\n",
    "select \n",
    "    object_id,\n",
    "    domain as urlp\n",
    "from {0}\n",
    "union all\n",
    "select\n",
    "    object_id,\n",
    "    concat(domain,'[0]',lev0) as urlp\n",
    "from {0}\n",
    "union all\n",
    "select \n",
    "    object_id,\n",
    "    concat(domain,'[1]',lev1) as urlp\n",
    "from {0}\n",
    "union all\n",
    "select \n",
    "    object_id,\n",
    "    concat(domain,'[2]',lev2) as urlp\n",
    "from {0}\n",
    ";\n",
    "\n",
    "'''.format(table_name, train_start_date, train_end_date, test_date)\n",
    "\n",
    "\n",
    "\n",
    "select_query = '''\n",
    "select b.*\n",
    "from\n",
    "   (select\n",
    "      a.*,\n",
    "      count(*) over (partition by url) as url_count\n",
    "   from {0} a\n",
    "    ) b\n",
    "    where \n",
    "       b.url_count > 100\n",
    "       and hash(b.cookie) % 20011 = 0\n",
    "\n",
    "'''.format(table_name)\n",
    "\n",
    "#for q in create_query.split(';'):\n",
    "#    hc.sql(q)\n",
    "\n",
    "rdd = hc.sql(select_query).rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1d2c150e579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m rdd15 = rdd.flatMap(lambda row: [(row['cookie'] + str(1*test(row['object_id'],test_date)) , row['domain']),\n\u001b[0m\u001b[0;32m     10\u001b[0m                                 \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cookie'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'[0]'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lev0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                 \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cookie'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'[1]'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lev1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "#Group by cookie and form url list the cookie visited\n",
    "def test(id, test_date):\n",
    "    ''' Is id (== cookie + '-'+date) from test sample or not ( and is from train sample)? Output is True or False\n",
    "        In this case test sample == sample from 4th of July.    \n",
    "    '''    \n",
    "    return re.findall('([0-9]{4}-[0-9]{2}-[0-9]{2})',id)[0] == test_date\n",
    "\n",
    "    \n",
    "rdd15 = rdd.flatMap(lambda row: [(row['cookie'] + str(1*test(row['object_id'],test_date)) , row['domain']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[0]'+row['lev0']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[1]'+row['lev1']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[2]'+row['lev2']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[r]'+row['ref_domain']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[r0]'+row['ref_lev0']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[r1]'+row['ref_lev1']),\n",
    "                                (row['cookie'] + str(1*test(row['object_id'],test_date)), row['domain']+'[r2]'+row['ref_lev2'])\n",
    "])\n",
    "rdd15.cache()\n",
    "rdd2 = rdd15.reduceByKey(lambda a,b: a + ';;' + b)\n",
    "\n",
    "# Label target cookies (=cookies visited target url) and exclude some urls\n",
    "\n",
    "#target_urls =['mkb.ru/facility/private_person/cards/credit_card','mkb.ru']\n",
    "target_urls =['avito.ru'] #['raiffeisen.ru/retail/cards/credit/']\n",
    "exclude_urls = target_urls + [] #['raiffeisen.ru']\n",
    "\n",
    "def handle_row(row,targ_urls, exclud_urls):\n",
    "    proc_urls = row[1]\n",
    "    for u in exclud_urls:\n",
    "        proc_urls = re.sub('[^;;]*'+ u +'[^;;]*','',proc_urls)\n",
    "    return (\n",
    "        bool(int(row[0][-1])),\n",
    "        row[0],\n",
    "        any(tu in row[1] for tu in targ_urls), \n",
    "        re.sub('[;]{3,}',';;',proc_urls)\n",
    "    )\n",
    "\n",
    "rdd3 = rdd2.map(lambda row: handle_row(row,target_urls, exclude_urls))\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "\n",
    "tf = HashingTF(numFeatures = 10 ** 4)\n",
    "\n",
    "#transform urls (as Bag of Words) into features and form features with labels\n",
    "train_data = rdd3.filter(lambda row: not row[0]).map(lambda row: LabeledPoint(row[2], tf.transform(row[3].split(';;'))))\n",
    "test_data  = rdd3.filter(lambda row:     row[0]).map(lambda row: LabeledPoint(row[2], tf.transform(row[3].split(';;'))))\n",
    "# TODO count visits or not\n",
    "# split into train and test samples\n",
    "#train_data, test_data = all_data.randomSplit([6, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train NaiveBayes model\n",
    "train_data.cache()\n",
    "modelNB = NaiveBayes.train(train_data)\n",
    "\n",
    "def predict_proba_NB(f,model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability. f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Function selects winning class with it probability.\n",
    "    Output: tuple with model selected class number as first element (type int) and it probability as second (type float).\n",
    "    '''\n",
    "    logp = [[i,f.dot(model.theta[i]) + model.pi[i]] for i in range(len(model.theta))] # classes with log probabilities\n",
    "    wi = sorted(logp, key = lambda e:  - e[1])[0][0] #winning index\n",
    "    prob = 1./sum([np.exp(e[1] - logp[wi][1]) for e in logp]) #winning class probability\n",
    "    return wi, prob\n",
    "\n",
    "def predict_proba_NB_2(f, model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability for 2-class classification.\n",
    "    f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Output: probability of class 1 (type float).\n",
    "    '''\n",
    "    if len(model.theta) != 2:\n",
    "        print('Model is NOT a 2-class classifier')\n",
    "        return None\n",
    "    logp = [f.dot(model.theta[i]) + model.pi[i] for i in range(2)]    \n",
    "    return 1./(1. + np.exp(logp[0] - logp[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-eb472eff5cee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#LogisticRegression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodelLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodelLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclearThreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, step, miniBatchFraction, initialWeights, regParam, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[0;32m    310\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[1;32m--> 211\u001b[1;33m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rdd, i)\u001b[0m\n\u001b[0;32m    308\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithSGD\", rdd, int(iterations),\n\u001b[0;32m    309\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LogisticRegression model\n",
    "modelLR = LogisticRegressionWithSGD.train(train_data)\n",
    "modelLR.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Current approach\n",
    "#rdd_tmp = rdd3.map(lambda row: (row[1],row[2]))\n",
    "# NB: works only for boolean value but not for int.\n",
    "rdd5 = rdd15.leftOuterJoin(rdd3.map(lambda row: (row[1],row[2]))).map(lambda row: row[1]) \\\n",
    "   .aggregateByKey((0,0),\n",
    "     (lambda acc, value: (acc[0] + int((not value is None) and value), acc[1] + 1)),\n",
    "     (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "   ).filter(lambda row: row[1][1] > 2 and row[1][1] > 100) \\\n",
    "    .map(lambda row: (row[0], pyspark.sql.Row(Positive = row[1][0],Total = row[1][1], Score = np.log((row[1][0]+0.5)/(row[1][1] - row[1][0] + 0.5)))))\n",
    "\n",
    "# Select 1% quantile estimation (using sampling) of Score field\n",
    "n = 10**5 #Sampling size\n",
    "tmp = rdd5.map(lambda row: row[1].Score).randomSplit([n,max(rdd5.count() - n, 0)])[0]\n",
    "tmp_len = tmp.countApprox(timeout = 100)\n",
    "score_threshold = tmp.sortBy(lambda x:-x).zipWithIndex().map(lambda r:(r[1], r[0])).lookup(int(tmp_len * 0.01))[0]\n",
    "\n",
    "rdd5 = rdd5.filter(lambda row: row[1].Score >= score_threshold)\n",
    "rdd5.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coordinates: 37.24N, -115.81W'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Coordinates: {latitude}, {longitude}'.format(latitude='37.24N', longitude='-115.81W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def calc_score(url_list, rdd):\n",
    "    '''\n",
    "    url_list is a list of target url pieces.\n",
    "    rdd consists of url pieces as key and it score as value.\n",
    "    Output is score for current approach\n",
    "    '''    \n",
    "    try:\n",
    "        return rdd.filter(lambda row: row[0] in url_list).map(lambda r:r[1].Score).max()\n",
    "    except ValueError:\n",
    "        return - 10 ** 30\n",
    "\n",
    "def calc_score1(url_list, rdd_dict):\n",
    "    '''\n",
    "    url_list is a list of target url pieces.\n",
    "    rdd_dict consists of url pieces as key and it score as value.\n",
    "    Output is score for current approach\n",
    "    '''\n",
    "    m = - 10 ** 30\n",
    "    for k in rdd_dict.keys():\n",
    "        if k in url_list:\n",
    "            m = max(rdd_dict[k], m)\n",
    "    return m\n",
    "rdd5_dict = rdd5.map(lambda (k,v): (k, v.Score)).collectAsMap()\n",
    "test_data_ca = rdd3.filter(lambda row: row[0]).map(lambda row: LabeledPoint(row[2], calc_score1(row[3].split(';;'),rdd5_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_ca.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'4-club.ru[r]4-club.ru', u'4-club.ru[1]', u'4-club.ru[2]', u'4-club.ru']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing result\n",
    "df_test = test_data.map( lambda lp: pyspark.sql.Row(\n",
    "        Label = lp.label,\n",
    "        NaiveBayes = float(predict_proba_NB_2(lp.features, modelNB)),\n",
    "        LogisticRegression = float(modelLR.predict(lp.features))\n",
    "    )).toDF().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build AUCROC metric and print results\n",
    "import sklearn\n",
    "AUCROC = {}\n",
    "for c in df_test.columns:\n",
    "    if c!= 'Label':\n",
    "        AUCROC[c] = sklearn.metrics.roc_auc_score(df_test['Label'],df_test[c])\n",
    "        \n",
    "print('Methods AUCROC performance on test sample ({0:.0f} samples with {1:.0f} positives):\\n'.format(df_test.size,df_test['Label'].sum()) +\n",
    "     '\\n'.join(['{0:<30}{1:.5f}'.format(k,v) for (k,v) in AUCROC.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
