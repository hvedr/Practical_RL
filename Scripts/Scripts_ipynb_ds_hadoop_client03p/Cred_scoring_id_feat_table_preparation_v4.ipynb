{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание таблицы факторов по кукам для кред скоринга\n",
    "### Расчет на бою"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class calc_cred_score():\n",
    "\n",
    "    init_query = '''\n",
    "    set hive.vectorized.execution.enabled=true;\n",
    "    set mapreduce.map.memory.mb=4096;\n",
    "    set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "    set mapreduce.task.io.sort.mb=1024;\n",
    "    set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "    set mapreduce.reduce.memory.mb=7000;\n",
    "    set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "    set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "    set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "    set hive.optimize.ppd=true;\n",
    "    set hive.merge.smallfiles.avgsize=536870912;\n",
    "    set hive.merge.mapredfiles=true;\n",
    "    set hive.merge.mapfiles=true;\n",
    "    set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "    set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "    set hive.exec.parallel=true;\n",
    "    set hive.exec.max.created.files=10000000;\n",
    "    set hive.exec.compress.output=true;\n",
    "    set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "    set hive.exec.max.dynamic.partitions=1000000;\n",
    "    set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "    set io.seqfile.compression.type=BLOCK;\n",
    "    set mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec;\n",
    "    set mapreduce.map.failures.maxpercent=5;\n",
    "\n",
    "    set hive.tez.auto.reducer.parallelism=true;\n",
    "    set hive.tez.min.partition.factor=0.25;\n",
    "    set hive.tez.max.partition.factor=2.0;\n",
    "    set tez.runtime.pipelined.sorter.lazy-allocate.memory=true;\n",
    "    set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "    '''\n",
    "\n",
    "    create_accum_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS `user_kposminin.id_feat_accum`(\n",
    "          `id` string, \n",
    "          `load_src` string, \n",
    "          `first_id_ymd` string, \n",
    "          `last_id_ymd` string, \n",
    "          `ymd_cnt` int, \n",
    "          `urlfr_cnt` bigint, -- `cnt` bigint, \n",
    "          `visits_cnt` bigint, \n",
    "          `hits` bigint, \n",
    "          `emailru_sum` bigint, \n",
    "          `mobile_sum` double, \n",
    "          `vk_sum` double, \n",
    "          `social_sum` double, \n",
    "          `work_hours_hits_sum` double, \n",
    "          `avg_hour_sum_sq` bigint, \n",
    "          `avg_hour_sum` bigint, \n",
    "          `max_score1` double, \n",
    "          `max_score2` double, \n",
    "          `max_score3` double, \n",
    "          `max_score4` double, \n",
    "          `max_score5` double, \n",
    "          `max_score6` double, \n",
    "          `max_score7` double, \n",
    "          `max_score8` double, \n",
    "          `min_score1` double, \n",
    "          `min_score2` double, \n",
    "          `min_score3` double, \n",
    "          `min_score4` double, \n",
    "          `min_score5` double, \n",
    "          `min_score6` double, \n",
    "          `min_score7` double, \n",
    "          `min_score8` double, \n",
    "          `sum_score1` double, \n",
    "          `sum_score2` double, \n",
    "          `sum_score3` double, \n",
    "          `sum_score4` double, \n",
    "          `sum_score5` double, \n",
    "          `sum_score6` double, \n",
    "          `sum_score7` double, \n",
    "          `sum_score8` double, \n",
    "          `cnt_score1` int, \n",
    "          `cnt_score2` int, \n",
    "          `cnt_score3` int, \n",
    "          `cnt_score4` int, \n",
    "          `cnt_score5` int, \n",
    "          `cnt_score6` int, \n",
    "          `cnt_score7` int, \n",
    "          `cnt_score8` int, \n",
    "          `good_urlfr_sum_score1` double, \n",
    "          `good_urlfr_sum_score2` double, \n",
    "          `good_urlfr_sum_score3` double,\n",
    "          `good_urlfr_sum_score4` double,\n",
    "          `good_urlfr_sum_score5` double,\n",
    "          `good_urlfr_sum_score6` double,\n",
    "          `good_urlfr_sum_score7` double,\n",
    "          `good_urlfr_sum_score8` double)\n",
    "    partitioned by (\n",
    "          `first_calc_ymd` string, \n",
    "          `last_calc_ymd` string      \n",
    "          )\n",
    "    ;\n",
    "\n",
    "    '''\n",
    " \n",
    "    feat_1d_query_pattern = '''\n",
    "\n",
    "    -- #ymd. Id_feats 1 day calc\n",
    "    with mymd as \n",
    "     (select\n",
    "       target,\n",
    "       max(ymd) as max_ymd\n",
    "      from\n",
    "       user_kposminin.urlfr_scores\n",
    "      where \n",
    "        ymd < date_add('#ymd',-3)\n",
    "      group by target\n",
    "     )\n",
    "\n",
    "    insert overwrite table user_kposminin.id_feat_accum partition (first_calc_ymd, last_calc_ymd)\n",
    "    select \n",
    "     id,\n",
    "     load_src, \n",
    "     ymd as first_id_ymd, \n",
    "     ymd as last_id_ymd,\n",
    "     1 as ymd_cnt,\n",
    "     count(distinct urlfr) as urlfr_cnt,\n",
    "     count(urlfr) as visits_cnt,\n",
    "     sum(cnt) as hits,\n",
    "     sum(if(urlfr like 'e.mail.ru%',1,0)) as emailru_sum,\n",
    "     sum(if(urlfr like 'm.%',1,0)) as mobile_sum,\n",
    "     sum(if(urlfr rlike '^(m\\\\.)?vk.com', 1, 0)) as vk_sum,\n",
    "     sum(if(urlfr rlike '^(m\\\\.)?vk.com' or urlfr rlike '^(m\\\\.)?(ok|odnoklassniki)\\\\.ru' or urlfr rlike '^(m\\\\.)?my.mail.ru',1,0)) as social_sum,\n",
    "     sum(if(avg_hour >= 9 and avg_hour <= 20,cnt,0)) as work_hours_hits_sum,\n",
    "     sum( avg_hour * avg_hour) as avg_hour_sum_sq,\n",
    "     sum(avg_hour) as avg_hour_sum,\n",
    "     max(score1) as max_score1,\n",
    "     max(score2) as max_score2,\n",
    "     max(score3) as max_score3,\n",
    "     max(score4) as max_score4,\n",
    "     max(score5) as max_score5,\n",
    "     max(score6) as max_score6,\n",
    "     max(score7) as max_score7,\n",
    "     max(score8) as max_score8,\n",
    "     min(score1) as min_score1,\n",
    "     min(score2) as min_score2,\n",
    "     min(score3) as min_score3,\n",
    "     min(score4) as min_score4,\n",
    "     min(score5) as min_score5,\n",
    "     min(score6) as min_score6,\n",
    "     min(score7) as min_score7,\n",
    "     min(score8) as min_score8,\n",
    "     sum(score1) as sum_score1,\n",
    "     sum(score2) as sum_score2,\n",
    "     sum(score3) as sum_score3,\n",
    "     sum(score4) as sum_score4, \n",
    "     sum(score5) as sum_score5,\n",
    "     sum(score6) as sum_score6, \n",
    "     sum(score7) as sum_score7,\n",
    "     sum(score8) as sum_score8, \n",
    "     count(score1) as cnt_score1,\n",
    "     count(score2) as cnt_score2,\n",
    "     count(score3) as cnt_score3,\n",
    "     count(score4) as cnt_score4, \n",
    "     count(score5) as cnt_score5,\n",
    "     count(score6) as cnt_score6, \n",
    "     count(score7) as cnt_score7,\n",
    "     count(score8) as cnt_score8, \n",
    "     count( if(score1 > -0.2, urlfr,Null)) as good_urlfr_sum_score1,\n",
    "     count( if(score2 > -9, urlfr,Null)) as good_urlfr_sum_score2,\n",
    "     count( if(score3 > -9, urlfr,Null)) as good_urlfr_sum_score3,\n",
    "     count( if(score4 > -2, urlfr,Null)) as good_urlfr_sum_score4,\n",
    "     count( if(score5 > -9, urlfr,Null)) as good_urlfr_sum_score5,\n",
    "     count( if(score6 > -0.2, urlfr,Null)) as good_urlfr_sum_score6,\n",
    "     count( if(score7 > -9, urlfr,Null)) as good_urlfr_sum_score7,\n",
    "     count( if(score8 > -0.2, urlfr,Null)) as good_urlfr_sum_score8,\n",
    "     '#ymd' as first_calc_ymd, \n",
    "     '#ymd' as last_calc_ymd\n",
    " \n",
    "    from\n",
    "     (select\n",
    "        v.id,\n",
    "        v.load_src as load_src,\n",
    "        v.ymd,\n",
    "        v.url_fragment as urlfr,\n",
    "        unix_timestamp(v.ymd, 'yyyy-MM-dd')/60/60 + v.average_visit_hour  as time_h,\n",
    "        1 as time_std,\n",
    "        v.visit_count as cnt,\n",
    "        v.average_visit_hour as avg_hour,\n",
    "        t1.score as score1,\n",
    "        t2.score as score2,\n",
    "        t3.score as score3,\n",
    "        t4.score as score4,\n",
    "        t5.score as score5,\n",
    "        log((t2.cnt_positive + 0.1)/(t3.cnt_positive - t2.cnt_positive + 0.1)) as score6,\n",
    "        t7.score as score7,\n",
    "        log((t5.cnt_positive + 0.1)/(t7.cnt_positive - t5.cnt_positive + 0.1)) as score8\n",
    "      from\n",
    "        prod_odd.visit_feature v\n",
    "        left semi join (\n",
    "          select \n",
    "            uid_str as id,\n",
    "            property_value as phone_num,\n",
    "            load_src\n",
    "          from\n",
    "            prod_dds.md_uid_property \n",
    "          where\n",
    "            property_cd = 'PHONE'\n",
    "          ) m on m.id = v.id and m.load_src = v.load_src\n",
    "        left join (\n",
    "            select urlfr,score\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'ccall_tinkoff_approve_from_fullapp'\n",
    "        ) t1 on t1.urlfr = v.url_fragment\n",
    "        left join (\n",
    "            select urlfr,score,positive as cnt_positive\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'tinkoff_platinum_approved_application03@tinkoff_action' \n",
    "        ) t2 on t2.urlfr = v.url_fragment\n",
    "        left join (\n",
    "            select urlfr,score,positive as cnt_positive\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'tinkoff_platinum_complete_application03@tinkoff_action'\n",
    "        ) t3 on t3.urlfr = v.url_fragment\n",
    "        left join (\n",
    "            select urlfr,score\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'tinkoff_LON_CCR_default'\n",
    "        ) t4 on t4.urlfr = v.url_fragment\n",
    "        left join (\n",
    "            select urlfr,score,positive as cnt_positive\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'tinkoff_platinum_approved_application03_1m'\n",
    "        ) t5 on t5.urlfr = v.url_fragment\n",
    "        left join (\n",
    "            select urlfr,score,positive as cnt_positive\n",
    "              from mymd td\n",
    "             inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd\n",
    "             where td.target = 'tinkoff_platinum_complete_application03_1m'\n",
    "        ) t7 on t7.urlfr = v.url_fragment\n",
    "      where \n",
    "        v.ymd = '#ymd' \n",
    "     ) a \n",
    "    group by\n",
    "      id,load_src,ymd\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "    accumulator_merge_pattern = '''\n",
    "\n",
    "    insert overwrite table user_kposminin.id_feat_accum partition (first_calc_ymd,last_calc_ymd)\n",
    "    select\n",
    "      nvl(a.id,b.id) as id,\n",
    "      nvl(a.load_src,b.load_src) as load_src,\n",
    "      least(a.first_id_ymd,b.first_id_ymd) as first_id_ymd,\n",
    "      greatest(a.last_id_ymd,b.last_id_ymd) as last_id_ymd,\n",
    "      nvl(a.ymd_cnt,0) + nvl(b.ymd_cnt,0) as ymd_cnt,\n",
    "      nvl(a.urlfr_cnt,0) + nvl(b.urlfr_cnt,0) as urlfr_cnt,\n",
    "      nvl(a.visits_cnt,0) + nvl(b.visits_cnt,0) as visits_cnt,\n",
    "      nvl(a.hits,0) + nvl(b.hits,0) as hits,\n",
    "      nvl(a.emailru_sum,0) + nvl(b.emailru_sum,0) as emailru_sum,\n",
    "      nvl(a.mobile_sum,0) + nvl(b.mobile_sum,0) as mobile_sum,\n",
    "      nvl(a.vk_sum,0) + nvl(b.vk_sum,0) as vk_sum,\n",
    "      nvl(a.social_sum,0) + nvl(b.social_sum,0) as social_sum,\n",
    "      nvl(a.work_hours_hits_sum,0) + nvl(b.work_hours_hits_sum,0) as work_hours_hits_sum,\n",
    "      nvl(a.avg_hour_sum_sq,0) + nvl(b.avg_hour_sum_sq,0) as avg_hour_sum_sq,\n",
    "      nvl(a.avg_hour_sum,0) + nvl(b.avg_hour_sum,0) as avg_hour_sum,\n",
    "      greatest(a.max_score1,b.max_score1) as max_score1,\n",
    "      greatest(a.max_score2,b.max_score2) as max_score2,\n",
    "      greatest(a.max_score3,b.max_score3) as max_score3,\n",
    "      greatest(a.max_score4,b.max_score4) as max_score4,\n",
    "      greatest(a.max_score5,b.max_score5) as max_score5,\n",
    "      greatest(a.max_score6,b.max_score6) as max_score6,\n",
    "      greatest(a.max_score7,b.max_score7) as max_score7,\n",
    "      greatest(a.max_score8,b.max_score8) as max_score8,\n",
    "      least(a.min_score1, b.min_score1) as min_score1,\n",
    "      least(a.min_score2, b.min_score2) as min_score2,\n",
    "      least(a.min_score3, b.min_score3) as min_score3,\n",
    "      least(a.min_score4, b.min_score4) as min_score4,\n",
    "      least(a.min_score5, b.min_score5) as min_score5,\n",
    "      least(a.min_score6, b.min_score6) as min_score6,\n",
    "      least(a.min_score7, b.min_score7) as min_score7,\n",
    "      least(a.min_score8, b.min_score8) as min_score8,\n",
    "      nvl(a.sum_score1,0) + nvl(b.sum_score1,0) as sum_score1,\n",
    "      nvl(a.sum_score2,0) + nvl(b.sum_score2,0) as sum_score2,\n",
    "      nvl(a.sum_score3,0) + nvl(b.sum_score3,0) as sum_score3,\n",
    "      nvl(a.sum_score4,0) + nvl(b.sum_score4,0) as sum_score4,\n",
    "      nvl(a.sum_score5,0) + nvl(b.sum_score5,0) as sum_score5,\n",
    "      nvl(a.sum_score6,0) + nvl(b.sum_score6,0) as sum_score6,\n",
    "      nvl(a.sum_score7,0) + nvl(b.sum_score7,0) as sum_score7,\n",
    "      nvl(a.sum_score8,0) + nvl(b.sum_score8,0) as sum_score8,\n",
    "      nvl(a.cnt_score1,0) + nvl(b.cnt_score1,0) as cnt_score1,\n",
    "      nvl(a.cnt_score2,0) + nvl(b.cnt_score2,0) as cnt_score2,\n",
    "      nvl(a.cnt_score3,0) + nvl(b.cnt_score3,0) as cnt_score3,\n",
    "      nvl(a.cnt_score4,0) + nvl(b.cnt_score4,0) as cnt_score4,\n",
    "      nvl(a.cnt_score5,0) + nvl(b.cnt_score5,0) as cnt_score5,\n",
    "      nvl(a.cnt_score6,0) + nvl(b.cnt_score6,0) as cnt_score6,\n",
    "      nvl(a.cnt_score7,0) + nvl(b.cnt_score7,0) as cnt_score7,\n",
    "      nvl(a.cnt_score8,0) + nvl(b.cnt_score8,0) as cnt_score8,\n",
    "      nvl(a.good_urlfr_sum_score1,0) + nvl(b.good_urlfr_sum_score1,0) as good_urlfr_sum_score1,\n",
    "      nvl(a.good_urlfr_sum_score2,0) + nvl(b.good_urlfr_sum_score2,0) as good_urlfr_sum_score2,\n",
    "      nvl(a.good_urlfr_sum_score3,0) + nvl(b.good_urlfr_sum_score3,0) as good_urlfr_sum_score3,\n",
    "      nvl(a.good_urlfr_sum_score4,0) + nvl(b.good_urlfr_sum_score4,0) as good_urlfr_sum_score4,\n",
    "      nvl(a.good_urlfr_sum_score5,0) + nvl(b.good_urlfr_sum_score5,0) as good_urlfr_sum_score5,\n",
    "      nvl(a.good_urlfr_sum_score6,0) + nvl(b.good_urlfr_sum_score6,0) as good_urlfr_sum_score6,\n",
    "      nvl(a.good_urlfr_sum_score7,0) + nvl(b.good_urlfr_sum_score7,0) as good_urlfr_sum_score7,\n",
    "      nvl(a.good_urlfr_sum_score8,0) + nvl(b.good_urlfr_sum_score8,0) as good_urlfr_sum_score8,\n",
    "      '#new_first_calc_ymd' as first_calc_ymd,\n",
    "      '#new_last_calc_ymd' as last_calc_ymd\n",
    "    from \n",
    "      (select * from user_kposminin.id_feat_accum a where a.first_calc_ymd = '#a_first_calc_ymd' and a.last_calc_ymd = '#a_last_calc_ymd') a\n",
    "      full join (select * from user_kposminin.id_feat_accum b where b.first_calc_ymd = '#b_first_calc_ymd' and b.last_calc_ymd = '#b_last_calc_ymd') b \n",
    "        on a.id = b.id and a.load_src = b.load_src\n",
    "    ;\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    clear_partition_query = '''\n",
    "    alter table user_kposminin.id_feat_accum drop partition (first_calc_ymd = '#first_calc_ymd', last_calc_ymd = '#last_calc_ymd');\n",
    "    '''\n",
    "\n",
    "    create_feat_table_query = '''\n",
    "\n",
    "    create table user_kposminin.id_feat_ccall as\n",
    "    select \n",
    "         a.phone_mobile, \n",
    "         count(distinct a.id) as id_cnt,\n",
    "         count(distinct acc.id) as acc_id_cnt,\n",
    "         a.call_ymd as call_ymd,\n",
    "         max(a.approve) as approve,\n",
    "         sum(acc.urlfr_cnt) as urlfr_cnt,\n",
    "         sum(acc.visits_cnt) as visits_cnt, \n",
    "         sum(acc.hits) as hits,\n",
    "         max(acc.max_score1) as max_score1,\n",
    "         sum(acc.sum_score1) / sum(acc.cnt_score1) as avg_score1,\n",
    "         min(acc.min_score1) as min_score1,\n",
    "         max(acc.max_score2) as max_score2,\n",
    "         sum(acc.sum_score2) / sum(acc.cnt_score2) as avg_score2,\n",
    "         min(acc.min_score2) as min_score2,\n",
    "         max(acc.max_score3) as max_score3,\n",
    "         sum(acc.sum_score3) / sum(acc.cnt_score3) as avg_score3,\n",
    "         min(acc.min_score3) as min_score3,\n",
    "         max(acc.max_score4) as max_score4,\n",
    "         sum(acc.sum_score4) / sum(acc.cnt_score4) as avg_score4,\n",
    "         min(acc.min_score4) as min_score4,\n",
    "         max(acc.max_score5) as max_score5,\n",
    "         sum(acc.sum_score5) / sum(acc.cnt_score4) as avg_score5,\n",
    "         min(acc.min_score5) as min_score5,\n",
    "         max(acc.max_score6) as max_score6,\n",
    "         sum(acc.sum_score6) / sum(acc.cnt_score4) as avg_score6,\n",
    "         min(acc.min_score6) as min_score6,\n",
    "         sum(acc.emailru_sum) / sum(acc.visits_cnt) as emailru_share,\n",
    "         sum(acc.mobile_sum) / sum(acc.visits_cnt) as mobile_share,\n",
    "         sum(acc.vk_sum) / sum(acc.visits_cnt) as vk_share,\n",
    "         sum(acc.social_sum) / sum(acc.visits_cnt) as social_share,\n",
    "         sum(acc.work_hours_hits_sum) / sum(acc.hits) as work_hours_hits_share,\n",
    "         sqrt(sum(acc.avg_hour_sum_sq)/(sum(acc.visits_cnt)-1) - power(sum(acc.avg_hour_sum)/(sum(acc.visits_cnt) - 1), 2)) as hour_std,\n",
    "         sum(acc.good_urlfr_sum_score1) / sum(acc.cnt_score1) as good_urlfr_share_score1, \n",
    "         sum(acc.good_urlfr_sum_score2) / sum(acc.cnt_score2) as good_urlfr_share_score2,\n",
    "         sum(acc.good_urlfr_sum_score3) / sum(acc.cnt_score3) as good_urlfr_share_score3, \n",
    "         sum(acc.good_urlfr_sum_score4) / sum(acc.cnt_score4) as good_urlfr_share_score4, \n",
    "         sum(acc.good_urlfr_sum_score5) / sum(acc.cnt_score5) as good_urlfr_share_score5,\n",
    "         sum(acc.good_urlfr_sum_score6) / sum(acc.cnt_score6) as good_urlfr_share_score6,\n",
    "         max(trim(pc.provider)) as mob_provider,\n",
    "         max(r.ind) as ind,\n",
    "         max(r.pop_country_share) as pop_country_share, \n",
    "         max(r.pop_city_share) as pop_city_share, \n",
    "         max(r.population / r.area_sq_km) as density,\n",
    "         max(r.area_sq_km) as area_sq_km,\n",
    "         max(trim(r.federal_district)) as federal_district, \n",
    "         max(r.avg_salary_2015_rub) as avg_salary_2015_rub, \n",
    "         max(r.utc_time_zone_val) as utc_time_zone_val\n",
    "\n",
    "\n",
    "      from\n",
    "         user_kposminin.ccall_aza_id a\n",
    "         inner join user_kposminin.id_feat_accum acc on acc.id = a.id and acc.last_calc_ymd = date_add(a.call_ymd, -1)\n",
    "         left join dds_dic.phone_codes pc on trim(pc.phone_code) = substr(a.phone_mobile,2,9)\n",
    "         left join dds_dic.region_stat r on r.ind = pc.region_id\n",
    "     group by\n",
    "             a.phone_mobile,\n",
    "             a.call_ymd\n",
    "    ;\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, hc = None, n_threads = 10, init_query = True):\n",
    "        import datetime\n",
    "        if(init_query):\n",
    "            self.query = self.init_query + self.create_accum_query\n",
    "        else:\n",
    "            self.query = ''            \n",
    "        self.log = ''\n",
    "        \n",
    "    def calc_days(self, days, merge = True, clean = True):\n",
    "        ''' Calc 1d feat and accum it for days (datetime.datetime list)'''\n",
    "        days = sorted(days)\n",
    "        acc_first_calc_ymd, acc_last_calc_ymd = [days[0]] * 2\n",
    "        for day in days:\n",
    "            new_first_calc_ymd = min(day,acc_first_calc_ymd)\n",
    "            new_last_calc_ymd  = max(day,acc_last_calc_ymd)\n",
    "            \n",
    "            self.query += self.feat_1d_query_pattern.replace('#ymd',day.strftime('%Y-%m-%d'))\n",
    "            if merge & (days.index(day) > 0):\n",
    "                self.query += (self.accumulator_merge_pattern \n",
    "                       .replace('#a_first_calc_ymd', acc_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#a_last_calc_ymd', acc_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#b_first_calc_ymd', day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#b_last_calc_ymd', day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#new_first_calc_ymd', new_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#new_last_calc_ymd',  new_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                               )\n",
    "                if clean & (days.index(day) > 1):\n",
    "                    self.query += (self.clear_partition_query \n",
    "                       .replace('#first_calc_ymd', acc_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#last_calc_ymd', acc_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                                  )\n",
    "                if ((clean == 'full_clean') & (days.index(day) > 1)): \n",
    "                    self.query += (self.clear_partition_query \n",
    "                       .replace('#first_calc_ymd', day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#last_calc_ymd', day.strftime('%Y-%m-%d'))\n",
    "                                  )\n",
    "            acc_first_calc_ymd = new_first_calc_ymd\n",
    "            acc_last_calc_ymd = new_last_calc_ymd\n",
    "    \n",
    "    def calc_day_range(self, first_day = None, last_day = None, n_days = None, merge = True, clean = True):        \n",
    "        assert (first_day is None) + (last_day is None) + (n_days is None) == 1, '''calc_day_range Error:\n",
    "              exactly two of three params must be filled: first_day, last_day, n_days'''.replace('\\n',' ')\n",
    "        if first_day is None:\n",
    "            day_range = [last_day + datetime.timedelta(days = i) for i in range(-n_days + 1,1)]\n",
    "        elif last_day is None:\n",
    "            day_range = [first_day + datetime.timedelta(days = i) for i in range(n_days)]\n",
    "        else:\n",
    "            first_day,last_day = min(first_day,last_day), max(first_day,last_day)\n",
    "            day_range = [first_day + datetime.timedelta(days = i) for i in range((last_day - first_day).days + 1)]\n",
    "        self.calc_days(day_range, merge, clean)\n",
    "        \n",
    "    def get_query(self):\n",
    "        return self.query\n",
    "\n",
    "    def merge_days(self, day_tuples, clean = False):\n",
    "        ''' day_tuples is a list of (first_day,last_day) to merge'''\n",
    "        day_tuples = sorted(day_tuples)\n",
    "        acc_first_calc_ymd, acc_last_calc_ymd = day_tuples[0]\n",
    "        first = True\n",
    "        for (first_day,last_day) in day_tuples[1:]:\n",
    "            new_first_calc_ymd = min(first_day,acc_first_calc_ymd)\n",
    "            new_last_calc_ymd  = max(last_day,acc_last_calc_ymd)\n",
    "            self.query += (self.accumulator_merge_pattern \n",
    "                       .replace('#a_first_calc_ymd', acc_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#a_last_calc_ymd',   acc_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#b_first_calc_ymd', first_day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#b_last_calc_ymd',   last_day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#new_first_calc_ymd', new_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#new_last_calc_ymd',   new_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                               )\n",
    "            if (not clean is False) & ((not first) or (clean == 'full_clean')):\n",
    "                self.query += (self.clear_partition_query \n",
    "                       .replace('#first_calc_ymd', acc_first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                       .replace('#last_calc_ymd',   acc_last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                                  )\n",
    "            if (clean == 'full_clean'): \n",
    "                self.query += (self.clear_partition_query \n",
    "                       .replace('#first_calc_ymd', first_day.strftime('%Y-%m-%d'))\n",
    "                       .replace('#last_calc_ymd',   last_day.strftime('%Y-%m-%d'))\n",
    "                                  )\n",
    "            acc_first_calc_ymd = new_first_calc_ymd\n",
    "            acc_last_calc_ymd = new_last_calc_ymd\n",
    "            first = False\n",
    "            \n",
    "    def calc_full_feat(self):\n",
    "        self.query += self.create_feat_table_query\n",
    "        \n",
    "    def execute_query(self, hc):\n",
    "        '''Execute query using hc HiveContext'''\n",
    "        for q in self.query.split(';'):\n",
    "            if re.search('[^ \\t\\n]',q):\n",
    "                self.log += 'Executing {}\\n'.format(q)\n",
    "                hc.sql(q)\n",
    "    \n",
    "    def get_log(self):\n",
    "        return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! nohup /opt/anaconda/bin/python cred_scor_feat_table_calc_4.py 2016-12-01 2016-12-15 > cred_scor_feat_table_calc_27.log 2>&1 &\n",
    "#os.popen('beeline -u \"jdbc:hive2://ds-hadoop-cs01p:10000/\" -n kposminin -e \"' + ' '.join(c.get_query().split(';')[30].split('\\n')[5:])+ '\"').read()\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 2)\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set('spark.driver.memory','4g')\n",
    "        .set(\"spark.executor.memory\", '2g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)\n",
    "       )\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "sc = SparkContext()\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query0 = '''\n",
    "WITH t AS \n",
    " (\n",
    "   SELECT\n",
    "   v.url_fragment AS urlfr\n",
    "   ,count(distinct if(ta.ymd between v.ymd and date_add(v.ymd,3),ta.id,Null)) as cnt_positive\n",
    "   ,count(distinct v.id) as cnt_total\n",
    "  FROM\n",
    "   prod_odd.visit_feature v\n",
    "   left join (\n",
    "     SELECT DISTINCT a.id,a.ymd \n",
    "     FROM prod_features_liveinternet.user_action a \n",
    "     WHERE a.ymd between date_add('2017-03-01', -30) and date_add('2017-03-01',3) \n",
    "       and a.action_type = 'tinkoff_platinum_complete_application'\n",
    "   ) ta on v.id = ta.id\n",
    "  WHERE\n",
    "   v.ymd between date_add('2017-03-01', -30) and '2017-03-01'\n",
    "   and v.load_src = 'LI.02'\n",
    "  GROUP BY \n",
    "   v.url_fragment\n",
    "   ) \n",
    "INSERT OVERWRITE TABLE \n",
    "  user_kposminin.urlfr_tgt_cnt PARTITION (ymd='2017-03-01', target='tinkoff_platinum_complete_application03_1m') \n",
    "SELECT \n",
    " urlfr AS urlfr\n",
    " ,nvl(cnt_positive, 0) as cnt_positive\n",
    " ,cnt_total\n",
    " ,log((cnt_positive + 0.1)/(cnt_total - cnt_positive + 0.1)) as score\n",
    "FROM t \n",
    ";\n",
    "\n",
    "insert overwrite table user_kposminin.urlfr_scores partition (ymd, target)\n",
    "select\n",
    "  urlfr,\n",
    "  cnt_positive as positive,\n",
    "  cnt_total as total,\n",
    "  score as score,\n",
    "  ymd,\n",
    "  target\n",
    "from user_kposminin.urlfr_tgt_cnt\n",
    "where\n",
    "  target = 'tinkoff_platinum_complete_application03_1m'\n",
    "  and ymd = '2017-03-01'\n",
    "  and (cnt_total > 30000 or cnt_positive > 20)\n",
    ";\n",
    "\n",
    "WITH t AS \n",
    " (\n",
    "   SELECT\n",
    "   v.url_fragment AS urlfr\n",
    "   ,count(distinct if(ta.ymd between v.ymd and date_add(v.ymd,3),ta.id,Null)) as cnt_positive\n",
    "   ,count(distinct v.id) as cnt_total\n",
    "  FROM\n",
    "   prod_odd.visit_feature v\n",
    "   left join (\n",
    "     SELECT DISTINCT a.id,a.ymd \n",
    "     FROM prod_features_liveinternet.user_action a \n",
    "     WHERE a.ymd between date_add('2017-03-01', -30) and date_add('2017-03-01',3) \n",
    "       and a.action_type = 'tinkoff_platinum_approved_application'\n",
    "   ) ta on v.id = ta.id\n",
    "  WHERE\n",
    "   v.ymd between date_add('2017-03-01', -30) and '2017-03-01'\n",
    "   and v.load_src = 'LI.02'\n",
    "  GROUP BY \n",
    "   v.url_fragment\n",
    "   ) \n",
    "INSERT OVERWRITE TABLE \n",
    "  user_kposminin.urlfr_tgt_cnt PARTITION (ymd='2017-03-01', target='tinkoff_platinum_approved_application03_1m') \n",
    "SELECT \n",
    " urlfr AS urlfr\n",
    " ,nvl(cnt_positive, 0) as cnt_positive\n",
    " ,cnt_total\n",
    " ,log((cnt_positive + 0.1)/(cnt_total - cnt_positive + 0.1)) as score\n",
    "FROM t \n",
    ";\n",
    "\n",
    "insert overwrite table user_kposminin.urlfr_scores partition (ymd, target)\n",
    "select\n",
    "  urlfr,\n",
    "  cnt_positive as positive,\n",
    "  cnt_total as total,\n",
    "  score as score,\n",
    "  ymd,\n",
    "  target\n",
    "from user_kposminin.urlfr_tgt_cnt\n",
    "where\n",
    "  target = 'tinkoff_platinum_approved_application03_1m'\n",
    "  and ymd = '2017-03-01'\n",
    "  and (cnt_total > 30000 or cnt_positive > 10)\n",
    ";\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    set hive.vectorized.execution.enabled=true;\n",
      "    set mapreduce.map.memory.mb=4096;\n",
      "    set mapreduce.map.child.java.opts=-Xmx4g;\n",
      "    set mapreduce.task.io.sort.mb=1024;\n",
      "    set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
      "    set mapreduce.reduce.memory.mb=7000;\n",
      "    set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
      "    set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
      "    set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
      "    set hive.optimize.ppd=true;\n",
      "    set hive.merge.smallfiles.avgsize=536870912;\n",
      "    set hive.merge.mapredfiles=true;\n",
      "    set hive.merge.mapfiles=true;\n",
      "    set hive.hadoop.supports.splittable.combineinputformat=true;\n",
      "    set hive.exec.reducers.bytes.per.reducer=536870912;\n",
      "    set hive.exec.parallel=true;\n",
      "    set hive.exec.max.created.files=10000000;\n",
      "    set hive.exec.compress.output=true;\n",
      "    set hive.exec.dynamic.partition.mode=nonstrict;\n",
      "    set hive.exec.max.dynamic.partitions=1000000;\n",
      "    set hive.exec.max.dynamic.partitions.pernode=100000;\n",
      "    set io.seqfile.compression.type=BLOCK;\n",
      "    set mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec;\n",
      "    set mapreduce.map.failures.maxpercent=5;\n",
      "\n",
      "    set hive.tez.auto.reducer.parallelism=true;\n",
      "    set hive.tez.min.partition.factor=0.25;\n",
      "    set hive.tez.max.partition.factor=2.0;\n",
      "    set tez.runtime.pipelined.sorter.lazy-allocate.memory=true;\n",
      "    set hive.exec.dynamic.partition.mode=nonstrict;\n",
      "    \n",
      "    CREATE TABLE IF NOT EXISTS `user_kposminin.id_feat_accum`(\n",
      "          `id` string, \n",
      "          `load_src` string, \n",
      "          `first_id_ymd` string, \n",
      "          `last_id_ymd` string, \n",
      "          `ymd_cnt` int, \n",
      "          `urlfr_cnt` bigint, -- `cnt` bigint, \n",
      "          `visits_cnt` bigint, \n",
      "          `hits` bigint, \n",
      "          `emailru_sum` bigint, \n",
      "          `mobile_sum` double, \n",
      "          `vk_sum` double, \n",
      "          `social_sum` double, \n",
      "          `work_hours_hits_sum` double, \n",
      "          `avg_hour_sum_sq` bigint, \n",
      "          `avg_hour_sum` bigint, \n",
      "          `max_score1` double, \n",
      "          `max_score2` double, \n",
      "          `max_score3` double, \n",
      "          `max_score4` double, \n",
      "          `max_score5` double, \n",
      "          `max_score6` double, \n",
      "          `max_score7` double, \n",
      "          `max_score8` double, \n",
      "          `min_score1` double, \n",
      "          `min_score2` double, \n",
      "          `min_score3` double, \n",
      "          `min_score4` double, \n",
      "          `min_score5` double, \n",
      "          `min_score6` double, \n",
      "          `min_score7` double, \n",
      "          `min_score8` double, \n",
      "          `sum_score1` double, \n",
      "          `sum_score2` double, \n",
      "          `sum_score3` double, \n",
      "          `sum_score4` double, \n",
      "          `sum_score5` double, \n",
      "          `sum_score6` double, \n",
      "          `sum_score7` double, \n",
      "          `sum_score8` double, \n",
      "          `cnt_score1` int, \n",
      "          `cnt_score2` int, \n",
      "          `cnt_score3` int, \n",
      "          `cnt_score4` int, \n",
      "          `cnt_score5` int, \n",
      "          `cnt_score6` int, \n",
      "          `cnt_score7` int, \n",
      "          `cnt_score8` int, \n",
      "          `good_urlfr_sum_score1` double, \n",
      "          `good_urlfr_sum_score2` double, \n",
      "          `good_urlfr_sum_score3` double,\n",
      "          `good_urlfr_sum_score4` double,\n",
      "          `good_urlfr_sum_score5` double,\n",
      "          `good_urlfr_sum_score6` double,\n",
      "          `good_urlfr_sum_score7` double,\n",
      "          `good_urlfr_sum_score8` double)\n",
      "    partitioned by (\n",
      "          `first_calc_ymd` string, \n",
      "          `last_calc_ymd` string      \n",
      "          )\n",
      "    ;\n",
      "\n",
      "    \n",
      "\n",
      "    insert overwrite table user_kposminin.id_feat_accum partition (first_calc_ymd,last_calc_ymd)\n",
      "    select\n",
      "      nvl(a.id,b.id) as id,\n",
      "      nvl(a.load_src,b.load_src) as load_src,\n",
      "      least(a.first_id_ymd,b.first_id_ymd) as first_id_ymd,\n",
      "      greatest(a.last_id_ymd,b.last_id_ymd) as last_id_ymd,\n",
      "      nvl(a.ymd_cnt,0) + nvl(b.ymd_cnt,0) as ymd_cnt,\n",
      "      nvl(a.urlfr_cnt,0) + nvl(b.urlfr_cnt,0) as urlfr_cnt,\n",
      "      nvl(a.visits_cnt,0) + nvl(b.visits_cnt,0) as visits_cnt,\n",
      "      nvl(a.hits,0) + nvl(b.hits,0) as hits,\n",
      "      nvl(a.emailru_sum,0) + nvl(b.emailru_sum,0) as emailru_sum,\n",
      "      nvl(a.mobile_sum,0) + nvl(b.mobile_sum,0) as mobile_sum,\n",
      "      nvl(a.vk_sum,0) + nvl(b.vk_sum,0) as vk_sum,\n",
      "      nvl(a.social_sum,0) + nvl(b.social_sum,0) as social_sum,\n",
      "      nvl(a.work_hours_hits_sum,0) + nvl(b.work_hours_hits_sum,0) as work_hours_hits_sum,\n",
      "      nvl(a.avg_hour_sum_sq,0) + nvl(b.avg_hour_sum_sq,0) as avg_hour_sum_sq,\n",
      "      nvl(a.avg_hour_sum,0) + nvl(b.avg_hour_sum,0) as avg_hour_sum,\n",
      "      greatest(a.max_score1,b.max_score1) as max_score1,\n",
      "      greatest(a.max_score2,b.max_score2) as max_score2,\n",
      "      greatest(a.max_score3,b.max_score3) as max_score3,\n",
      "      greatest(a.max_score4,b.max_score4) as max_score4,\n",
      "      greatest(a.max_score5,b.max_score5) as max_score5,\n",
      "      greatest(a.max_score6,b.max_score6) as max_score6,\n",
      "      greatest(a.max_score7,b.max_score7) as max_score7,\n",
      "      greatest(a.max_score8,b.max_score8) as max_score8,\n",
      "      least(a.min_score1, b.min_score1) as min_score1,\n",
      "      least(a.min_score2, b.min_score2) as min_score2,\n",
      "      least(a.min_score3, b.min_score3) as min_score3,\n",
      "      least(a.min_score4, b.min_score4) as min_score4,\n",
      "      least(a.min_score5, b.min_score5) as min_score5,\n",
      "      least(a.min_score6, b.min_score6) as min_score6,\n",
      "      least(a.min_score7, b.min_score7) as min_score7,\n",
      "      least(a.min_score8, b.min_score8) as min_score8,\n",
      "      nvl(a.sum_score1,0) + nvl(b.sum_score1,0) as sum_score1,\n",
      "      nvl(a.sum_score2,0) + nvl(b.sum_score2,0) as sum_score2,\n",
      "      nvl(a.sum_score3,0) + nvl(b.sum_score3,0) as sum_score3,\n",
      "      nvl(a.sum_score4,0) + nvl(b.sum_score4,0) as sum_score4,\n",
      "      nvl(a.sum_score5,0) + nvl(b.sum_score5,0) as sum_score5,\n",
      "      nvl(a.sum_score6,0) + nvl(b.sum_score6,0) as sum_score6,\n",
      "      nvl(a.sum_score7,0) + nvl(b.sum_score7,0) as sum_score7,\n",
      "      nvl(a.sum_score8,0) + nvl(b.sum_score8,0) as sum_score8,\n",
      "      nvl(a.cnt_score1,0) + nvl(b.cnt_score1,0) as cnt_score1,\n",
      "      nvl(a.cnt_score2,0) + nvl(b.cnt_score2,0) as cnt_score2,\n",
      "      nvl(a.cnt_score3,0) + nvl(b.cnt_score3,0) as cnt_score3,\n",
      "      nvl(a.cnt_score4,0) + nvl(b.cnt_score4,0) as cnt_score4,\n",
      "      nvl(a.cnt_score5,0) + nvl(b.cnt_score5,0) as cnt_score5,\n",
      "      nvl(a.cnt_score6,0) + nvl(b.cnt_score6,0) as cnt_score6,\n",
      "      nvl(a.cnt_score7,0) + nvl(b.cnt_score7,0) as cnt_score7,\n",
      "      nvl(a.cnt_score8,0) + nvl(b.cnt_score8,0) as cnt_score8,\n",
      "      nvl(a.good_urlfr_sum_score1,0) + nvl(b.good_urlfr_sum_score1,0) as good_urlfr_sum_score1,\n",
      "      nvl(a.good_urlfr_sum_score2,0) + nvl(b.good_urlfr_sum_score2,0) as good_urlfr_sum_score2,\n",
      "      nvl(a.good_urlfr_sum_score3,0) + nvl(b.good_urlfr_sum_score3,0) as good_urlfr_sum_score3,\n",
      "      nvl(a.good_urlfr_sum_score4,0) + nvl(b.good_urlfr_sum_score4,0) as good_urlfr_sum_score4,\n",
      "      nvl(a.good_urlfr_sum_score5,0) + nvl(b.good_urlfr_sum_score5,0) as good_urlfr_sum_score5,\n",
      "      nvl(a.good_urlfr_sum_score6,0) + nvl(b.good_urlfr_sum_score6,0) as good_urlfr_sum_score6,\n",
      "      nvl(a.good_urlfr_sum_score7,0) + nvl(b.good_urlfr_sum_score7,0) as good_urlfr_sum_score7,\n",
      "      nvl(a.good_urlfr_sum_score8,0) + nvl(b.good_urlfr_sum_score8,0) as good_urlfr_sum_score8,\n",
      "      '2017-05-01' as first_calc_ymd,\n",
      "      '2017-05-31' as last_calc_ymd\n",
      "    from \n",
      "      (select * from user_kposminin.id_feat_accum a where a.first_calc_ymd = '2017-05-01' and a.last_calc_ymd = '2017-05-15') a\n",
      "      full join (select * from user_kposminin.id_feat_accum b where b.first_calc_ymd = '2017-05-16' and b.last_calc_ymd = '2017-05-31') b \n",
      "        on a.id = b.id and a.load_src = b.load_src\n",
      "    ;\n",
      "\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "c = calc_cred_score()\n",
    "#c.calc_day_range(first_day = datetime.datetime(2016,12,1) ,last_day = datetime.datetime(2016,12,31),merge = True, clean = True)\n",
    "c.merge_days([(datetime.datetime(2017,5,1),datetime.datetime(2017,5,15)),(datetime.datetime(2017,5,16),datetime.datetime(2017,5,31))], clean = True)\n",
    "#c.merge_days([(datetime.datetime(2017,4,1),datetime.datetime(2017,4,30)),(datetime.datetime(2017,5,1),datetime.datetime(2017,5,15)),(datetime.datetime(2017,5,16),datetime.datetime(2017,5,31))], clean = True)\n",
    "#c.merge_days([(datetime.datetime(2017,3,16),datetime.datetime(2017,3,31)),(datetime.datetime(2017,4,1),datetime.datetime(2017,4,30)),\n",
    "#              (datetime.datetime(2017,4,1),datetime.datetime(2017,4,15))], clean = False)\n",
    "#c.merge_days([(datetime.datetime(2017,3,1),datetime.datetime(2017,3,15)),(datetime.datetime(2017,3,16),datetime.datetime(2017,4,30))], clean = True)\n",
    "\n",
    "print(c.get_query())\n",
    "\n",
    "#print('-'*100)\n",
    "#c.execute_query(hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заполняем промежутки по 15 дней. В итоге весь год посчитан. История за 2 месяца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "day_tuples = sorted([(datetime.datetime(2016,i,1),datetime.datetime(2016,i,15)) for i in range(1,13)] + \n",
    "             [(datetime.datetime(2016,i,16),datetime.datetime(2016,i+1,1) - datetime.timedelta(days=1)) for i in range(1,12)] + \n",
    "             [(datetime.datetime(2016,12,16),datetime.datetime(2016,12,31))])\n",
    "import datetime\n",
    "for i in range(len(day_tuples) - 4):\n",
    "    c = calc_cred_score(hc, init_query = False)\n",
    "    c.merge_days(day_tuples[i:i+4], clean = 'clean')\n",
    "    day = day_tuples[i+3][1] + datetime.timedelta(days=1)\n",
    "    inner_day_tuple = [(min([e[0] for e in day_tuples[i:i+4]]),max([e[1] for e in day_tuples[i:i+4]]))]\n",
    "    while day < day_tuples[i+4][1]:\n",
    "        inner_day_tuple.append((day,day))\n",
    "        day += datetime.timedelta(days=1)\n",
    "    c.merge_days(inner_day_tuple, clean = False)\n",
    "    #print(c.get_query())\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    create table user_kposminin.id_feat_ccall as\n",
      "    select \n",
      "         a.phone_mobile, \n",
      "         count(distinct a.id) as id_cnt,\n",
      "         count(distinct acc.id) as acc_id_cnt,\n",
      "         a.call_ymd as call_ymd,\n",
      "         max(a.approve) as approve,\n",
      "         sum(acc.urlfr_cnt) as urlfr_cnt,\n",
      "         sum(acc.visits_cnt) as visits_cnt, \n",
      "         sum(acc.hits) as hits,\n",
      "         max(acc.max_score1) as max_score1,\n",
      "         sum(acc.sum_score1) / sum(acc.cnt_score1) as avg_score1,\n",
      "         min(acc.min_score1) as min_score1,\n",
      "         max(acc.max_score2) as max_score2,\n",
      "         sum(acc.sum_score2) / sum(acc.cnt_score2) as avg_score2,\n",
      "         min(acc.min_score2) as min_score2,\n",
      "         max(acc.max_score3) as max_score3,\n",
      "         sum(acc.sum_score3) / sum(acc.cnt_score3) as avg_score3,\n",
      "         min(acc.min_score3) as min_score3,\n",
      "         max(acc.max_score4) as max_score4,\n",
      "         sum(acc.sum_score4) / sum(acc.cnt_score4) as avg_score4,\n",
      "         min(acc.min_score4) as min_score4,\n",
      "         max(acc.max_score5) as max_score5,\n",
      "         sum(acc.sum_score5) / sum(acc.cnt_score4) as avg_score5,\n",
      "         min(acc.min_score5) as min_score5,\n",
      "         max(acc.max_score6) as max_score6,\n",
      "         sum(acc.sum_score6) / sum(acc.cnt_score4) as avg_score6,\n",
      "         min(acc.min_score6) as min_score6,\n",
      "         sum(acc.emailru_sum) / sum(acc.visits_cnt) as emailru_share,\n",
      "         sum(acc.mobile_sum) / sum(acc.visits_cnt) as mobile_share,\n",
      "         sum(acc.vk_sum) / sum(acc.visits_cnt) as vk_share,\n",
      "         sum(acc.social_sum) / sum(acc.visits_cnt) as social_share,\n",
      "         sum(acc.work_hours_hits_sum) / sum(acc.hits) as work_hours_hits_share,\n",
      "         sqrt(sum(acc.avg_hour_sum_sq)/(sum(acc.visits_cnt)-1) - power(sum(acc.avg_hour_sum)/(sum(acc.visits_cnt) - 1), 2)) as hour_std,\n",
      "         sum(acc.good_urlfr_sum_score1) / sum(acc.cnt_score1) as good_urlfr_share_score1, \n",
      "         sum(acc.good_urlfr_sum_score2) / sum(acc.cnt_score2) as good_urlfr_share_score2,\n",
      "         sum(acc.good_urlfr_sum_score3) / sum(acc.cnt_score3) as good_urlfr_share_score3, \n",
      "         sum(acc.good_urlfr_sum_score4) / sum(acc.cnt_score4) as good_urlfr_share_score4, \n",
      "         sum(acc.good_urlfr_sum_score5) / sum(acc.cnt_score5) as good_urlfr_share_score5,\n",
      "         sum(acc.good_urlfr_sum_score6) / sum(acc.cnt_score6) as good_urlfr_share_score6,\n",
      "         max(trim(pc.provider)) as mob_provider,\n",
      "         max(r.ind) as ind,\n",
      "         max(r.pop_country_share) as pop_country_share, \n",
      "         max(r.pop_city_share) as pop_city_share, \n",
      "         max(r.population / r.area_sq_km) as density,\n",
      "         max(r.area_sq_km) as area_sq_km,\n",
      "         max(trim(r.federal_district)) as federal_district, \n",
      "         max(r.avg_salary_2015_rub) as avg_salary_2015_rub, \n",
      "         max(r.utc_time_zone_val) as utc_time_zone_val\n",
      "\n",
      "\n",
      "      from\n",
      "         user_kposminin.ccall_aza_id a\n",
      "         inner join user_kposminin.id_feat_accum acc on acc.id = a.id and acc.last_calc_ymd = date_add(a.call_ymd, -1)\n",
      "         left join dds_dic.phone_codes pc on trim(pc.phone_code) = substr(a.phone_mobile,2,9)\n",
      "         left join dds_dic.region_stat r on r.ind = pc.region_id\n",
      "     group by\n",
      "             a.phone_mobile,\n",
      "             a.call_ymd\n",
      "    ;\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = '''create table user_kposminin.ccall_aza_id as \n",
    "select distinct phone_mobile, approve, call_ymd, id from user_kposminin.ccall_visits\n",
    ";'''\n",
    "\n",
    "c = calc_cred_score(hc, init_query = False)\n",
    "c.calc_full_feat()\n",
    "print(c.get_query())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблица построена. Проверяем работоспособность - строим классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "\n",
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 2)\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set('spark.driver.memory','4g')\n",
    "        .set(\"spark.executor.memory\", '2g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)\n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def metrics(y_true,y_score,lift = None, return_str = False):\n",
    "    import sklearn\n",
    "    import collections\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        res = collections.OrderedDict()\n",
    "        samp_size = len(y_true)\n",
    "        res['Sample size'] = samp_size\n",
    "        res['Posit share'] = sum(y_true) * 1./ samp_size\n",
    "        res['Sample size'] = len(y_true)\n",
    "        res['AUC ROC'] = sklearn.metrics.roc_auc_score(y_true = y_true, y_score = y_score)\n",
    "        res['AUC PR'] = sklearn.metrics.average_precision_score( y_true,  y_score)\n",
    "        res['Log loss'] = sklearn.metrics.log_loss(y_true = y_true, y_pred = y_score)\n",
    "        if lift:\n",
    "            predictions_and_labels = sorted(zip(y_score,y_true), key = lambda e:-e[0])\n",
    "            for l in lift:\n",
    "                res['Lift ' + str(l)] = sum([e[1] for e in predictions_and_labels[:int(l * samp_size)]]) * 1. / int(l * samp_size) / res['Posit share']                \n",
    "        if return_str:\n",
    "            res = '\\n'.join(['{:<12}: {:.5f}'.format(k,v) for (k,v) in res.items()]) + '.'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv(clf, X, y, folds = 5, metrics = 'roc_auc,pr_auc'):\n",
    "    '''calc cross-validation metrics for clf classfier on X,y data.\n",
    "       clf must support predict_proba method.\n",
    "       Returns dictionary with metrics values on each fold.\n",
    "    '''\n",
    "    assert X.shape[0] == len(y), 'X and y lengths doesnt match'\n",
    "    idx = range(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    fold_idx = []\n",
    "    for i in range(folds):\n",
    "        fold_idx.append(idx[i*len(idx)/folds:(i+1)*len(idx)/folds])\n",
    "    res = {k:[] for k in metrics.split(',')}\n",
    "    \n",
    "    for i in range(folds):\n",
    "        train_idx = reduce(lambda x,y: x+y,(fold_idx[:i] + fold_idx[(i+1):]))\n",
    "        valid_idx = fold_idx[i]\n",
    "        \n",
    "        clf.fit(X[train_idx],y[train_idx])\n",
    "        valid_pred = clf.predict_proba(X[valid_idx])[:,1]\n",
    "        \n",
    "        if('roc_auc' in metrics):\n",
    "            res['roc_auc'].append(\n",
    "                sklearn.metrics.roc_auc_score(\n",
    "                  y_true = y[valid_idx],\n",
    "                  y_score = valid_pred\n",
    "                )\n",
    "            )\n",
    "        if('pr_auc' in metrics):\n",
    "            res['pr_auc'].append(\n",
    "                sklearn.metrics.average_precision_score(\n",
    "                  y_true = y[valid_idx],\n",
    "                  y_score = valid_pred\n",
    "                )\n",
    "            )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import os\n",
    "import sklearn, sklearn.cross_validation\n",
    "from pylightgbm.models import GBMClassifier\n",
    "\n",
    "federal_districts = [u'ЦФО',u'СЗФО',u'ЮФО',u'СКФО',u'ПФО',u'УФО',u'СФО',u'ДВФО'] # todo\n",
    "\n",
    "def encode(v, classes, default_value = -1):\n",
    "    '''Encode text value v which values are from classes list. Returns v index and -1 if it wasn't found in the list.'''\n",
    "    try:\n",
    "        return classes.index(v)\n",
    "    except ValueError:\n",
    "        return default_value\n",
    "\n",
    "\n",
    "exec_path = \"/opt/share/LightGBM-master/lightgbm\"\n",
    "os.environ[\"LIGHTGBM_EXEC\"] = exec_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((333259, 47), (28946, 47))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = (hc.sql('select * from user_kposminin.id_feat_ccall')        \n",
    "        .toPandas()\n",
    "         )\n",
    "\n",
    "#df_all.loc[:,'federal_district'] = df_all.federal_district.map(lambda v:encode(v, federal_districts))\n",
    "df_train = df_all[df_all['call_ymd'] <  '2016-12-01']\n",
    "df_test  = df_all[df_all['call_ymd'] >= '2016-12-01']\n",
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "categ_vars = [ 'mob_provider',u'federal_district']\n",
    "le = {v:preprocessing.LabelEncoder() for v in categ_vars}\n",
    "for v in categ_vars:\n",
    "    df_train.loc[:,v] = le[v].fit_transform(df_train[v])\n",
    "    df_test.loc[:,v]  = le[v].transform(df_test[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_all.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def target_encoding(cat_vars, label, df_train, df_test, folds = 5, smooth_N = 100, inplace = False):\n",
    "    '''\n",
    "       Make target encoding of categorical_variable from cat_vars list using label.\n",
    "       df_train, df_test are Pandas.DataFrame instances.\n",
    "       folds is number of folds to split.\n",
    "       smooth_N is smoothing number.\n",
    "       returns Pandas DataFrames with categorical_variable + '_encoded' columns for train and test.\n",
    "    '''\n",
    "    assert type(df_train) == type(df_test) == type(pd.DataFrame()), 'df_train and df_test must be Pandas DataFrame.'\n",
    "    assert all(v in df_train.columns for v in cat_vars + [label]) & \\\n",
    "           all(v in df_test.columns for v in cat_vars + [label]),   \\\n",
    "           'all categorical variables and label must be df_train and df_test columns'\n",
    "    \n",
    "    idx = df_train.index.tolist()\n",
    "    np.random.shuffle(idx)\n",
    "    fold_idx = []    \n",
    "    for i in range(folds):\n",
    "        fold_idx.append(idx[i*len(idx)/folds:(i+1)*len(idx)/folds])\n",
    "    \n",
    "    df_train_encoded = pd.DataFrame(index = idx, columns = [v + '_encoded' for v in cat_vars])\n",
    "    df_test_encoded = pd.DataFrame(index = df_test.index, columns = [v + '_encoded' for v in cat_vars])\n",
    "    \n",
    "    for i in range(folds):\n",
    "        train_idx = reduce(lambda x,y: x+y,(fold_idx[:i] + fold_idx[(i+1):]))\n",
    "        valid_idx = fold_idx[i]\n",
    "        \n",
    "        global_mean = df_train.ix[train_idx,label].mean()\n",
    "        for v in cat_vars:\n",
    "            print(v)\n",
    "            v_stat = df_train.ix[train_idx].groupby(v)[label].agg([sum,len])\n",
    "            df_train_encoded.ix[valid_idx,v + '_encoded'] = df_train.ix[valid_idx,v].map( lambda c:\n",
    "             (v_stat.loc[c,'sum'] + smooth_N * global_mean) / (v_stat.loc[c,'len'] + smooth_N) if c in v_stat.index else global_mean)\n",
    "    \n",
    "    for v in cat_vars:\n",
    "        v_stat = df_train.groupby(v)[label].agg([sum,len])\n",
    "        df_test_encoded.loc[:,v + '_encoded'] = df_test.loc[:,v].map( lambda c:\n",
    "             (v_stat.loc[c,'sum'] + smooth_N * global_mean) / (v_stat.loc[c,'len'] + smooth_N) if c in v_stat.index else global_mean)\n",
    "        \n",
    "    if inplace == True:\n",
    "        df_train = df_train.join(df_train_encoded)\n",
    "        df_test = df_test.join(df_test_encoded)\n",
    "    else:\n",
    "        return df_train_encoded, df_test_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mob_provider\n",
      "federal_district\n",
      "mob_provider\n",
      "federal_district\n",
      "mob_provider\n",
      "federal_district\n"
     ]
    }
   ],
   "source": [
    "df_train_enc,df_test_enc =  target_encoding(cat_vars = [ 'mob_provider',u'federal_district'], label = 'approve', df_train = df_train, df_test = df_test, folds = 3, smooth_N = 100)\n",
    "#df_train.columns\n",
    "df_train = df_train.join(df_train_enc)\n",
    "df_test  = df_test.join(df_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_list = [\n",
    "    df_train.columns[5:],\n",
    "    [c for c in df_train.columns[5:] if not 'score5' in c], # good ??\n",
    "    [c for c in df_train.columns[5:] if not 'score4' in c], \n",
    "    [c for c in df_train.columns[5:] if not 'score1' in c], \n",
    "    [c for c in df_train.columns[5:] if not any('score{}'.format(i) in c for i in list('236'))],\n",
    "    df_train.columns[5:-11], # bad\n",
    "    df_train.columns[5:-17], # bad\n",
    "    df_train.columns[5:]  + df_train.columns[1:2],\n",
    "    [c for c in (df_train.columns[5:]  + df_train.columns[1:2]) if not 'score5' in c], # good ??\n",
    "    [c for c in (df_train.columns[5:-2]  + df_train.columns[1:2]) if not 'score5' in c],\n",
    "    df_train.columns[5:-2] + df_train.columns[1:2],   \n",
    "    [c for c in (df_train.columns[5:]  + df_train.columns[1:2]) if not 'share' in c]\n",
    "]\n",
    "\n",
    "label = 'approve'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-31 13:52:58.993424\n",
      "2017-05-31 14:27:04.858684\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "result = []\n",
    "for i in range(len(feat_list)):\n",
    "    clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf=50,\n",
    "       # is_unbalance = True,\n",
    "        num_iterations = 120,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = 127,\n",
    "        learning_rate = 0.05,\n",
    "        metric = 'auc',\n",
    "        verbose = False\n",
    "    )\n",
    "    result.append((i,feat_list[i], cv(clf, df_train[feat_list[i]].values, df_train[label].values, folds = 5, metrics = 'roc_auc,pr_auc')))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feats</th>\n",
       "      <th>metrics</th>\n",
       "      <th>avg_auc_roc</th>\n",
       "      <th>std_auc_roc</th>\n",
       "      <th>min_auc_roc</th>\n",
       "      <th>avg_auc_pr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Index([               u'urlfr_cnt',           ...</td>\n",
       "      <td>{u'pr_auc': [0.511794013588, 0.513386253207, 0...</td>\n",
       "      <td>0.668986</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.667104</td>\n",
       "      <td>0.516982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[urlfr_cnt, visits_cnt, hits, max_score1, avg_...</td>\n",
       "      <td>{u'pr_auc': [0.515478496553, 0.51499617237, 0....</td>\n",
       "      <td>0.669223</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.668141</td>\n",
       "      <td>0.516767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[urlfr_cnt, visits_cnt, hits, max_score1, avg_...</td>\n",
       "      <td>{u'pr_auc': [0.516846995118, 0.512782025885, 0...</td>\n",
       "      <td>0.668198</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>0.515902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[urlfr_cnt, visits_cnt, hits, max_score2, avg_...</td>\n",
       "      <td>{u'pr_auc': [0.511834229821, 0.506280271948, 0...</td>\n",
       "      <td>0.659296</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.657187</td>\n",
       "      <td>0.508054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[urlfr_cnt, visits_cnt, hits, max_score1, avg_...</td>\n",
       "      <td>{u'pr_auc': [0.513497564319, 0.521698216584, 0...</td>\n",
       "      <td>0.666274</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.664360</td>\n",
       "      <td>0.514638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Index([u'urlfr_cnt', u'visits_cnt', u'hits', u...</td>\n",
       "      <td>{u'pr_auc': [0.506971324264, 0.503659634598, 0...</td>\n",
       "      <td>0.658319</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.655521</td>\n",
       "      <td>0.504773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Index([u'urlfr_cnt', u'visits_cnt', u'hits', u...</td>\n",
       "      <td>{u'pr_auc': [0.503215024669, 0.505294663653, 0...</td>\n",
       "      <td>0.656823</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.654138</td>\n",
       "      <td>0.503058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Index([               u'urlfr_cnt',           ...</td>\n",
       "      <td>{u'pr_auc': [0.514410032847, 0.51649660086, 0....</td>\n",
       "      <td>0.669756</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.667237</td>\n",
       "      <td>0.517546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              feats  \\\n",
       "0   0  Index([               u'urlfr_cnt',           ...   \n",
       "1   1  [urlfr_cnt, visits_cnt, hits, max_score1, avg_...   \n",
       "2   2  [urlfr_cnt, visits_cnt, hits, max_score1, avg_...   \n",
       "3   3  [urlfr_cnt, visits_cnt, hits, max_score2, avg_...   \n",
       "4   4  [urlfr_cnt, visits_cnt, hits, max_score1, avg_...   \n",
       "5   5  Index([u'urlfr_cnt', u'visits_cnt', u'hits', u...   \n",
       "6   6  Index([u'urlfr_cnt', u'visits_cnt', u'hits', u...   \n",
       "7   7  Index([               u'urlfr_cnt',           ...   \n",
       "\n",
       "                                             metrics  avg_auc_roc  \\\n",
       "0  {u'pr_auc': [0.511794013588, 0.513386253207, 0...     0.668986   \n",
       "1  {u'pr_auc': [0.515478496553, 0.51499617237, 0....     0.669223   \n",
       "2  {u'pr_auc': [0.516846995118, 0.512782025885, 0...     0.668198   \n",
       "3  {u'pr_auc': [0.511834229821, 0.506280271948, 0...     0.659296   \n",
       "4  {u'pr_auc': [0.513497564319, 0.521698216584, 0...     0.666274   \n",
       "5  {u'pr_auc': [0.506971324264, 0.503659634598, 0...     0.658319   \n",
       "6  {u'pr_auc': [0.503215024669, 0.505294663653, 0...     0.656823   \n",
       "7  {u'pr_auc': [0.514410032847, 0.51649660086, 0....     0.669756   \n",
       "\n",
       "   std_auc_roc  min_auc_roc  avg_auc_pr  \n",
       "0     0.001475     0.667104    0.516982  \n",
       "1     0.001317     0.668141    0.516767  \n",
       "2     0.001627     0.665775    0.515902  \n",
       "3     0.002111     0.657187    0.508054  \n",
       "4     0.001960     0.664360    0.514638  \n",
       "5     0.001968     0.655521    0.504773  \n",
       "6     0.001969     0.654138    0.503058  \n",
       "7     0.002703     0.667237    0.517546  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.DataFrame(result,columns = ['id','feats','metrics'])\n",
    "df_res['avg_auc_roc'] = df_res['metrics'].map(lambda d: np.mean(d['roc_auc']))\n",
    "df_res['std_auc_roc'] = df_res['metrics'].map(lambda d: np.std(d['roc_auc']))\n",
    "df_res['min_auc_roc'] = df_res['metrics'].map(lambda d: min(d['roc_auc']))\n",
    "df_res['avg_auc_pr'] = df_res['metrics'].map(lambda d: np.mean(d['pr_auc']))\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исключение score5 улучшает модель (?), добавление id_cnt тоже улучшает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 754), 'avg_score1'),\n",
       " ((38, 739), 'urlfr_cnt'),\n",
       " ((27, 735), 'min_score1'),\n",
       " ((5, 615), 'avg_score4'),\n",
       " ((18, 547), 'hour_std'),\n",
       " ((40, 535), 'visits_cnt'),\n",
       " ((13, 517), 'good_urlfr_share_score3'),\n",
       " ((17, 460), 'hits'),\n",
       " ((23, 459), 'max_score3'),\n",
       " ((4, 438), 'avg_score3'),\n",
       " ((11, 401), 'good_urlfr_share_score1'),\n",
       " ((3, 394), 'avg_score2'),\n",
       " ((33, 386), 'mob_provider'),\n",
       " ((6, 362), 'avg_score5'),\n",
       " ((37, 361), 'social_share'),\n",
       " ((35, 354), 'pop_city_share'),\n",
       " ((7, 353), 'avg_score6'),\n",
       " ((14, 348), 'good_urlfr_share_score4'),\n",
       " ((41, 346), 'vk_share'),\n",
       " ((1, 345), 'avg_salary_2015_rub'),\n",
       " ((30, 342), 'min_score4'),\n",
       " ((25, 341), 'max_score5'),\n",
       " ((34, 338), 'mobile_share'),\n",
       " ((16, 337), 'good_urlfr_share_score6'),\n",
       " ((42, 315), 'work_hours_hits_share'),\n",
       " ((31, 304), 'min_score5'),\n",
       " ((21, 301), 'max_score1'),\n",
       " ((22, 299), 'max_score2'),\n",
       " ((29, 293), 'min_score3'),\n",
       " ((32, 283), 'min_score6'),\n",
       " ((0, 278), 'area_sq_km'),\n",
       " ((15, 278), 'good_urlfr_share_score5'),\n",
       " ((9, 274), 'emailru_share'),\n",
       " ((8, 265), 'density'),\n",
       " ((28, 253), 'min_score2'),\n",
       " ((39, 232), 'utc_time_zone_val'),\n",
       " ((24, 216), 'max_score4'),\n",
       " ((20, 215), 'ind'),\n",
       " ((26, 181), 'max_score6'),\n",
       " ((19, 107), 'id_cnt'),\n",
       " ((12, 95), 'good_urlfr_share_score2'),\n",
       " ((10, 75), 'federal_district'),\n",
       " ((36, 49), 'pop_country_share')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Feature importances')\n",
    "sorted(zip(sorted(clf.feature_importance().items()),yet_another_feat_list[-1]),key = lambda v:-v[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод: берем все факторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Далее тюним параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = df_train.columns[1:2] + df_train.columns[5:]\n",
    "label = 'approve'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ставим высокий learning rate и определяем для него число деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading data in 4.245451 seconds\n",
      "[LightGBM] [Info] Number of postive: 97346, number of negative: 169262\n",
      "[LightGBM] [Info] Number of data: 266608, number of features: 45\n",
      "[LightGBM] [Info] Finished initializing training\n",
      "[LightGBM] [Info] Started training...\n",
      "[LightGBM] [Info] 0.301915 seconds elapsed, finished iteration 1\n",
      "[LightGBM] [Info] 0.559169 seconds elapsed, finished iteration 2\n",
      "[LightGBM] [Info] 0.803832 seconds elapsed, finished iteration 3\n",
      "[LightGBM] [Info] 1.015187 seconds elapsed, finished iteration 4\n",
      "[LightGBM] [Info] 1.237167 seconds elapsed, finished iteration 5\n",
      "[LightGBM] [Info] 1.452098 seconds elapsed, finished iteration 6\n",
      "[LightGBM] [Info] 1.674369 seconds elapsed, finished iteration 7\n",
      "[LightGBM] [Info] 1.902531 seconds elapsed, finished iteration 8\n",
      "[LightGBM] [Info] 2.152716 seconds elapsed, finished iteration 9\n",
      "[LightGBM] [Info] 2.512865 seconds elapsed, finished iteration 10\n",
      "[LightGBM] [Info] 2.862582 seconds elapsed, finished iteration 11\n",
      "[LightGBM] [Info] 3.112670 seconds elapsed, finished iteration 12\n",
      "[LightGBM] [Info] 3.460999 seconds elapsed, finished iteration 13\n",
      "[LightGBM] [Info] 3.715526 seconds elapsed, finished iteration 14\n",
      "[LightGBM] [Info] 3.975631 seconds elapsed, finished iteration 15\n",
      "[LightGBM] [Info] 4.245366 seconds elapsed, finished iteration 16\n",
      "[LightGBM] [Info] 4.509178 seconds elapsed, finished iteration 17\n",
      "[LightGBM] [Info] 4.750797 seconds elapsed, finished iteration 18\n",
      "[LightGBM] [Info] 4.994874 seconds elapsed, finished iteration 19\n",
      "[LightGBM] [Info] 5.288048 seconds elapsed, finished iteration 20\n",
      "[LightGBM] [Info] 5.579671 seconds elapsed, finished iteration 21\n",
      "[LightGBM] [Info] 5.817532 seconds elapsed, finished iteration 22\n",
      "[LightGBM] [Info] 6.054275 seconds elapsed, finished iteration 23\n",
      "[LightGBM] [Info] 6.279896 seconds elapsed, finished iteration 24\n",
      "[LightGBM] [Info] 6.494276 seconds elapsed, finished iteration 25\n",
      "[LightGBM] [Info] 6.737500 seconds elapsed, finished iteration 26\n",
      "[LightGBM] [Info] 6.963761 seconds elapsed, finished iteration 27\n",
      "[LightGBM] [Info] 7.179923 seconds elapsed, finished iteration 28\n",
      "[LightGBM] [Info] 7.394518 seconds elapsed, finished iteration 29\n",
      "[LightGBM] [Info] 7.605463 seconds elapsed, finished iteration 30\n",
      "[LightGBM] [Info] Finished training\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 30 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction"
     ]
    }
   ],
   "source": [
    "clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf=100,\n",
    "       # is_unbalance = True,\n",
    "        num_iterations = 120,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = 127,\n",
    "        learning_rate = 0.1,\n",
    "        metric = 'auc',\n",
    "    )\n",
    "res = []\n",
    "for n_trees in [30,50,100,150,250]:\n",
    "    clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf=100,\n",
    "       # is_unbalance = True,\n",
    "        num_iterations = n_trees,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = 127,\n",
    "        learning_rate = 0.1,\n",
    "        metric = 'auc',\n",
    "        verbose = False\n",
    "    )\n",
    "    res.append((n_trees, cv(clf, df_train[feats].values, df_train[label].values, folds = 5, metrics = 'roc_auc,pr_auc')))  \n",
    "\n",
    "print('\\n'.join('{} {}'.format(e[0],np.mean(e[1]['roc_auc'])) for e in res))\n",
    "\n",
    "#df_res = pd.DataFrame(result,columns = ['id','feats','metrics'])\n",
    "#df_res['avg_auc_roc'] = df_res['metrics'].map(lambda d: np.mean(d['roc_auc']))\n",
    "#df_res['std_auc_roc'] = df_res['metrics'].map(lambda d: np.std(d['roc_auc']))\n",
    "#df_res['min_auc_roc'] = df_res['metrics'].map(lambda d: min(d['roc_auc']))\n",
    "#df_res['avg_auc_pr'] = df_res['metrics'].map(lambda d: np.mean(d['pr_auc']))\n",
    "#df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                  u'id_cnt',                u'urlfr_cnt',\n",
       "                     u'visits_cnt',                     u'hits',\n",
       "                     u'max_score1',               u'avg_score1',\n",
       "                     u'min_score1',               u'max_score2',\n",
       "                     u'avg_score2',               u'min_score2',\n",
       "                     u'max_score3',               u'avg_score3',\n",
       "                     u'min_score3',               u'max_score4',\n",
       "                     u'avg_score4',               u'min_score4',\n",
       "                     u'max_score5',               u'avg_score5',\n",
       "                     u'min_score5',               u'max_score6',\n",
       "                     u'avg_score6',               u'min_score6',\n",
       "                  u'emailru_share',             u'mobile_share',\n",
       "                       u'vk_share',             u'social_share',\n",
       "          u'work_hours_hits_share',                 u'hour_std',\n",
       "        u'good_urlfr_share_score1',  u'good_urlfr_share_score2',\n",
       "        u'good_urlfr_share_score3',  u'good_urlfr_share_score4',\n",
       "        u'good_urlfr_share_score5',  u'good_urlfr_share_score6',\n",
       "                   u'mob_provider',                      u'ind',\n",
       "              u'pop_country_share',           u'pop_city_share',\n",
       "                        u'density',               u'area_sq_km',\n",
       "               u'federal_district',      u'avg_salary_2015_rub',\n",
       "              u'utc_time_zone_val',     u'mob_provider_encoded',\n",
       "       u'federal_district_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'min_data_in_leaf': [10,50,100,200,500],\n",
    "    'num_leaves': [15,31,63,127,255],\n",
    "    'bagging_fraction': [0.5,0.8, 1],\n",
    "    'bagging_freq': [5,10,30],\n",
    "    'feature_fraction': [0.5, 0.8, 1]\n",
    "}\n",
    "\n",
    "clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf = 100,\n",
    "       # is_unbalance = True,\n",
    "        num_iterations = 70,\n",
    "        bagging_fraction = 1,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = 15,\n",
    "        learning_rate = 0.1,\n",
    "        feature_fraction = 0.8,\n",
    "        metric = 'auc',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "param_set = (\n",
    "    {'learning_rate':0.1, 'num_iterations':50, 'num_leaves':127},\n",
    "    {'learning_rate':0.1, 'num_iterations':70, 'num_leaves':15},\n",
    "    {'learning_rate':0.08, 'num_iterations':100, 'num_leaves':15},\n",
    "    {'learning_rate':0.08, 'num_iterations':100, 'num_leaves':31},\n",
    "    {'learning_rate':0.06, 'num_iterations':150, 'num_leaves':31},\n",
    "    {'learning_rate':0.06, 'num_iterations':150, 'num_leaves':15},\n",
    "    {'learning_rate':0.05, 'num_iterations':200, 'num_leaves':31},\n",
    "    {'learning_rate':0.04, 'num_iterations':200, 'num_leaves':63},\n",
    "    {'learning_rate':0.04, 'num_iterations':300, 'num_leaves':63},\n",
    "    {'learning_rate':0.03, 'num_iterations':400, 'num_leaves':31},\n",
    "    {'learning_rate':0.05, 'num_iterations':300, 'num_leaves':31},\n",
    ")\n",
    "\n",
    "\n",
    "#n_iter = 60\n",
    "res= []\n",
    "for param in param_set:\n",
    "    clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf = 100,\n",
    "        # is_unbalance = True,\n",
    "        num_iterations = param['num_iterations'],\n",
    "        bagging_fraction = 1,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = param['num_iterations'],\n",
    "        learning_rate = param['learning_rate'],\n",
    "        feature_fraction = 0.8,\n",
    "        metric = 'auc',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    res.append((param,cv(clf, df_train[feats].values, df_train[label].values, folds = 5, metrics = 'roc_auc,pr_auc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading data in 5.608001 seconds\n",
      "[LightGBM] [Info] Number of postive: 121590, number of negative: 211669\n",
      "[LightGBM] [Info] Number of data: 333259, number of features: 42\n",
      "[LightGBM] [Info] Finished initializing training\n",
      "[LightGBM] [Info] Started training...\n",
      "[LightGBM] [Info] Iteration:1, valid_1 auc : 0.658906\n",
      "[LightGBM] [Info] 0.269187 seconds elapsed, finished iteration 1\n",
      "[LightGBM] [Info] Iteration:2, valid_1 auc : 0.662137\n",
      "[LightGBM] [Info] 0.546721 seconds elapsed, finished iteration 2\n",
      "[LightGBM] [Info] Iteration:3, valid_1 auc : 0.665258\n",
      "[LightGBM] [Info] 0.841890 seconds elapsed, finished iteration 3\n",
      "[LightGBM] [Info] Iteration:4, valid_1 auc : 0.666226\n",
      "[LightGBM] [Info] 1.256363 seconds elapsed, finished iteration 4\n",
      "[LightGBM] [Info] Iteration:5, valid_1 auc : 0.668245\n",
      "[LightGBM] [Info] 1.621473 seconds elapsed, finished iteration 5\n",
      "[LightGBM] [Info] Iteration:6, valid_1 auc : 0.668524\n",
      "[LightGBM] [Info] 1.961882 seconds elapsed, finished iteration 6\n",
      "[LightGBM] [Info] Iteration:7, valid_1 auc : 0.669038\n",
      "[LightGBM] [Info] 2.250237 seconds elapsed, finished iteration 7\n",
      "[LightGBM] [Info] Iteration:8, valid_1 auc : 0.669624\n",
      "[LightGBM] [Info] 2.544642 seconds elapsed, finished iteration 8\n",
      "[LightGBM] [Info] Iteration:9, valid_1 auc : 0.670130\n",
      "[LightGBM] [Info] 2.839785 seconds elapsed, finished iteration 9\n",
      "[LightGBM] [Info] Iteration:10, valid_1 auc : 0.670255\n",
      "[LightGBM] [Info] 3.139344 seconds elapsed, finished iteration 10\n",
      "[LightGBM] [Info] Iteration:11, valid_1 auc : 0.670448\n",
      "[LightGBM] [Info] 3.452181 seconds elapsed, finished iteration 11\n",
      "[LightGBM] [Info] Iteration:12, valid_1 auc : 0.670790\n",
      "[LightGBM] [Info] 3.734255 seconds elapsed, finished iteration 12\n",
      "[LightGBM] [Info] Iteration:13, valid_1 auc : 0.670988\n",
      "[LightGBM] [Info] 4.079287 seconds elapsed, finished iteration 13\n",
      "[LightGBM] [Info] Iteration:14, valid_1 auc : 0.671460\n",
      "[LightGBM] [Info] 4.384985 seconds elapsed, finished iteration 14\n",
      "[LightGBM] [Info] Iteration:15, valid_1 auc : 0.671771\n",
      "[LightGBM] [Info] 4.684692 seconds elapsed, finished iteration 15\n",
      "[LightGBM] [Info] Iteration:16, valid_1 auc : 0.671994\n",
      "[LightGBM] [Info] 4.978013 seconds elapsed, finished iteration 16\n",
      "[LightGBM] [Info] Iteration:17, valid_1 auc : 0.672200\n",
      "[LightGBM] [Info] 5.294537 seconds elapsed, finished iteration 17\n",
      "[LightGBM] [Info] Iteration:18, valid_1 auc : 0.672632\n",
      "[LightGBM] [Info] 5.636645 seconds elapsed, finished iteration 18\n",
      "[LightGBM] [Info] Iteration:19, valid_1 auc : 0.672834\n",
      "[LightGBM] [Info] 5.942714 seconds elapsed, finished iteration 19\n",
      "[LightGBM] [Info] Iteration:20, valid_1 auc : 0.673229\n",
      "[LightGBM] [Info] 6.273267 seconds elapsed, finished iteration 20\n",
      "[LightGBM] [Info] Iteration:21, valid_1 auc : 0.673386\n",
      "[LightGBM] [Info] 6.632092 seconds elapsed, finished iteration 21\n",
      "[LightGBM] [Info] Iteration:22, valid_1 auc : 0.673714\n",
      "[LightGBM] [Info] 6.937803 seconds elapsed, finished iteration 22\n",
      "[LightGBM] [Info] Iteration:23, valid_1 auc : 0.674099\n",
      "[LightGBM] [Info] 7.270912 seconds elapsed, finished iteration 23\n",
      "[LightGBM] [Info] Iteration:24, valid_1 auc : 0.674434\n",
      "[LightGBM] [Info] 7.572519 seconds elapsed, finished iteration 24\n",
      "[LightGBM] [Info] Iteration:25, valid_1 auc : 0.674887\n",
      "[LightGBM] [Info] 7.876475 seconds elapsed, finished iteration 25\n",
      "[LightGBM] [Info] Iteration:26, valid_1 auc : 0.675128\n",
      "[LightGBM] [Info] 8.155282 seconds elapsed, finished iteration 26\n",
      "[LightGBM] [Info] Iteration:27, valid_1 auc : 0.675564\n",
      "[LightGBM] [Info] 8.427316 seconds elapsed, finished iteration 27\n",
      "[LightGBM] [Info] Iteration:28, valid_1 auc : 0.675805\n",
      "[LightGBM] [Info] 8.686052 seconds elapsed, finished iteration 28\n",
      "[LightGBM] [Info] Iteration:29, valid_1 auc : 0.675886\n",
      "[LightGBM] [Info] 9.006598 seconds elapsed, finished iteration 29\n",
      "[LightGBM] [Info] Iteration:30, valid_1 auc : 0.675912\n",
      "[LightGBM] [Info] 9.306941 seconds elapsed, finished iteration 30\n",
      "[LightGBM] [Info] Iteration:31, valid_1 auc : 0.676027\n",
      "[LightGBM] [Info] 9.618401 seconds elapsed, finished iteration 31\n",
      "[LightGBM] [Info] Iteration:32, valid_1 auc : 0.676129\n",
      "[LightGBM] [Info] 9.974640 seconds elapsed, finished iteration 32\n",
      "[LightGBM] [Info] Iteration:33, valid_1 auc : 0.676303\n",
      "[LightGBM] [Info] 10.350024 seconds elapsed, finished iteration 33\n",
      "[LightGBM] [Info] Iteration:34, valid_1 auc : 0.676371\n",
      "[LightGBM] [Info] 10.627781 seconds elapsed, finished iteration 34\n",
      "[LightGBM] [Info] Iteration:35, valid_1 auc : 0.676435\n",
      "[LightGBM] [Info] 10.975627 seconds elapsed, finished iteration 35\n",
      "[LightGBM] [Info] Iteration:36, valid_1 auc : 0.676572\n",
      "[LightGBM] [Info] 11.332126 seconds elapsed, finished iteration 36\n",
      "[LightGBM] [Info] Iteration:37, valid_1 auc : 0.676668\n",
      "[LightGBM] [Info] 11.664724 seconds elapsed, finished iteration 37\n",
      "[LightGBM] [Info] Iteration:38, valid_1 auc : 0.676786\n",
      "[LightGBM] [Info] 11.960829 seconds elapsed, finished iteration 38\n",
      "[LightGBM] [Info] Iteration:39, valid_1 auc : 0.676929\n",
      "[LightGBM] [Info] 12.250090 seconds elapsed, finished iteration 39\n",
      "[LightGBM] [Info] Iteration:40, valid_1 auc : 0.677013\n",
      "[LightGBM] [Info] 12.556711 seconds elapsed, finished iteration 40\n",
      "[LightGBM] [Info] Iteration:41, valid_1 auc : 0.677169\n",
      "[LightGBM] [Info] 12.835021 seconds elapsed, finished iteration 41\n",
      "[LightGBM] [Info] Iteration:42, valid_1 auc : 0.677362\n",
      "[LightGBM] [Info] 13.101031 seconds elapsed, finished iteration 42\n",
      "[LightGBM] [Info] Iteration:43, valid_1 auc : 0.677627\n",
      "[LightGBM] [Info] 13.385040 seconds elapsed, finished iteration 43\n",
      "[LightGBM] [Info] Iteration:44, valid_1 auc : 0.677685\n",
      "[LightGBM] [Info] 13.748253 seconds elapsed, finished iteration 44\n",
      "[LightGBM] [Info] Iteration:45, valid_1 auc : 0.677726\n",
      "[LightGBM] [Info] 14.100955 seconds elapsed, finished iteration 45\n",
      "[LightGBM] [Info] Iteration:46, valid_1 auc : 0.678085\n",
      "[LightGBM] [Info] 14.428398 seconds elapsed, finished iteration 46\n",
      "[LightGBM] [Info] Iteration:47, valid_1 auc : 0.678068\n",
      "[LightGBM] [Info] 14.756279 seconds elapsed, finished iteration 47\n",
      "[LightGBM] [Info] Iteration:48, valid_1 auc : 0.678160\n",
      "[LightGBM] [Info] 15.153964 seconds elapsed, finished iteration 48\n",
      "[LightGBM] [Info] Iteration:49, valid_1 auc : 0.678414\n",
      "[LightGBM] [Info] 15.509873 seconds elapsed, finished iteration 49\n",
      "[LightGBM] [Info] Iteration:50, valid_1 auc : 0.678521\n",
      "[LightGBM] [Info] 15.866051 seconds elapsed, finished iteration 50\n",
      "[LightGBM] [Info] Iteration:51, valid_1 auc : 0.678686\n",
      "[LightGBM] [Info] 16.235314 seconds elapsed, finished iteration 51\n",
      "[LightGBM] [Info] Iteration:52, valid_1 auc : 0.678901\n",
      "[LightGBM] [Info] 16.494923 seconds elapsed, finished iteration 52\n",
      "[LightGBM] [Info] Iteration:53, valid_1 auc : 0.678994\n",
      "[LightGBM] [Info] 16.766283 seconds elapsed, finished iteration 53\n",
      "[LightGBM] [Info] Iteration:54, valid_1 auc : 0.679108\n",
      "[LightGBM] [Info] 17.058944 seconds elapsed, finished iteration 54\n",
      "[LightGBM] [Info] Iteration:55, valid_1 auc : 0.679201\n",
      "[LightGBM] [Info] 17.452048 seconds elapsed, finished iteration 55\n",
      "[LightGBM] [Info] Iteration:56, valid_1 auc : 0.679368\n",
      "[LightGBM] [Info] 17.778193 seconds elapsed, finished iteration 56\n",
      "[LightGBM] [Info] Iteration:57, valid_1 auc : 0.679560\n",
      "[LightGBM] [Info] 18.134108 seconds elapsed, finished iteration 57\n",
      "[LightGBM] [Info] Iteration:58, valid_1 auc : 0.679526\n",
      "[LightGBM] [Info] 18.429842 seconds elapsed, finished iteration 58\n",
      "[LightGBM] [Info] Iteration:59, valid_1 auc : 0.679822\n",
      "[LightGBM] [Info] 18.689162 seconds elapsed, finished iteration 59\n",
      "[LightGBM] [Info] Iteration:60, valid_1 auc : 0.679942\n",
      "[LightGBM] [Info] 18.963681 seconds elapsed, finished iteration 60\n",
      "[LightGBM] [Info] Iteration:61, valid_1 auc : 0.680107\n",
      "[LightGBM] [Info] 19.354401 seconds elapsed, finished iteration 61\n",
      "[LightGBM] [Info] Iteration:62, valid_1 auc : 0.680120\n",
      "[LightGBM] [Info] 19.687792 seconds elapsed, finished iteration 62\n",
      "[LightGBM] [Info] Iteration:63, valid_1 auc : 0.680176\n",
      "[LightGBM] [Info] 20.026795 seconds elapsed, finished iteration 63\n",
      "[LightGBM] [Info] Iteration:64, valid_1 auc : 0.680313\n",
      "[LightGBM] [Info] 20.393602 seconds elapsed, finished iteration 64\n",
      "[LightGBM] [Info] Iteration:65, valid_1 auc : 0.680433\n",
      "[LightGBM] [Info] 20.706924 seconds elapsed, finished iteration 65\n",
      "[LightGBM] [Info] Iteration:66, valid_1 auc : 0.680572\n",
      "[LightGBM] [Info] 21.113340 seconds elapsed, finished iteration 66\n",
      "[LightGBM] [Info] Iteration:67, valid_1 auc : 0.680742\n",
      "[LightGBM] [Info] 21.515529 seconds elapsed, finished iteration 67\n",
      "[LightGBM] [Info] Iteration:68, valid_1 auc : 0.680956\n",
      "[LightGBM] [Info] 21.787040 seconds elapsed, finished iteration 68\n",
      "[LightGBM] [Info] Iteration:69, valid_1 auc : 0.681152\n",
      "[LightGBM] [Info] 22.153011 seconds elapsed, finished iteration 69\n",
      "[LightGBM] [Info] Iteration:70, valid_1 auc : 0.681395\n",
      "[LightGBM] [Info] 22.545105 seconds elapsed, finished iteration 70\n",
      "[LightGBM] [Info] Iteration:71, valid_1 auc : 0.681482\n",
      "[LightGBM] [Info] 22.851952 seconds elapsed, finished iteration 71\n",
      "[LightGBM] [Info] Iteration:72, valid_1 auc : 0.681511\n",
      "[LightGBM] [Info] 23.200412 seconds elapsed, finished iteration 72\n",
      "[LightGBM] [Info] Iteration:73, valid_1 auc : 0.681474\n",
      "[LightGBM] [Info] 23.487807 seconds elapsed, finished iteration 73\n",
      "[LightGBM] [Info] Iteration:74, valid_1 auc : 0.681458\n",
      "[LightGBM] [Info] 23.719778 seconds elapsed, finished iteration 74\n",
      "[LightGBM] [Info] Iteration:75, valid_1 auc : 0.681492\n",
      "[LightGBM] [Info] 23.961410 seconds elapsed, finished iteration 75\n",
      "[LightGBM] [Info] Iteration:76, valid_1 auc : 0.681570\n",
      "[LightGBM] [Info] 24.178622 seconds elapsed, finished iteration 76\n",
      "[LightGBM] [Info] Iteration:77, valid_1 auc : 0.681494\n",
      "[LightGBM] [Info] 24.397066 seconds elapsed, finished iteration 77\n",
      "[LightGBM] [Info] Iteration:78, valid_1 auc : 0.681490\n",
      "[LightGBM] [Info] 24.683557 seconds elapsed, finished iteration 78\n",
      "[LightGBM] [Info] Iteration:79, valid_1 auc : 0.681580\n",
      "[LightGBM] [Info] 24.906821 seconds elapsed, finished iteration 79\n",
      "[LightGBM] [Info] Iteration:80, valid_1 auc : 0.681548\n",
      "[LightGBM] [Info] 25.158339 seconds elapsed, finished iteration 80\n",
      "[LightGBM] [Info] Iteration:81, valid_1 auc : 0.681588\n",
      "[LightGBM] [Info] 25.401523 seconds elapsed, finished iteration 81\n",
      "[LightGBM] [Info] Iteration:82, valid_1 auc : 0.681638\n",
      "[LightGBM] [Info] 25.584423 seconds elapsed, finished iteration 82\n",
      "[LightGBM] [Info] Iteration:83, valid_1 auc : 0.681761\n",
      "[LightGBM] [Info] 25.822577 seconds elapsed, finished iteration 83\n",
      "[LightGBM] [Info] Iteration:84, valid_1 auc : 0.681845\n",
      "[LightGBM] [Info] 25.998255 seconds elapsed, finished iteration 84\n",
      "[LightGBM] [Info] Iteration:85, valid_1 auc : 0.681908\n",
      "[LightGBM] [Info] 26.315656 seconds elapsed, finished iteration 85\n",
      "[LightGBM] [Info] Iteration:86, valid_1 auc : 0.682093\n",
      "[LightGBM] [Info] 26.578669 seconds elapsed, finished iteration 86\n",
      "[LightGBM] [Info] Iteration:87, valid_1 auc : 0.682154\n",
      "[LightGBM] [Info] 26.830376 seconds elapsed, finished iteration 87\n",
      "[LightGBM] [Info] Iteration:88, valid_1 auc : 0.682293\n",
      "[LightGBM] [Info] 27.067974 seconds elapsed, finished iteration 88\n",
      "[LightGBM] [Info] Iteration:89, valid_1 auc : 0.682218\n",
      "[LightGBM] [Info] 27.231249 seconds elapsed, finished iteration 89\n",
      "[LightGBM] [Info] Iteration:90, valid_1 auc : 0.682282\n",
      "[LightGBM] [Info] 27.412702 seconds elapsed, finished iteration 90\n",
      "[LightGBM] [Info] Iteration:91, valid_1 auc : 0.682302\n",
      "[LightGBM] [Info] 27.640130 seconds elapsed, finished iteration 91\n",
      "[LightGBM] [Info] Iteration:92, valid_1 auc : 0.682434\n",
      "[LightGBM] [Info] 27.847725 seconds elapsed, finished iteration 92\n",
      "[LightGBM] [Info] Iteration:93, valid_1 auc : 0.682361\n",
      "[LightGBM] [Info] 28.108975 seconds elapsed, finished iteration 93\n",
      "[LightGBM] [Info] Iteration:94, valid_1 auc : 0.682277\n",
      "[LightGBM] [Info] 28.279623 seconds elapsed, finished iteration 94\n",
      "[LightGBM] [Info] Iteration:95, valid_1 auc : 0.682290\n",
      "[LightGBM] [Info] 28.489612 seconds elapsed, finished iteration 95\n",
      "[LightGBM] [Info] Iteration:96, valid_1 auc : 0.682328\n",
      "[LightGBM] [Info] 28.722681 seconds elapsed, finished iteration 96\n",
      "[LightGBM] [Info] Iteration:97, valid_1 auc : 0.682437\n",
      "[LightGBM] [Info] 28.917432 seconds elapsed, finished iteration 97\n",
      "[LightGBM] [Info] Iteration:98, valid_1 auc : 0.682405\n",
      "[LightGBM] [Info] 29.113685 seconds elapsed, finished iteration 98\n",
      "[LightGBM] [Info] Iteration:99, valid_1 auc : 0.682477\n",
      "[LightGBM] [Info] 29.315595 seconds elapsed, finished iteration 99\n",
      "[LightGBM] [Info] Iteration:100, valid_1 auc : 0.682314\n",
      "[LightGBM] [Info] 29.491813 seconds elapsed, finished iteration 100\n",
      "[LightGBM] [Info] Iteration:101, valid_1 auc : 0.682290\n",
      "[LightGBM] [Info] 29.748224 seconds elapsed, finished iteration 101\n",
      "[LightGBM] [Info] Iteration:102, valid_1 auc : 0.682241\n",
      "[LightGBM] [Info] 30.031480 seconds elapsed, finished iteration 102\n",
      "[LightGBM] [Info] Iteration:103, valid_1 auc : 0.682119\n",
      "[LightGBM] [Info] 30.281290 seconds elapsed, finished iteration 103\n",
      "[LightGBM] [Info] Iteration:104, valid_1 auc : 0.682082\n",
      "[LightGBM] [Info] 30.613943 seconds elapsed, finished iteration 104\n",
      "[LightGBM] [Info] Iteration:105, valid_1 auc : 0.682041\n",
      "[LightGBM] [Info] 30.895437 seconds elapsed, finished iteration 105\n",
      "[LightGBM] [Info] Iteration:106, valid_1 auc : 0.682123\n",
      "[LightGBM] [Info] 31.083564 seconds elapsed, finished iteration 106\n",
      "[LightGBM] [Info] Iteration:107, valid_1 auc : 0.682316\n",
      "[LightGBM] [Info] 31.373364 seconds elapsed, finished iteration 107\n",
      "[LightGBM] [Info] Iteration:108, valid_1 auc : 0.682247\n",
      "[LightGBM] [Info] 31.616463 seconds elapsed, finished iteration 108\n",
      "[LightGBM] [Info] Iteration:109, valid_1 auc : 0.682187\n",
      "[LightGBM] [Info] 31.788513 seconds elapsed, finished iteration 109\n",
      "[LightGBM] [Info] Iteration:110, valid_1 auc : 0.682221\n",
      "[LightGBM] [Info] 32.030137 seconds elapsed, finished iteration 110\n",
      "[LightGBM] [Info] Iteration:111, valid_1 auc : 0.682056\n",
      "[LightGBM] [Info] 32.239258 seconds elapsed, finished iteration 111\n",
      "[LightGBM] [Info] Iteration:112, valid_1 auc : 0.681856\n",
      "[LightGBM] [Info] 32.489463 seconds elapsed, finished iteration 112\n",
      "[LightGBM] [Info] Iteration:113, valid_1 auc : 0.681825\n",
      "[LightGBM] [Info] 32.689232 seconds elapsed, finished iteration 113\n",
      "[LightGBM] [Info] Iteration:114, valid_1 auc : 0.681712\n",
      "[LightGBM] [Info] 32.884318 seconds elapsed, finished iteration 114\n",
      "[LightGBM] [Info] Iteration:115, valid_1 auc : 0.681615\n",
      "[LightGBM] [Info] 33.129401 seconds elapsed, finished iteration 115\n",
      "[LightGBM] [Info] Iteration:116, valid_1 auc : 0.681604\n",
      "[LightGBM] [Info] 33.337727 seconds elapsed, finished iteration 116\n",
      "[LightGBM] [Info] Iteration:117, valid_1 auc : 0.681562\n",
      "[LightGBM] [Info] 33.542558 seconds elapsed, finished iteration 117\n",
      "[LightGBM] [Info] Iteration:118, valid_1 auc : 0.681145\n",
      "[LightGBM] [Info] 33.794275 seconds elapsed, finished iteration 118\n",
      "[LightGBM] [Info] Iteration:119, valid_1 auc : 0.681075\n",
      "[LightGBM] [Info] 34.040944 seconds elapsed, finished iteration 119\n",
      "[LightGBM] [Info] Iteration:120, valid_1 auc : 0.681133\n",
      "[LightGBM] [Info] 34.342511 seconds elapsed, finished iteration 120\n",
      "[LightGBM] [Info] Finished training\n"
     ]
    }
   ],
   "source": [
    "clf = GBMClassifier(\n",
    "        exec_path=exec_path,\n",
    "        min_data_in_leaf=50,\n",
    "       # is_unbalance = True,\n",
    "        num_iterations = 120,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 10,\n",
    "        num_leaves = 127,\n",
    "        learning_rate = 0.05,\n",
    "        metric = 'auc',\n",
    "    )\n",
    "#clf.fit(df_train[feats], df_train[label])\n",
    "#df_test['pred'] = clf.predict_proba(df_test[feats])[:,1]\n",
    "\n",
    "clf.fit(df_train[feats], df_train[label], test_data = [(df_test[feats], df_test[label])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Это успех"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 120 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n"
     ]
    }
   ],
   "source": [
    "df_test['pred'] = clf.predict_proba(df_test[feats])[:,1]\n",
    "#print(metrics(y_true = df_test[label], y_score = df_test.pred, lift = None, return_str = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = [f for f in feats if not f == 'mob_provider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 120 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "Sample size : 1928.00000\n",
      "Posit share : 0.48081\n",
      "AUC ROC     : 0.64123\n",
      "AUC PR      : 0.59594\n",
      "Log loss    : 0.69476.\n"
     ]
    }
   ],
   "source": [
    "df_another_test = (hc.sql('select * from user_kposminin.id_feat_20170429')        \n",
    "        .toPandas()\n",
    "         )\n",
    "for v in categ_vars:\n",
    "    df_another_test.loc[:,v] = le[v].fit_transform(df_another_test[v])\n",
    "df_another_test['pred'] = clf.predict_proba(df_another_test[feats])[:,1]\n",
    "print(metrics(y_true = df_another_test[label], y_score = df_another_test.pred, lift = None, return_str = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного маловато. С чем это связано: устаревание модели, неконсистентность данных, выброс за счет малой выборки либо заявки немного разные (обучение + первый тест из выборки Азамата, которая неизвестно как сформирована. another_test сформирован из всех заявок по звонкам)?\n",
    "### Гипотезу малой выборки проверим построением доверительного интервала для AUC ROC бутстреппингом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC percentiles [ 0.66129092  0.66584395  0.67504882  0.68097747  0.68805436  0.69782539\n",
      "  0.70172197]\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(df_another_test.index,3)\n",
    "roc_auc_bootstrap = []\n",
    "for _ in range(1000):\n",
    "    sample_idx = np.random.choice(df_test.index, df_another_test.shape[0], replace = True)\n",
    "    roc_auc_bootstrap.append(sklearn.metrics.roc_auc_score(y_true = df_test.ix[sample_idx,label], y_score = df_test.ix[sample_idx,'pred']))\n",
    "print('ROC AUC percentiles {}'.format(np.percentile(roc_auc_bootstrap,q=[5,10,30,50,70,90,95])))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наблюдаемый AUC ROC не влезает даже в 95% доверительный интервал. Итак, низкий AUC ROC не связан с малой выборкой. \n",
    "#### Возможно, причина в разнородности заявок в обучении и в последнем тесте ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
