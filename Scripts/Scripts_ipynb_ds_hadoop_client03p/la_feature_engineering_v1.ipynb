{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Построение Look-alike модели для целевой аудитории раздела веб-сайта\n",
    "\n",
    "Используются различные подходы:\n",
    "- Логистическая регрессия\n",
    "- Naive Bayes\n",
    "- текущий подход, рассмотренный в Wiki[https://wiki.tcsbank.ru/pages/viewpage.action?pageId=176096365].\n",
    "\n",
    "Сравнение методов производится по метрике AUC ROC.\n",
    "\n",
    "** Модификация - все через Hive, в Spark только само обучение и вывод результатов. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyhs2\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "start = datetime.datetime.now()\n",
    "target_url ='raiffeisen.ru/retail/cards/credit' #['avito.ru/moskva'] #\n",
    "exclude_url = 'raiffeisen.ru' #['avito.ru']  #\n",
    "\n",
    "#source_table_name = 'user_kposminin.access_log_sample'\n",
    "#train_start_date, train_end_date = '2016-06-01', '2016-06-02' \n",
    "#test_date = '2016-06-30'\n",
    "\n",
    "source_table_name = 'user_kposminin.access_log_sample5'\n",
    "train_start_date, train_end_date = '2016-07-07', '2016-07-07'\n",
    "test_date = '2016-07-14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hive queries\n",
    "\n",
    "#based on urlp column\n",
    "\n",
    "calc_id = 19 #int(hc.sql('select max(id) from user_kposminin.calcs').collect()[0][0]) + 1\n",
    "\n",
    "target_expression = '('+'or'.join(' url like \"%' + u + '%\" ' for u in target_urls)+')'\n",
    "exclude_expression = 'not ('+'or'.join(' urlp like \"%' + u + '%\" ' for u in exclude_urls)+')'\n",
    "\n",
    "update_calcs_table_query = '''\n",
    "insert into user_kposminin.calcs values(\n",
    "    {calc_id},\n",
    "    {date},\n",
    "    \"Look alike model comparison\",\"Comparison of NaiveBayes, LogisticRegression and Current approach\",\n",
    "    \"train_start_date: {train_start_date}, train_end_date:train_end_date,\n",
    "        test_date: {test_date}, source_table_name: {source_table_name},\n",
    "        target_urls: {target_urls}, exclude_urls: {exclude_urls}\"\n",
    "    )\n",
    "'''.format(\n",
    "    calc_id = calc_id, \n",
    "    train_start_date = train_start_date, \n",
    "    train_end_date = train_end_date, \n",
    "    test_date = test_date,\n",
    "    source_table_name = source_table_name,\n",
    "    target_urls = str(target_urls),    \n",
    "    exclude_urls = str(exclude_urls),\n",
    "    date = str(datetime.datetime.now().date())\n",
    ")\n",
    "\n",
    "def calc_day(date,calc_id, source_table_name)\n",
    "    create_tables_in_hive_query = '''\n",
    "    drop table if exists user_kposminin.id_up{calc_id};\n",
    "\n",
    "    create table user_kposminin.id_up{calc_id} as\n",
    "    select\n",
    "        a.ymd,\n",
    "        a.id,\n",
    "        concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2)\n",
    "        ) as up,\n",
    "        count(*) as cnt,\n",
    "        max(timestamp) - min(timestamp) as duration,\n",
    "        stddev(timestamp) as timestd\n",
    "    from \n",
    "        (\n",
    "           select ymd, id, url, timestamp\n",
    "           from {source_table_name}\n",
    "           where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "        ) a\n",
    "    group by ymd, id, concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2)\n",
    "        )\n",
    "  \n",
    "    union all\n",
    "    \n",
    "    select\n",
    "        a.ymd,\n",
    "        a.id,\n",
    "        concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3)\n",
    "        ) as up,\n",
    "        count(*) as cnt,\n",
    "        max(timestamp) - min(timestamp) as duration,\n",
    "        stddev(timestamp) as timestd\n",
    "    from \n",
    "        (\n",
    "           select ymd, id, url, timestamp\n",
    "           from {source_table_name}\n",
    "           where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "        ) a\n",
    "    group by ymd, id, concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3)\n",
    "        )\n",
    "\n",
    "    union all\n",
    "    \n",
    "    select\n",
    "        a.ymd,\n",
    "        a.id,\n",
    "        concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4)\n",
    "        ) as up,\n",
    "        count(*) as cnt,\n",
    "        max(timestamp) - min(timestamp) as duration,\n",
    "        stddev(timestamp) as timestd\n",
    "    from \n",
    "        (\n",
    "           select ymd, id, url, timestamp\n",
    "           from {source_table_name}\n",
    "           where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "        ) a\n",
    "    group by ymd, id, concat(\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1),\n",
    "           '[0]',\n",
    "           regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4)\n",
    "        )\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**********************************************************************************************************************\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drop table if exists user_kposminin.urls_w_levels_train26;\n",
      "create table user_kposminin.urls_w_levels_train26 as\n",
      "select\n",
      "    a.id\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
      "    ,a.timestamp\n",
      "from (\n",
      "           select ymd, id, url, timestamp\n",
      "           from user_kposminin.access_log_sample5\n",
      "           where ymd between \"2016-07-07\" and \"2016-07-07\"\n",
      "         ) a\n",
      "where\n",
      "    url not like \"raiffeisen.ru%\"\n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.id_up_train26;\n",
      "\n",
      "create table user_kposminin.id_up_train26 as \n",
      "   select \n",
      "        id,\n",
      "        domain as up,\n",
      "        count(*) as cnt,\n",
      "        max(timestamp) - min(timestamp) as duration,\n",
      "        stddev(timestamp) as timestd\n",
      "    from user_kposminin.urls_w_levels_train26\n",
      "    where length(domain) > 0\n",
      "    group by id, domain\n",
      "    union all\n",
      "    select\n",
      "        id,\n",
      "        concat(domain,'[0]',lev0) as up,\n",
      "        count(*) as cnt,\n",
      "        max(timestamp) - min(timestamp) as duration,\n",
      "        stddev(timestamp) as timestd\n",
      "    from user_kposminin.urls_w_levels_train26\n",
      "    where length(domain) > 0 and length(lev0) > 0\n",
      "    group by id, concat(domain,'[0]',lev0)\n",
      "    union all\n",
      "    select \n",
      "        id,\n",
      "        concat(domain,'[1]',lev1) as up,\n",
      "        count(*) as cnt,\n",
      "        max(timestamp) - min(timestamp) as duration,\n",
      "        stddev(timestamp) as timestd\n",
      "    from user_kposminin.urls_w_levels_train26\n",
      "    where length(domain) > 0 and length(lev1) > 0\n",
      "    group by id, concat(domain,'[1]',lev1)\n",
      "    union all\n",
      "    select\n",
      "        id,\n",
      "        concat(domain,'[2]',lev2) as up,\n",
      "        count(*) as cnt,\n",
      "        max(timestamp) - min(timestamp) as duration,\n",
      "        stddev(timestamp) as timestd\n",
      "    from user_kposminin.urls_w_levels_train26\n",
      "    where length(domain) > 0 and length(lev2) > 0\n",
      "    group by id, concat(domain,'[2]',lev2)\n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.pos_id_train26;\n",
      "create table user_kposminin.pos_id_train26 as\n",
      "select distinct\n",
      "    ymd, id\n",
      "from user_kposminin.access_log_sample5\n",
      "where ymd between \"2016-07-07\" and \"2016-07-07\"\n",
      "    and url like \"raiffeisen.ru/retail/cards/credit%\"         \n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.up_train26;\n",
      "create table user_kposminin.up_train26 as\n",
      "select        \n",
      "    up,\n",
      "    total,\n",
      "    positive,\n",
      "    log((positive + 0.1)/(total - positive + 0.1)) as score\n",
      "from (\n",
      "        select a.up, count(distinct a.id) as total, count(distinct b.id) as positive\n",
      "        from id_up_train26 a \n",
      "        left join pos_id_train26 b on a.id = b.id\n",
      "        group by up\n",
      "     ) c\n",
      "where\n",
      "   total > 30000\n",
      "   OR (positive > 1 and total > 20)\n",
      ";    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "a = '''\n",
    "    drop table if exists user_kposminin.urls_w_levels_train{calc_id};\n",
    "    create table user_kposminin.urls_w_levels_train{calc_id} as\n",
    "    select\n",
    "        a.id\n",
    "        ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
    "        ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
    "        ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
    "        ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
    "        ,a.timestamp\n",
    "    from (\n",
    "               select ymd, id, url, timestamp\n",
    "               from {source_table_name}\n",
    "               where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "             ) a\n",
    "    where\n",
    "        url not like \"{exclude_url}%\"\n",
    "    ;\n",
    "    \n",
    "    drop table if exists user_kposminin.id_up_train{calc_id};\n",
    "    \n",
    "    create table user_kposminin.id_up_train{calc_id} as \n",
    "       select \n",
    "            id,\n",
    "            domain as up,\n",
    "            count(*) as cnt,\n",
    "            max(timestamp) - min(timestamp) as duration,\n",
    "            stddev(timestamp) as timestd\n",
    "        from user_kposminin.urls_w_levels_train{calc_id}\n",
    "        where length(domain) > 0\n",
    "        group by id, domain\n",
    "        union all\n",
    "        select\n",
    "            id,\n",
    "            concat(domain,'[0]',lev0) as up,\n",
    "            count(*) as cnt,\n",
    "            max(timestamp) - min(timestamp) as duration,\n",
    "            stddev(timestamp) as timestd\n",
    "        from user_kposminin.urls_w_levels_train{calc_id}\n",
    "        where length(domain) > 0 and length(lev0) > 0\n",
    "        group by id, concat(domain,'[0]',lev0)\n",
    "        union all\n",
    "        select \n",
    "            id,\n",
    "            concat(domain,'[1]',lev1) as up,\n",
    "            count(*) as cnt,\n",
    "            max(timestamp) - min(timestamp) as duration,\n",
    "            stddev(timestamp) as timestd\n",
    "        from user_kposminin.urls_w_levels_train{calc_id}\n",
    "        where length(domain) > 0 and length(lev1) > 0\n",
    "        group by id, concat(domain,'[1]',lev1)\n",
    "        union all\n",
    "        select\n",
    "            id,\n",
    "            concat(domain,'[2]',lev2) as up,\n",
    "            count(*) as cnt,\n",
    "            max(timestamp) - min(timestamp) as duration,\n",
    "            stddev(timestamp) as timestd\n",
    "        from user_kposminin.urls_w_levels_train{calc_id}\n",
    "        where length(domain) > 0 and length(lev2) > 0\n",
    "        group by id, concat(domain,'[2]',lev2)\n",
    "    ;\n",
    "    \n",
    "    drop table if exists user_kposminin.pos_id_train{calc_id};\n",
    "    create table user_kposminin.pos_id_train{calc_id} as\n",
    "    select distinct\n",
    "        ymd, id\n",
    "    from {source_table_name}\n",
    "    where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "        and url like \"{target_url}%\"         \n",
    "    ;\n",
    "    \n",
    "    drop table if exists user_kposminin.up_train{calc_id};\n",
    "    create table user_kposminin.up_train{calc_id} as\n",
    "    select        \n",
    "        up,\n",
    "        total,\n",
    "        positive,\n",
    "        log((positive + 0.1)/(total - positive + 0.1)) as score\n",
    "    from (\n",
    "            select a.up, count(distinct a.id) as total, count(distinct b.id) as positive\n",
    "            from id_up_train{calc_id} a \n",
    "            left join pos_id_train{calc_id} b on a.id = b.id\n",
    "            group by up\n",
    "         ) c\n",
    "    where\n",
    "       total > 30000\n",
    "       OR (positive > 1 and total > 20)\n",
    "    ;    \n",
    "    \n",
    "    \n",
    "    \n",
    "    SELECT\n",
    "        ymd,\n",
    "        uid,\n",
    "        (uid IN (SELECT uid from #pos_id_table)) as label,\n",
    "        toInt32(max(score)) as smax,\n",
    "        toInt32(ssum/has_scores) as savg,\n",
    "        toInt32(sum(score * has_score)) as ssum,\n",
    "        toInt32(median(score)) as smedian,\n",
    "        toUInt16(stddevSamp(score)) as sstd,\n",
    "        toUInt32(sum(cnt)) as cntrepeat,\n",
    "        toUInt32(count()) as cntuniq,\n",
    "        toUInt64(sum(duration)) as duration,\n",
    "        toUInt8(sum(has_score)) as has_scores,\n",
    "        toUInt8(max(mobile)) as mobile,\n",
    "        toUInt8(max(emailru)) as emailru,\n",
    "        toUInt8(max(vkru)) as vkru,\n",
    "        toUInt8(max(okru)) as okru,\n",
    "        toUInt8(max(social_other)) as social_other,\n",
    "        length(groupArray(score) as sl) >= 1 ? sl[1] : toInt32(#mv) as s1,\n",
    "        length(sl) >= 2 ? sl[2] : toInt32(#mv) as s2,\n",
    "        length(sl) >= 3 ? sl[3] : toInt32(#mv) as s3,\n",
    "        length(sl) >= 4 ? sl[4] : toInt32(#mv) as s4,\n",
    "        length(sl) >= 5 ? sl[5] : toInt32(#mv) as s5,\n",
    "        length(sl) >= 6 ? sl[6] : toInt32(#mv) as s6,\n",
    "        length(sl) >= 7 ? sl[7] : toInt32(#mv) as s7,\n",
    "        length(sl) >= 8 ? sl[8] : toInt32(#mv) as s8,\n",
    "        length(sl) >= 9 ? sl[9] : toInt32(#mv) as s9,\n",
    "        length(sl) >= 10 ? sl[10] : toInt32(#mv) as s10,\n",
    "        length(sl) >= 1 ? sl[-1] : toInt32(#mv) as sm1,\n",
    "        length(sl) >= 2 ? sl[-2] : toInt32(#mv) as sm2,\n",
    "        length(sl) >= 3 ? sl[-3] : toInt32(#mv) as sm3,\n",
    "        length(sl) >= 4 ? sl[-4] : toInt32(#mv) as sm4,\n",
    "        length(sl) >= 5 ? sl[-5] : toInt32(#mv) as sm5        \n",
    "    FROM        \n",
    "        (SELECT\n",
    "            ymd,\n",
    "            uid,\n",
    "            (score=0) and (total = 0) ? toInt32(#mv) : score as score,\n",
    "            cnt,\n",
    "            (total > 0) ? 1 : 0 as has_score,\n",
    "            duration,\n",
    "            (up like 'm.%') as mobile,\n",
    "            (up like '%e.mail.ru%') as emailru,\n",
    "            match(up,'^vk\\\\.com|[^A-Za-z]vk\\\\.com|^vk.me|[^A-Za-z]vk\\\\.me|^vk\\\\.cc|[^A-Za-z]vk\\\\.cc|vkontakte\\\\.') as vkru,\n",
    "            match(up,'^ok\\\\.ru|[^A-Za-z]ok\\\\.ru|odnoklassniki\\\\.ru') as okru,\n",
    "            match(up,'^fb\\\\.com|[^A-Za-z]fb\\\\.com|instagram\\\\.com|twitter\\\\.com|my\\\\.mail\\\\.ru|livejournal\\\\.com|^lj\\\\.ru') as social_other\n",
    "        FROM\n",
    "            (select * from #id_up_table where uid > '#low' and uid <= '#high')\n",
    "        ANY LEFT JOIN (select up,score,total from #up_table) USING (up) \n",
    "        ORDER BY uid,score DESC\n",
    "        )\n",
    "    GROUP BY ymd,uid\n",
    "    \n",
    "\n",
    "    '''.format(\n",
    "    calc_id = 26, \n",
    "    train_start_date = train_start_date, \n",
    "    train_end_date = train_end_date, \n",
    "    test_date = test_date,\n",
    "    source_table_name = source_table_name,\n",
    "    target_url = target_url,\n",
    "    exclude_url = exclude_url\n",
    "    )\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.user_urlp_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_urlp_test{calc_id} as \n",
    "   select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        domain as urlp    \n",
    "    from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select\n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[0]',lev0) as urlp\n",
    "     from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[1]',lev1) as urlp\n",
    "    from user_kposminin.urls_w_levels_test{calc_id}\n",
    "    union all\n",
    "    select \n",
    "        concat(cookie, 1) as cookie,\n",
    "        concat(domain,'[2]',lev2) as urlp\n",
    "    from user_kposminin.urls_w_levels_test{calc_id}   \n",
    ";\n",
    "\n",
    "drop table if exists user_kposminin.user_train{calc_id};\n",
    "\n",
    "create table user_kposminin.user_train{calc_id} as\n",
    "select\n",
    "   concat(id, 0)  as cookie,\n",
    "   max(case when {target_expression} then 1 else 0 end) as label\n",
    "from \n",
    "   {source_table_name} al\n",
    "   where ymd between \"{train_start_date}\" and \"{train_end_date}\"\n",
    "group by id;\n",
    "\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.user_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_test{calc_id} as\n",
    "select\n",
    "   concat(id, 1) as cookie,\n",
    "   max(case when {target_expression} then 1 else 0 end) as label\n",
    "from \n",
    "   {source_table_name} al\n",
    "   where ymd = \"{test_date}\"\n",
    "group by id;\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.urlp_score_train{calc_id};\n",
    "\n",
    "create table user_kposminin.urlp_score_train{calc_id} as\n",
    "select\n",
    "    urlp,\n",
    "    log((positives + 0.5) / (total - positives + 0.5)) as score\n",
    "from\n",
    "    (select\n",
    "        urlp,\n",
    "        sum(label) as positives,\n",
    "        count(cookie) as total\n",
    "    from\n",
    "        (select distinct\n",
    "            a.urlp,\n",
    "            a.cookie,\n",
    "            b.label\n",
    "        from \n",
    "            (select * \n",
    "             from user_kposminin.user_urlp_train{calc_id}\n",
    "             where {exclude_expression}\n",
    "            ) a\n",
    "        left join user_kposminin.user_train{calc_id} b \n",
    "        on a.cookie = b.cookie\n",
    "        ) c\n",
    "    group by urlp\n",
    "    ) d\n",
    "where\n",
    "    total > 100\n",
    "    or positives > 5\n",
    ";\n",
    "\n",
    "\n",
    "drop table if exists user_kposminin.user_score_test{calc_id};\n",
    "\n",
    "create table user_kposminin.user_score_test{calc_id} as\n",
    "select\n",
    "    cs.cookie,\n",
    "    cs.score,\n",
    "    i.label\n",
    "from\n",
    "    (select \n",
    "        u.cookie,\n",
    "        max(s.score) as score\n",
    "    from \n",
    "        user_kposminin.user_urlp_test{calc_id} u\n",
    "    join user_kposminin.urlp_score_train{calc_id} s\n",
    "    on u.urlp = s.urlp\n",
    "    group by u.cookie) cs\n",
    "join user_kposminin.user_test{calc_id} i\n",
    "on i.cookie = cs.cookie;\n",
    "\n",
    "drop table if exists user_kposminin.user_score_test_exper{calc_id};\n",
    "\n",
    "create table user_kposminin.user_score_test_exper{calc_id} as\n",
    "select\n",
    "    cs.cookie,\n",
    "    cs.max_score,\n",
    "    cs.sum_score,\n",
    "    cs.avg_score,\n",
    "    cs.scores_list,\n",
    "    i.label    \n",
    "from\n",
    "    (select \n",
    "        u.cookie,\n",
    "        max(s.score) as max_score,\n",
    "        sum(s.score) as sum_score,\n",
    "        avg(s.score) as avg_score,\n",
    "        sort_array(collect_list(s.score)) as scores_list\n",
    "    from \n",
    "        user_kposminin.user_urlp_test{calc_id} u\n",
    "    join (\n",
    "      select * from user_kposminin.urlp_score_train{calc_id}\n",
    "      order by score desc\n",
    "      limit 20\n",
    "    ) s\n",
    "    on u.urlp = s.urlp\n",
    "    group by u.cookie) cs\n",
    "join user_kposminin.user_test16 i\n",
    "on i.cookie = cs.cookie'''.format(\n",
    "    calc_id = calc_id, \n",
    "    train_start_date = train_start_date, \n",
    "    train_end_date = train_end_date, \n",
    "    test_date = test_date,\n",
    "    source_table_name = source_table_name,\n",
    "    target_expression = target_expression,\n",
    "    exclude_expression = exclude_expression\n",
    ")\n",
    "\n",
    "train_labeledpoint_query = '''\n",
    "select\n",
    "    u.label,\n",
    "    cu.url_list\n",
    "from\n",
    "   (select\n",
    "      cookie,\n",
    "      collect_list(urlp) as url_list\n",
    "   from \n",
    "      user_kposminin.user_urlp_train{calc_id}\n",
    "   where {exclude_expression}\n",
    "   group by cookie) cu\n",
    "join user_kposminin.user_train{calc_id} u\n",
    "on cu.cookie = u.cookie\n",
    "'''.format(calc_id = calc_id, exclude_expression = exclude_expression)\n",
    "\n",
    "test_labeledpoint_query = '''\n",
    "select\n",
    "    u.label,\n",
    "    cu.url_list\n",
    "from\n",
    "   (select\n",
    "      cookie,\n",
    "      collect_list(urlp) as url_list\n",
    "   from \n",
    "      user_kposminin.user_urlp_test{calc_id}\n",
    "   where {exclude_expression}\n",
    "   group by cookie) cu\n",
    "join user_kposminin.user_test{calc_id} u\n",
    "on cu.cookie = u.cookie\n",
    "'''.format(calc_id = calc_id, exclude_expression = exclude_expression)\n",
    "\n",
    "current_approach_results_query = '''\n",
    "select\n",
    "    *\n",
    "from \n",
    "    user_kposminin.user_score_test_exper{calc_id}\n",
    "'''.format(calc_id = calc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drop table if exists user_kposminin.urls_w_levels_train19;\n",
      "\n",
      "create table user_kposminin.urls_w_levels_train19 as\n",
      "select\n",
      "    a.id as cookie\n",
      "    ,concat(a.id, \"-\", a.ymd) as object_id\n",
      "    ,a.ymd\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
      "    ,a.url\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)', 1) as ref_domain\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) ref_lev0\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) ref_lev1\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) ref_lev2\n",
      "    ,a.referrer\n",
      "    ,a.timestamp\n",
      "    from \n",
      "    (\n",
      "       select b.*\n",
      "        from\n",
      "           (select\n",
      "              al.*,\n",
      "              count(*) over (partition by url) as url_count\n",
      "           from user_kposminin.access_log_sample5 al\n",
      "              where ymd between \"2016-07-07\" and \"2016-07-07\"\n",
      "           ) b\n",
      "       where b.url_count > 50\n",
      "    ) a\n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.urls_w_levels_test19 ;\n",
      "\n",
      "create table user_kposminin.urls_w_levels_test19 as\n",
      "select\n",
      "    a.id as cookie\n",
      "    ,concat(a.id, \"-\", a.ymd) as object_id\n",
      "    ,a.ymd\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)', 1) as domain\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) lev0\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) lev1\n",
      "    ,regexp_extract(regexp_extract(a.url, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) lev2\n",
      "    ,a.url\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)', 1) as ref_domain\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?', 2) ref_lev0\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?', 3) ref_lev1\n",
      "    ,regexp_extract(regexp_extract(a.referrer, \"([^\\?]*)\", 0), '^([^/]*)/?([^/]*)?/?([^/]*)?/?([^/]*)?', 4) ref_lev2\n",
      "    ,a.referrer\n",
      "    ,a.timestamp\n",
      "    from \n",
      "    (\n",
      "       select b.*\n",
      "        from\n",
      "           (select\n",
      "              al.*,\n",
      "              count(*) over (partition by url) as url_count\n",
      "           from user_kposminin.access_log_sample5 al\n",
      "              where ymd = \"2016-07-14\"\n",
      "           ) b\n",
      "       where b.url_count > 50\n",
      "    ) a\n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.user_urlp_train19;\n",
      "\n",
      "create table user_kposminin.user_urlp_train19 as \n",
      "   select \n",
      "        concat(cookie, 0) as cookie,\n",
      "        domain as urlp    \n",
      "    from user_kposminin.urls_w_levels_train19\n",
      "    union all\n",
      "    select\n",
      "        concat(cookie, 0) as cookie,\n",
      "        concat(domain,'[0]',lev0) as urlp\n",
      "     from user_kposminin.urls_w_levels_train19\n",
      "    union all\n",
      "    select \n",
      "        concat(cookie, 0) as cookie,\n",
      "        concat(domain,'[1]',lev1) as urlp\n",
      "    from user_kposminin.urls_w_levels_train19\n",
      "    union all\n",
      "    select \n",
      "        concat(cookie, 0) as cookie,\n",
      "        concat(domain,'[2]',lev2) as urlp\n",
      "    from user_kposminin.urls_w_levels_train19\n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.user_urlp_test19;\n",
      "\n",
      "create table user_kposminin.user_urlp_test19 as \n",
      "   select \n",
      "        concat(cookie, 1) as cookie,\n",
      "        domain as urlp    \n",
      "    from user_kposminin.urls_w_levels_test19\n",
      "    union all\n",
      "    select\n",
      "        concat(cookie, 1) as cookie,\n",
      "        concat(domain,'[0]',lev0) as urlp\n",
      "     from user_kposminin.urls_w_levels_test19\n",
      "    union all\n",
      "    select \n",
      "        concat(cookie, 1) as cookie,\n",
      "        concat(domain,'[1]',lev1) as urlp\n",
      "    from user_kposminin.urls_w_levels_test19\n",
      "    union all\n",
      "    select \n",
      "        concat(cookie, 1) as cookie,\n",
      "        concat(domain,'[2]',lev2) as urlp\n",
      "    from user_kposminin.urls_w_levels_test19   \n",
      ";\n",
      "\n",
      "drop table if exists user_kposminin.user_train19;\n",
      "\n",
      "create table user_kposminin.user_train19 as\n",
      "select\n",
      "   concat(id, 0)  as cookie,\n",
      "   max(case when ( url like \"%raiffeisen.ru/retail/cards/credit%\" ) then 1 else 0 end) as label\n",
      "from \n",
      "   user_kposminin.access_log_sample5 al\n",
      "   where ymd between \"2016-07-07\" and \"2016-07-07\"\n",
      "group by id;\n",
      "\n",
      "\n",
      "\n",
      "drop table if exists user_kposminin.user_test19;\n",
      "\n",
      "create table user_kposminin.user_test19 as\n",
      "select\n",
      "   concat(id, 1) as cookie,\n",
      "   max(case when ( url like \"%raiffeisen.ru/retail/cards/credit%\" ) then 1 else 0 end) as label\n",
      "from \n",
      "   user_kposminin.access_log_sample5 al\n",
      "   where ymd = \"2016-07-14\"\n",
      "group by id;\n",
      "\n",
      "\n",
      "drop table if exists user_kposminin.urlp_score_train19;\n",
      "\n",
      "create table user_kposminin.urlp_score_train19 as\n",
      "select\n",
      "    urlp,\n",
      "    log((positives + 0.5) / (total - positives + 0.5)) as score\n",
      "from\n",
      "    (select\n",
      "        urlp,\n",
      "        sum(label) as positives,\n",
      "        count(cookie) as total\n",
      "    from\n",
      "        (select distinct\n",
      "            a.urlp,\n",
      "            a.cookie,\n",
      "            b.label\n",
      "        from \n",
      "            (select * \n",
      "             from user_kposminin.user_urlp_train19\n",
      "             where not ( urlp like \"%raiffeisen.ru/retail/cards/credit%\" or urlp like \"%raiffeisen.ru%\" )\n",
      "            ) a\n",
      "        left join user_kposminin.user_train19 b \n",
      "        on a.cookie = b.cookie\n",
      "        ) c\n",
      "    group by urlp\n",
      "    ) d\n",
      "where\n",
      "    total > 100\n",
      "    or positives > 5\n",
      ";\n",
      "\n",
      "\n",
      "drop table if exists user_kposminin.user_score_test19;\n",
      "\n",
      "create table user_kposminin.user_score_test19 as\n",
      "select\n",
      "    cs.cookie,\n",
      "    cs.score,\n",
      "    i.label\n",
      "from\n",
      "    (select \n",
      "        u.cookie,\n",
      "        max(s.score) as score\n",
      "    from \n",
      "        user_kposminin.user_urlp_test19 u\n",
      "    join user_kposminin.urlp_score_train19 s\n",
      "    on u.urlp = s.urlp\n",
      "    group by u.cookie) cs\n",
      "join user_kposminin.user_test19 i\n",
      "on i.cookie = cs.cookie;\n",
      "\n",
      "drop table if exists user_kposminin.user_score_test_exper19;\n",
      "\n",
      "create table user_kposminin.user_score_test_exper19 as\n",
      "select\n",
      "    cs.cookie,\n",
      "    cs.max_score,\n",
      "    cs.sum_score,\n",
      "    cs.avg_score,\n",
      "    cs.scores_list,\n",
      "    i.label    \n",
      "from\n",
      "    (select \n",
      "        u.cookie,\n",
      "        max(s.score) as max_score,\n",
      "        sum(s.score) as sum_score,\n",
      "        avg(s.score) as avg_score,\n",
      "        sort_array(collect_list(s.score)) as scores_list\n",
      "    from \n",
      "        user_kposminin.user_urlp_test19 u\n",
      "    join (\n",
      "      select * from user_kposminin.urlp_score_train19\n",
      "      order by score desc\n",
      "      limit 20\n",
      "    ) s\n",
      "    on u.urlp = s.urlp\n",
      "    group by u.cookie) cs\n",
      "join user_kposminin.user_test16 i\n",
      "on i.cookie = cs.cookie\n"
     ]
    }
   ],
   "source": [
    "print(create_tables_in_hive_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make calculations and create tables in Hive\n",
    "\n",
    "#hc.sql(update_calcs_table_query)\n",
    "#for q in create_tables_in_hive_query.split(';'):\n",
    "#    print(create_tables_in_hive_query.split(';').index(q))\n",
    "#    hc.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load train and test data to Spark\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "\n",
    "tf = HashingTF(numFeatures = 10 ** 6)\n",
    "\n",
    "#transform urls (as Bag of Words) into features and form features with labels\n",
    "train_data = hc.sql(train_labeledpoint_query).rdd.sample(withReplacement = False, fraction = 0.4, seed = 1) \\\n",
    "    .map(lambda r: LabeledPoint(r.label,tf.transform(r.url_list)))\n",
    "train_data.cache()\n",
    "\n",
    "test_data = hc.sql(test_labeledpoint_query).rdd.map(lambda r: LabeledPoint(r.label,tf.transform(r.url_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o81.trainNaiveBayesModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.3 failed 4 times, most recent failure: Lost task 0.3 in stage 3.3 (TID 4264, ds-hadoop-wk14p.tcsbank.ru): java.io.FileNotFoundException: /disk4/yarn/nm/usercache/k.p.osminin/appcache/application_1472200270641_0404/blockmgr-26f4f80e-5d9a-4c34-b3cb-ebeef7756931/1e/temp_shuffle_c3a85ab4-4966-40c9-8cd7-1829f3c46652 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.mllib.classification.NaiveBayes.run(NaiveBayes.scala:401)\n\tat org.apache.spark.mllib.classification.NaiveBayes$.train(NaiveBayes.scala:483)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainNaiveBayesModel(PythonMLLibAPI.scala:305)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: /disk4/yarn/nm/usercache/k.p.osminin/appcache/application_1472200270641_0404/blockmgr-26f4f80e-5d9a-4c34-b3cb-ebeef7756931/1e/temp_shuffle_c3a85ab4-4966-40c9-8cd7-1829f3c46652 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-848ac08cea65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train NaiveBayes model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodelNB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba_NB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, lambda_)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`data` should be an RDD of LabeledPoint\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trainNaiveBayesModel\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNaiveBayesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o81.trainNaiveBayesModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.3 failed 4 times, most recent failure: Lost task 0.3 in stage 3.3 (TID 4264, ds-hadoop-wk14p.tcsbank.ru): java.io.FileNotFoundException: /disk4/yarn/nm/usercache/k.p.osminin/appcache/application_1472200270641_0404/blockmgr-26f4f80e-5d9a-4c34-b3cb-ebeef7756931/1e/temp_shuffle_c3a85ab4-4966-40c9-8cd7-1829f3c46652 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.mllib.classification.NaiveBayes.run(NaiveBayes.scala:401)\n\tat org.apache.spark.mllib.classification.NaiveBayes$.train(NaiveBayes.scala:483)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainNaiveBayesModel(PythonMLLibAPI.scala:305)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: /disk4/yarn/nm/usercache/k.p.osminin/appcache/application_1472200270641_0404/blockmgr-26f4f80e-5d9a-4c34-b3cb-ebeef7756931/1e/temp_shuffle_c3a85ab4-4966-40c9-8cd7-1829f3c46652 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Train NaiveBayes model\n",
    "\n",
    "modelNB = NaiveBayes.train(train_data)\n",
    "\n",
    "def predict_proba_NB(f,model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability. f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Function selects winning class with it probability.\n",
    "    Output: tuple with model selected class number as first element (type int) and it probability as second (type float).\n",
    "    '''\n",
    "    logp = [[i,f.dot(model.theta[i]) + model.pi[i]] for i in range(len(model.theta))] # classes with log probabilities\n",
    "    wi = sorted(logp, key = lambda e:  - e[1])[0][0] #winning index\n",
    "    prob = 1./sum([np.exp(e[1] - logp[wi][1]) for e in logp]) #winning class probability\n",
    "    return wi, prob\n",
    "\n",
    "def predict_proba_NB_2(f, model):\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Naive Bayes model prediction with probability for 2-class classification.\n",
    "    f is features [Sparse] vector. model is mllib.NaiveBayesModel.\n",
    "    Output: probability of class 1 (type float).\n",
    "    '''\n",
    "    if len(model.theta) != 2:\n",
    "        print('Model is NOT a 2-class classifier')\n",
    "        return None\n",
    "    logp = [f.dot(model.theta[i]) + model.pi[i] for i in range(2)]    \n",
    "    return 1./(1. + np.exp(logp[0] - logp[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LogisticRegression model\n",
    "\n",
    "modelLR = LogisticRegressionWithSGD.train(train_data)\n",
    "modelLR.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Current approach (results only). All calculations in Hive\n",
    "\n",
    "ca_res = hc.sql(current_approach_results_query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing result\n",
    "\n",
    "df_test = test_data.map( lambda lp: pyspark.sql.Row(\n",
    "        Label = lp.label,\n",
    "        NaiveBayes = float(predict_proba_NB_2(lp.features, modelNB)),\n",
    "        LogisticRegression = float(modelLR.predict(lp.features))\n",
    "    )).toDF().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build AUCROC metric and print results\n",
    "\n",
    "import sklearn\n",
    "AUCROC = []\n",
    "for c in df_test.columns:\n",
    "    if c!= 'Label':\n",
    "        AUCROC.append([c,sklearn.metrics.roc_auc_score(df_test['Label'],df_test[c])])\n",
    "for c in [c for c in ca_res.columns if not c in [u'cookie', u'scores_list', u'label']]:\n",
    "    AUCROC.append(['CurApp_' + c, sklearn.metrics.roc_auc_score(ca_res['label'], ca_res[c])])\n",
    "for n in [2,3,5,7,10,15,20]:\n",
    "    AUCROC.append(['CurApp_Top' + str(n), sklearn.metrics.roc_auc_score(\n",
    "        ca_res['label'], [sum(r[-n:])/max(len(r[-n:]),1) for r in ca_res[u'scores_list'].values]\n",
    "    )])\n",
    "        \n",
    "print('\\nMethods AUCROC performance on test sample ({0:.0f} samples with {1:.0f} positives):\\n\\n'.format(\n",
    "        df_test.size,df_test['Label'].sum()) +'\\n'.join(['{0:<30}{1:.5f}'.format(k,v) for (k,v) in AUCROC])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Time of work {0}'.format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca =hc.sql(current_approach_results_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cookie=u'0004C0960ECE41CBFED41', max_score=-4.110873864173311, sum_score=-49.33048637007975, avg_score=-4.110873864173312, scores_list=[-4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311, -4.110873864173311], label=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
