{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Холодные звонки\n",
    "\n",
    "Эффективность замены доменов на кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 64).set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_query = '''\n",
    "select \n",
    "    ymd, \n",
    "    ccall_approve_label as label, \n",
    "    score_ccall_total, \n",
    "    score_ccall_approve_total, \n",
    "    score_ccall_not_approve_total, \n",
    "    score_ccall_approve_ccall_not_approve, \n",
    "    score_ccall_approve_total_weekly\n",
    "from user_kposminin.ccalls_scores_2\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_clusters_query = '''\n",
    "select \n",
    "    ymd, \n",
    "    full_app as label, \n",
    "    features\n",
    "from user_kposminin.ccalls_visits_clusters m\n",
    "left join user_kposminin.cold_calls_havent_started neg on neg.phone_num = m.phone_num\n",
    "WHERE neg.phone_num is Null\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_superclusters_query = '''\n",
    "select   \n",
    "  phone_num,\n",
    "  -- max(label) as label,\n",
    "  max(full_app) as label,\n",
    "  collect_set(supercluster) as features,\n",
    "  ymd\n",
    "from \n",
    "  (select\n",
    "    v.ymd,\n",
    "    v.phone_num,\n",
    "    cast(v.label as tinyint) as label,\n",
    "    cast(v.full_app as tinyint) as full_app,   \n",
    "  --  cast(v.visit_lag as tinyint) as visit_lag,\n",
    "    cast(c.supercluster as int) as supercluster\n",
    "  from user_kposminin.ccalls_visits_1 v\n",
    "  inner join user_kposminin.domain_clusters c on c.domain = split(v.urlfr,'#')[0]\n",
    "  left semi join user_kposminin.cold_calls_matched_5 m on m.phone_num = v.phone_num and m.ymd = v.ymd and m.havent_started = 0\n",
    "  ) a\n",
    "group by phone_num, ymd\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_superclusters_with_count_query = '''\n",
    "select \n",
    "  ymd,\n",
    "  phone_num,\n",
    "  max(label) as label,\n",
    "  collect_list(concat(supercluster,\":\",cnt)) as features\n",
    "from\n",
    "  (select   \n",
    "    phone_num,\n",
    "    -- max(label) as label,\n",
    "    max(full_app) as label,\n",
    "    supercluster,\n",
    "    stddev(visit_lag) as cnt,\n",
    "    ymd\n",
    "  from \n",
    "    (select\n",
    "      v.ymd,\n",
    "      v.phone_num,\n",
    "      cast(v.label as tinyint) as label,\n",
    "      cast(v.full_app as tinyint) as full_app,  \n",
    "      cast(c.supercluster as int) as supercluster,\n",
    "      1 as cnt,\n",
    "      v.visit_lag\n",
    "    from user_kposminin.ccalls_visits_1 v\n",
    "    inner join user_kposminin.domain_clusters c on concat(c.domain,'#') = v.urlfr\n",
    "    left semi join user_kposminin.cold_calls_matched_5 m on m.phone_num = v.phone_num and m.ymd = v.ymd and m.havent_started = 0\n",
    "    ) a\n",
    "  group by phone_num, ymd, supercluster\n",
    "--  order by ymd,phone_num, supercluster\n",
    "  ) b\n",
    "group by ymd, phone_num\n",
    "'''\n",
    "\n",
    "many_features_query = '''\n",
    "select \n",
    "  ymd,\n",
    "  phone_num,\n",
    "  max(label) as label,\n",
    "  sum(cnt_uniq) as cnt_uniq,\n",
    "  sum(cnt_w_repeat) as cnt_w_repeat,\n",
    "  count(supercluster) as cnt_sclust,\n",
    "  SUBSTR(phone_num,2,3) as phone_3digits,\n",
    "  max(\n",
    "       named_struct(\n",
    "        'work_hours_cnt_w_repeats', work_hours_cnt_w_repeats,\n",
    "        'supercluster', supercluster\n",
    "       )\n",
    "    ).supercluster as work_hours_sclust,\n",
    "  max(\n",
    "       named_struct(\n",
    "        'night_hours_cnt_w_repeats', night_hours_cnt_w_repeats,\n",
    "        'supercluster', supercluster\n",
    "       )\n",
    "    ).supercluster as night_hours_sclust,\n",
    "  collect_list(\n",
    "    concat(\n",
    "      supercluster,\",\"\n",
    "     ,std1,\",\"\n",
    "     ,std2,\",\"\n",
    "     ,cnt_w_repeat,\",\"\n",
    "     ,cnt_uniq,\",\"\n",
    "     ,duration,\",\"\n",
    "     ,avg_hour,\",\"\n",
    "     ,work_hours_cnt_w_repeats\n",
    "    )\n",
    "  ) as sclust_features\n",
    "from\n",
    "  (select   \n",
    "    phone_num,\n",
    "    -- max(label) as label,\n",
    "    max(full_app) as label,\n",
    "    supercluster,\n",
    "    stddev(visit_lag) as std1,\n",
    "    stddev(-24 * visit_lag + avg_hour) as std2,\n",
    "    sum(cnt) as cnt_w_repeat,\n",
    "    sum(if(avg_hour >  9 and avg_hour <  20,cnt,0)) as  work_hours_cnt_w_repeats,\n",
    "    sum(if(avg_hour <= 9 or  avg_hour >= 20,cnt,0)) as night_hours_cnt_w_repeats,\n",
    "    sum(1) as cnt_uniq,\n",
    "    sum(duration) as duration,\n",
    "    cast(percentile(avg_hour,0.5) as int) as avg_hour,\n",
    "    ymd\n",
    "  from \n",
    "    (select\n",
    "      v.ymd,\n",
    "      v.phone_num,\n",
    "      cast(v.label as tinyint) as label,\n",
    "      cast(v.full_app as tinyint) as full_app,  \n",
    "      cast(c.supercluster as int) as supercluster,\n",
    "      v.cnt,\n",
    "      v.visit_lag,\n",
    "      v.duration,\n",
    "      v.avg_hour\n",
    "    from user_kposminin.ccalls_visits_1 v\n",
    "    inner join user_kposminin.domain_clusters c on concat(c.domain,'#') = v.urlfr\n",
    "    left semi join user_kposminin.cold_calls_matched_5 m on m.phone_num = v.phone_num and m.ymd = v.ymd and m.havent_started = 0\n",
    "    ) a\n",
    "  group by phone_num, ymd, supercluster\n",
    "--  order by ymd,phone_num, supercluster\n",
    "  ) b\n",
    "group by ymd, phone_num\n",
    "\n",
    "'''\n",
    "#from scipy.sparse import coo_matrix, vstack,hstack\n",
    "\n",
    "#tf = HashingTF(numFeatures = 10 ** 5)\n",
    "\n",
    "all_data = hc.sql(many_features_query).rdd\n",
    "train_data = all_data \\\n",
    "    .filter(lambda row: row['ymd'] < '2016-05-16') \\\n",
    "    .collect()\n",
    "test_data  = all_data \\\n",
    "    .filter(lambda row: row['ymd'] >= '2016-05-16') \\\n",
    "    .collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform into sparse matrix\n",
    "from scipy.sparse import csr_matrix, vstack,hstack\n",
    "phone_codes = dict((e.split('\\t')[0],int(e.split('\\t')[1])) for e in open('phone_codes.txt','r').read().split('\\n'))\n",
    "\n",
    "def get_sparse_matrix(data):\n",
    "    labels, features = [0], csr_matrix((1,7006))\n",
    "    i = 0\n",
    "    batch_size = 3000\n",
    "    for i in range(len(data)/batch_size + 1):\n",
    "        batch = csr_matrix((1,7006))\n",
    "        for r in data[batch_size*i:batch_size*(i+1)]:\n",
    "            row = []\n",
    "            sp3s = 1e-10\n",
    "            sp4s = 1e-10\n",
    "            sp5s = 1e-10\n",
    "            sp7s = 1e-10\n",
    "            for s in r[9]:\n",
    "                sp = s.split(',')\n",
    "                row.append([\n",
    "                        int(sp[0]),\n",
    "                        0 if sp[1] == 'NaN' else float(sp[1]),\n",
    "                        0 if sp[2] == 'NaN' else float(sp[2]),\n",
    "                        float(sp[3]),\n",
    "                        float(sp[4]),\n",
    "                        float(sp[5]),\n",
    "                        int(sp[6]),\n",
    "                        float(sp[7]),\n",
    "                 ])\n",
    "                sp3s += row[-1][3]\n",
    "                sp4s += row[-1][4]\n",
    "                sp5s += row[-1][5]\n",
    "                sp7s += row[-1][7]\n",
    "            row = [[e[0],e[1],e[2],e[3]/sp3s,e[4]/sp4s,e[5]/sp5s,e[6],e[7]/sp7s] for e in row]\n",
    "            vals = [int(e) for e in r[3:9]]\n",
    "            cidx = range(6)\n",
    "            ridx = [0] * 6\n",
    "            for s in row:\n",
    "                sh = len(s) - 1\n",
    "                vals += s[1:]\n",
    "                cidx += [sh * s[0] + j + 6 for j in range(sh)]\n",
    "                ridx += [0] * (len(s)-1)\n",
    "\n",
    "        #idx  = [int(e.split(':')[0]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "        #vals = [int(e.split(':')[1]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "            labels.append(r.label)\n",
    "            new_row = csr_matrix((vals, (ridx,cidx)), shape=(1,7006), dtype = type(1.))\n",
    "            batch = vstack([batch,new_row])\n",
    "        features = vstack([features, batch[1:]])\n",
    "        print(str(i)+'/'+str(len(data)/batch_size + 1))\n",
    "    \n",
    "    return labels[1:],features[1:]\n",
    "\n",
    "\n",
    "\n",
    "def get_sparse_matrix_2(data):\n",
    "    labels, features = [0], csr_matrix((1,10100))\n",
    "    i = 0\n",
    "    batch_size = 3000\n",
    "    for i in range(len(data)/batch_size + 1):\n",
    "        k, vals, cidx, ridx = 0, [], [], []\n",
    "        for r in data[batch_size*i:batch_size*(i+1)]:\n",
    "            row = []\n",
    "            sp3s = 0\n",
    "            sp4s = 0\n",
    "            sp5s = 0\n",
    "            sp7s = 0\n",
    "            for s in r[9]:\n",
    "                sp = s.split(',')\n",
    "                row.append([\n",
    "                        int(sp[0]),\n",
    "                        0 if sp[1] == 'NaN' else float(sp[1]),\n",
    "                        0 if sp[2] == 'NaN' else float(sp[2]),\n",
    "                        float(sp[3]),\n",
    "                        float(sp[4]),\n",
    "                        float(sp[5]),\n",
    "                        int(sp[6]),\n",
    "                        float(sp[7]),\n",
    "                 ])\n",
    "                sp3s += row[-1][3]\n",
    "                sp4s += row[-1][4]\n",
    "                sp5s += row[-1][5]\n",
    "                sp7s += row[-1][7]\n",
    "            row = [[e[0],1,e[1],e[2],e[3]/sp3s if sp3s!=0 else 0,e[4]/sp4s if sp4s!=0 else 0,e[5]/sp5s if sp5s!=0 else 0,\n",
    "                    e[6],e[7]/sp7s if sp7s!=0 else 0] for e in row]\n",
    "            vals += [int(e) for e in r[3:6]]\n",
    "            cidx += range(3)\n",
    "            ridx += [k] * 3\n",
    "            for s in row:\n",
    "                sh = len(s) - 1\n",
    "                vals += s[1:]\n",
    "                cidx += [sh * s[0] + j + 4 for j in range(sh)]\n",
    "                ridx += [k] * sh\n",
    "            vals += [1, 1, 1]\n",
    "            cidx += [8004 + r[7], 9004 + r[8], 10004 + phone_codes[r[6]]]\n",
    "            ridx += [k, k, k]\n",
    "            labels.append(r.label)\n",
    "            k += 1\n",
    "        batch = csr_matrix((vals, (ridx,cidx)), shape=(k,10100), dtype = type(1.))\n",
    "        features = vstack([features, batch])\n",
    "        print(str(i)+'/'+str(len(data)/batch_size + 1))\n",
    "    \n",
    "    return labels[1:],features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/152\n",
      "1/152\n",
      "2/152\n",
      "3/152\n",
      "4/152\n",
      "5/152\n",
      "6/152\n",
      "7/152\n",
      "8/152\n",
      "9/152\n",
      "10/152\n",
      "11/152\n",
      "12/152\n",
      "13/152\n",
      "14/152\n",
      "15/152\n",
      "16/152\n",
      "17/152\n",
      "18/152\n",
      "19/152\n",
      "20/152\n",
      "21/152\n",
      "22/152\n",
      "23/152\n",
      "24/152\n",
      "25/152\n",
      "26/152\n",
      "27/152\n",
      "28/152\n",
      "29/152\n",
      "30/152\n",
      "31/152\n",
      "32/152\n",
      "33/152\n",
      "34/152\n",
      "35/152\n",
      "36/152\n",
      "37/152\n",
      "38/152\n",
      "39/152\n",
      "40/152\n",
      "41/152\n",
      "42/152\n",
      "43/152\n",
      "44/152\n",
      "45/152\n",
      "46/152\n",
      "47/152\n",
      "48/152\n",
      "49/152\n",
      "50/152\n",
      "51/152\n",
      "52/152\n",
      "53/152\n",
      "54/152\n",
      "55/152\n",
      "56/152\n",
      "57/152\n",
      "58/152\n",
      "59/152\n",
      "60/152\n",
      "61/152\n",
      "62/152\n",
      "63/152\n",
      "64/152\n",
      "65/152\n",
      "66/152\n",
      "67/152\n",
      "68/152\n",
      "69/152\n",
      "70/152\n",
      "71/152\n",
      "72/152\n",
      "73/152\n",
      "74/152\n",
      "75/152\n",
      "76/152\n",
      "77/152\n",
      "78/152\n",
      "79/152\n",
      "80/152\n",
      "81/152\n",
      "82/152\n",
      "83/152\n",
      "84/152\n",
      "85/152\n",
      "86/152\n",
      "87/152\n",
      "88/152\n",
      "89/152\n",
      "90/152\n",
      "91/152\n",
      "92/152\n",
      "93/152\n",
      "94/152\n",
      "95/152\n",
      "96/152\n",
      "97/152\n",
      "98/152\n",
      "99/152\n",
      "100/152\n",
      "101/152\n",
      "102/152\n",
      "103/152\n",
      "104/152\n",
      "105/152\n",
      "106/152\n",
      "107/152\n",
      "108/152\n",
      "109/152\n",
      "110/152\n",
      "111/152\n",
      "112/152\n",
      "113/152\n",
      "114/152\n",
      "115/152\n",
      "116/152\n",
      "117/152\n",
      "118/152\n",
      "119/152\n",
      "120/152\n",
      "121/152\n",
      "122/152\n",
      "123/152\n",
      "124/152\n",
      "125/152\n",
      "126/152\n",
      "127/152\n",
      "128/152\n",
      "129/152\n",
      "130/152\n",
      "131/152\n",
      "132/152\n",
      "133/152\n",
      "134/152\n",
      "135/152\n",
      "136/152\n",
      "137/152\n",
      "138/152\n",
      "139/152\n",
      "140/152\n",
      "141/152\n",
      "142/152\n",
      "143/152\n",
      "144/152\n",
      "145/152\n",
      "146/152\n",
      "147/152\n",
      "148/152\n",
      "149/152\n",
      "150/152\n",
      "151/152\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_features = get_sparse_matrix_2(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/34\n",
      "1/34\n",
      "2/34\n",
      "3/34\n",
      "4/34\n",
      "5/34\n",
      "6/34\n",
      "7/34\n",
      "8/34\n",
      "9/34\n",
      "10/34\n",
      "11/34\n",
      "12/34\n",
      "13/34\n",
      "14/34\n",
      "15/34\n",
      "16/34\n",
      "17/34\n",
      "18/34\n",
      "19/34\n",
      "20/34\n",
      "21/34\n",
      "22/34\n",
      "23/34\n",
      "24/34\n",
      "25/34\n",
      "26/34\n",
      "27/34\n",
      "28/34\n",
      "29/34\n",
      "30/34\n",
      "31/34\n",
      "32/34\n",
      "33/34\n"
     ]
    }
   ],
   "source": [
    "test_labels, test_features = get_sparse_matrix_2(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([train_features,train_labels,test_features,test_labels],open('train_test_data_superclust_w_all_feat.pck','w'))\n",
    "#train_features,train_labels,test_features,test_labels = pickle.load(open('train_test_data_superclust_w_all_feat.pck','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegr AUCROC: 0.597811773318\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 0.01)\n",
    "mLR.fit(X = train_features,y = train_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegr AUCROC: 0.595666620052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n",
      "  \" 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 1,class_weight = 'auto')\n",
    "mLR.fit(X = train_features,y = train_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.5, 'l1'), 0.59098475338660805], [(0.5, 'l2'), 0.5917140064965497], [(1, 'l1'), 0.59257132119265288], [(1, 'l2'), 0.5923741801180924], [(2, 'l1'), 0.59118285580880081], [(2, 'l2'), 0.59219354829882509]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "params = [\n",
    "    # C\n",
    "    [0.5,1,2],\n",
    "    # penalty\n",
    "    ['l1','l2'],    \n",
    "]\n",
    "\n",
    "#n_iter_search = 10\n",
    "aucroc = []\n",
    "for C,penalty in itertools.product(*params):\n",
    "    clf = sklearn.linear_model.LogisticRegression(penalty = penalty,C = C,class_weight = 'auto')\n",
    "    clf.fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    aucroc.append([(C,penalty),sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in clf.predict_proba(test_features)]\n",
    "    )])\n",
    "\n",
    "print(aucroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_index = [i for i in range(len(train_labels)) if train_labels[i] == 1 or np.random.randint(0,4) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sampled_labels = [train_labels[e] for e in sample_index]\n",
    "train_sampled_features = train_features[sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegr AUCROC: 0.592571391043\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 1, class_weight = 'auto')\n",
    "mLR.fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars AUCROC:    0.593023390056\n",
      "LinRegr AUCROC:    0.590445436645\n",
      "Ridge AUCROC:    0.591382633208\n",
      "ElasticNet AUCROC:    0.5\n",
      "BayesianRidge AUCROC:    0.593445548406\n",
      "Lasso AUCROC:    0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lin_models = {\n",
    "    'Ridge': linear_model.Ridge (alpha = .5),\n",
    "    'Lasso': linear_model.Lasso(),\n",
    "    'ElasticNet': linear_model.ElasticNet(),\n",
    "    'Lars': linear_model.Lars(),\n",
    "    'LinRegr': linear_model.LinearRegression(),\n",
    "    'BayesianRidge':  linear_model.BayesianRidge()\n",
    "}\n",
    "for m in lin_models:\n",
    "    lin_models[m].fit(X = train_features.toarray(),y = train_labels)\n",
    "    print('{0} AUCROC:    {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true =  test_labels , \n",
    "                y_score = lin_models[m].predict(test_features.toarray()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest AUCROC: 0.593976064676\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "#from sklearn import svm\n",
    "clf = {\n",
    "   # 'LinSVC': sklearn.svm.LinearSVC(penalty='l1', loss='squared_hinge',C=1.0, class_weight='auto', max_iter=1000),\n",
    "   #'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'RandomForest': sklearn.ensemble.RandomForestClassifier(max_depth = 5,n_estimators = 50),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(v-4) % 8 for v in (sorted([e[1] for e in sorted(zip(clf['RandomForest'].feature_importances_,range(len(clf['RandomForest'].feature_importances_))),reverse = True)[:31]]))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBClassifier.get_xgb_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfXGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clfXGB = xgb.XGBClassifier()\n",
    "clfXGB.fit(X = train_features,y = train_labels)\n",
    "print('{0} AUCROC: {1}'.format('XGBoost',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clfXGB.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUCROC: 0.606434786497\n"
     ]
    }
   ],
   "source": [
    "print('{0} AUCROC: {1}'.format('XGBoost',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clfXGB.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBClassifier.score of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(zip(clfXGB.feature_importances_,range(len(clfXGB.feature_importances_))),inverse = True)[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost AUCROC: 0.602660247649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clfAB = AdaBoostClassifier()\n",
    "clfAB.fit(X = train_features,y = train_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('AdaBoost',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clfAB.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clfAB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-44810bc55b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfAB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clfAB' is not defined"
     ]
    }
   ],
   "source": [
    "len(clfAB.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'nthread': -1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 0,\n",
       " 'silent': True,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    " \n",
    "sys.path.append('xgboost/wrapper/')\n",
    "import xgboost as xgb\n",
    " \n",
    " \n",
    "class XGBoostClassifier():\n",
    "    def __init__(self, num_boost_round=10, **params):\n",
    "        self.clf = None\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.params = params\n",
    "        self.params.update({'objective': 'multi:softprob'})\n",
    " \n",
    "    def fit(self, X, y, num_boost_round=None):\n",
    "        num_boost_round = num_boost_round or self.num_boost_round\n",
    "        self.label2num = dict((label, i) for i, label in enumerate(sorted(set(y))))\n",
    "        dtrain = xgb.DMatrix(X, label=[self.label2num[label] for label in y])\n",
    "        self.clf = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=num_boost_round)\n",
    " \n",
    "    def predict(self, X):\n",
    "        num2label = dict((i, label)for label, i in self.label2num.items())\n",
    "        Y = self.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return np.array([num2label[i] for i in y])\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.clf.predict(dtest)\n",
    " \n",
    "    def score(self, X, y):\n",
    "        Y = self.predict_proba(X)\n",
    "        return 1 / logloss(y, Y)\n",
    " \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        if 'num_boost_round' in params:\n",
    "            self.num_boost_round = params.pop('num_boost_round')\n",
    "        if 'objective' in params:\n",
    "            del params['objective']\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "def logloss(y_true, Y_pred):\n",
    "    label2num = dict((name, i) for i, name in enumerate(sorted(set(y_true))))\n",
    "    return -1 * sum(math.log(y[label2num[label]]) if y[label2num[label]] > 0 else -np.inf for y, label in zip(Y_pred, y_true)) / len(Y_pred)\n",
    "\n",
    "\n",
    "def main():\n",
    "    clf = XGBoostClassifier(\n",
    "        eval_metric = 'auc',\n",
    "        num_class = 2,\n",
    "        nthread = 4,\n",
    "        eta = 0.1,\n",
    "        num_boost_round = 80,\n",
    "        max_depth = 12,\n",
    "        subsample = 0.5,\n",
    "        colsample_bytree = 1.0,\n",
    "        silent = 1,\n",
    "        )\n",
    "    parameters = {\n",
    "        'num_boost_round': [100, 250, 500],\n",
    "        'eta': [0.05, 0.1, 0.3],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "    }\n",
    "    clf = GridSearchCV(clf, parameters, n_jobs=1, cv=2)\n",
    "    \n",
    "    clf.fit([[1,2], [3,4], [2,1], [4,3]], ['a', 'b', 'a', 'b'])\n",
    "    best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "    print(score)\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                \n",
    "    print(clf.predict([[1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD AUCROC: 0.540228960601\n",
      "Nearest centriod AUCROC: 0.522471749342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.neighbors\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'SGD': sklearn.linear_model.SGDClassifier(penalty = 'elasticnet',loss = 'log',class_weight = 'auto'),\n",
    "    'Nearest centriod': sklearn.neighbors.NearestCentroid(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_features,y = train_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = clf[m].predict(test_features)\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Multinpomial AUCROC: 0.583131827909\n",
      "NaiveBayes Bernoulli AUCROC: 0.572231601103\n",
      "NaiveBayes Gaussian AUCROC: 0.517012349753\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes\n",
    "\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'NaiveBayes Multinomial': sklearn.naive_bayes.MultinomialNB(alpha = 0.001),\n",
    "    'NaiveBayes Bernoulli': sklearn.naive_bayes.BernoulliNB(alpha = 0.01),\n",
    "    'NaiveBayes Gaussian': sklearn.naive_bayes.GaussianNB(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features.toarray(),y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features.toarray())]\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BernoulliRBM' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-325a8c2df076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n\u001b[0;32m     15\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     )))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BernoulliRBM' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'neural_network.BernoulliRBM': sklearn.neural_network.BernoulliRBM(),\n",
    "   # 'NaiveBayes Bernoulli': sklearn.naive_bayes.BernoulliNB(alpha = 0.01),\n",
    "   # 'NaiveBayes Gaussian': sklearn.naive_bayes.GaussianNB(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features.toarray())]\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
