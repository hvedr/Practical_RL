{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кредитный скоринг по модели. Ноутбук устарел . Можно стереть наверное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re, sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cPickle\n",
    "from pylightgbm.models import GBMClassifier\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "#import sklearn, sklearn.cross_validation\n",
    "#from pyspark.mllib.regression import LabeledPoint\n",
    "#from pyspark.mllib.feature import HashingTF\n",
    "#from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "#import scipy.sparse as sps\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.mllib.linalg import SparseVector\n",
    "#from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "#import hashlib\n",
    "#from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('{0}: cred scoring started.').format(datetime.datetime.now())\n",
    "sys.stdout.flush()\n",
    "\n",
    "HIVE_HOST = 'ds-hadoop-cs01p'\n",
    "HIVE_PORT = 10000\n",
    "HIVE_USER = 'bigdatasys'\n",
    "CONF={'hive.vectorized.execution.enabled':'true'\n",
    "    ,'mapreduce.map.memory.mb':'4096'\n",
    "    ,'mapreduce.map.child.java.opts':'-Xmx4g'\n",
    "    ,'mapreduce.task.io.sort.mb':'1024'\n",
    "    ,'mapreduce.reduce.child.java.opts':'-Xmx4g'\n",
    "    ,'mapreduce.reduce.memory.mb':'7000'\n",
    "    ,'mapreduce.reduce.shuffle.input.buffer.percent':'0.5'\n",
    "    ,'mapreduce.input.fileinputformat.split.minsize':'536870912'\n",
    "    ,'mapreduce.input.fileinputformat.split.maxsize':'1073741824'\n",
    "    ,'hive.optimize.ppd':'true'\n",
    "    ,'hive.merge.smallfiles.avgsize':'536870912'\n",
    "    ,'hive.merge.mapredfiles':'true'\n",
    "    ,'hive.merge.mapfiles':'true'\n",
    "    ,'hive.hadoop.supports.splittable.combineinputformat':'true'\n",
    "    ,'hive.exec.reducers.bytes.per.reducer':'536870912'\n",
    "    ,'hive.exec.parallel':'true'\n",
    "    ,'hive.exec.max.created.files':'10000000'\n",
    "    ,'hive.exec.compress.output':'true'\n",
    "    ,'hive.exec.dynamic.partition.mode':'nonstrict'\n",
    "    ,'hive.exec.max.dynamic.partitions':'1000000'\n",
    "    ,'hive.exec.max.dynamic.partitions.pernode':'100000'\n",
    "    ,'io.seqfile.compression.type':'BLOCK'\n",
    "    ,'mapreduce.map.failures.maxpercent':'5'\n",
    "          }\n",
    "\n",
    "from pyhive import hive\n",
    "conn = hive.Connection(host=HIVE_HOST, port=HIVE_PORT, username=HIVE_USER, configuration=CONF)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "\n",
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 16)\n",
    "        .set(\"spark.driver.maxResultSize\", \"12g\")\n",
    "        .set('spark.driver.memory','12g')\n",
    "        .set(\"spark.executor.memory\", '8g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)\n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-07 12:44:11.606571: cred scoring started.\n"
     ]
    }
   ],
   "source": [
    "# to delete\n",
    "def add_months(ymd,months):\n",
    "    '''ymd is datetime.datetime.'''\n",
    "    year = ymd.year + (ymd.month + months - 1) / 12\n",
    "    month = (ymd.month + months - 1) % 12 + 1\n",
    "    day = ymd.day\n",
    "    return datetime.datetime(year,month,day)\n",
    "\n",
    "\n",
    "def get_last_features_date(cursor, first_calc_ymd, last_calc_ymd):\n",
    "    cursor.execute('''select 1 from prod_features_liveinternet.phone_x_feature \n",
    "                  where first_calc_ymd = \"#first_calc_ymd\" and last_calc_ymd = \"#last_calc_ymd\"'''\n",
    "               .replace( \"#first_calc_ymd\",first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "               .replace( \"#last_calc_ymd\",last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "            ) \n",
    "    return not len(cursor.fetchone()) > 0\n",
    "\n",
    "def main():\n",
    "    \n",
    "    acc_months = 2\n",
    "    ymd_to_calc   = datetime.datetime.combine(datetime.date.today(),datetime.time(0,0,0)) - datetime.timedelta(days = 2) #!!!!!!!!!!!!!!!!!!   # yesterday with 0 time \n",
    "    first_calc_ymd = add_months(datetime.datetime(ymd_to_calc.year,ymd_to_calc.month,1), - acc_months)\n",
    "    \n",
    "    ymd_loaded = get_last_features_date(cursor)\n",
    "    print('{}. Getting visits later than {}.\\n'.format(datetime.datetime.now(),ymd_loaded))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    cnt = 0\n",
    "    while (ymd_to_calc.strftime('%Y-%m-%d') > ymd_loaded):\n",
    "        cnt += 1\n",
    "        if cnt > 70:\n",
    "            print('{}. Failed to wait for features to be loaded later than {}. Terminating.\\n'.format(datetime.datetime.now(),ymd_loaded) + '*'*60)\n",
    "            sys.stdout.flush()\n",
    "            exit(1)\n",
    "        print('Sleeping for 1000 sec. ymd_to_calc: {}. ymd_loaded: {}.'.format(ymd_to_calc.strftime('%Y-%m-%d'),ymd_loaded))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1000)\n",
    "        ymd_loaded = get_last_visits_date(cursor)\n",
    "    \n",
    "    print('{} Got visits for {}. Calculating signals for {}'.format(datetime.datetime.now(),ymd_loaded,ymd_to_calc.strftime('%Y-%m-%d')))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-07 13:12:07.875281. Getting features for (2017-04-01,2017-06-05).\n",
      "2017-06-07 13:12:09.806471. Got features or (2017-04-01,2017-06-05). Calculating cred scores.\n"
     ]
    }
   ],
   "source": [
    "def add_months(ymd,months):\n",
    "    '''ymd is datetime.datetime.'''\n",
    "    year = ymd.year + (ymd.month + months - 1) / 12\n",
    "    month = (ymd.month + months - 1) % 12 + 1\n",
    "    day = ymd.day\n",
    "    return datetime.datetime(year,month,day)\n",
    "\n",
    "\n",
    "def check_features_dates(hc, first_calc_ymd, last_calc_ymd):\n",
    "    return ( len(hc.sql('''select 1 from prod_features_liveinternet.phone_x_feature \n",
    "                           where first_calc_ymd = \"#first_calc_ymd\" and last_calc_ymd = \"#last_calc_ymd\"\n",
    "                           limit 1'''\n",
    "                               .replace( \"#first_calc_ymd\",first_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                               .replace( \"#last_calc_ymd\",last_calc_ymd.strftime('%Y-%m-%d'))\n",
    "                        ).collect()\n",
    "                 ) > 0\n",
    "            )\n",
    "def main():\n",
    "    \n",
    "    acc_months = 2\n",
    "    # yesterday with 0 time\n",
    "    #ymd_to_calc   = datetime.datetime.combine(datetime.date.today(),datetime.time(0,0,0)) - datetime.timedelta(days = 1)  \n",
    "    ymd_to_calc   = datetime.datetime(2017, 6, 5)\n",
    "    first_calc_ymd = add_months(datetime.datetime(ymd_to_calc.year,ymd_to_calc.month,1), - acc_months)\n",
    "    \n",
    "    print('{}. Getting features for ({},{}).'\n",
    "                    .format(datetime.datetime.now(),first_calc_ymd.strftime('%Y-%m-%d'), ymd_to_calc.strftime('%Y-%m-%d')))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    cnt = 0\n",
    "    while (not check_features_dates(hc, first_calc_ymd, ymd_to_calc)):\n",
    "        cnt += 1\n",
    "        if cnt > 70:\n",
    "            print('{}. Failed to get features to be loaded for ({},{}). Terminating.\\n'\n",
    "                    .format(datetime.datetime.now(),first_calc_ymd.strftime('%Y-%m-%d'), ymd_to_calc.strftime('%Y-%m-%d')) + '*'*60)\n",
    "            sys.stdout.flush()\n",
    "            exit(1)\n",
    "        print('{}.Sleeping for 1000 sec. (first_calc_ymd,ymd_to_calc): ({},{}).'\n",
    "              .format(datetime.datetime.now(),first_calc_ymd.strftime('%Y-%m-%d'), ymd_to_calc.strftime('%Y-%m-%d')))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1000)\n",
    "    \n",
    "    print('{}. Got features or ({},{}). Calculating cred scores.'\n",
    "                    .format(datetime.datetime.now(),first_calc_ymd.strftime('%Y-%m-%d'), ymd_to_calc.strftime('%Y-%m-%d')))\n",
    "    sys.stdout.flush()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hand_pipe1 = cPickle.load(open('data/cred_scor_classifier_pipe3.pck','r'))\n",
    "feats                = hand_pipe1[0][1]\n",
    "categorical_features = hand_pipe1[1][1]\n",
    "labelencoder         = hand_pipe1[2][1]\n",
    "target_encoded       = hand_pipe1[3][1]\n",
    "clf                  = hand_pipe1[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 17:45:57.831054. 0 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 18:01:43.492195. 1 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 18:17:27.656690. 2 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 18:33:46.585368. 3 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 18:50:47.311975. 4 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 19:06:45.865694. 5 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 19:22:44.987500. 6 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 19:38:44.317915. 7 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 19:54:55.288688. 8 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 20:11:14.861393. 9 handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 20:27:36.990157. a handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 20:43:55.036458. b handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 21:00:11.836942. c handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 21:16:26.000663. d handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 21:33:02.689697. e handled.\n",
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n",
      "2017-06-07 21:49:04.931263. f handled.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    hc.sql('drop table prod_features_liveinternet.phone_cred_scor_tmp')\n",
    "    os.popen('hadoop fs -mkdir -p /prod_features/liveinternet/phone_cred_scor_tmp').read()\n",
    "    os.popen('hadoop fs -rm -r /prod_features/liveinternet/phone_cred_scor_tmp/*').read()\n",
    "\n",
    "    for ind in list('0123456789abcdef'):\n",
    "        df_batch = (hc.sql('''select * from prod_features_liveinternet.phone_x_feature \n",
    "                        where  first_calc_ymd = \"#first_calc_ymd\" \n",
    "                               and last_calc_ymd = \"#last_calc_ymd\"\n",
    "                               and substr(md5(phone_num),1,1) = '#ind' '''\n",
    "                       .replace( \"#first_calc_ymd\",'2017-04-01')\n",
    "                       .replace( \"#last_calc_ymd\",'2017-06-05')\n",
    "                       .replace( \"#ind\",ind)\n",
    "                    )        \n",
    "                .toPandas()\n",
    "                 )\n",
    "        for v in categorical_features:\n",
    "            df_batch.loc[:,v]  = df_batch.loc[:,v].map(lambda f: labelencoder[v].get(f,-1))\n",
    "            df_batch.loc[:,v + '_encoded'] = df_batch[v].map(target_encoded[v])\n",
    "        df_batch['pred'] = clf.predict_proba(df_batch[feats])[:,1]\n",
    "        df_batch[['phone_num','pred','last_calc_ymd']].to_csv('data/phone_cred_scor_tmp.txt'.format(ind),index = False,header = False,quote_char = '')\n",
    "        os.popen('hadoop fs -put data/phone_cred_scor_tmp.txt /prod_features/liveinternet/phone_cred_scor_tmp/phone_cred_scor_tmp_{}.txt'.format(ind)).read()\n",
    "        print('{}. {} handled.'.format(datetime.datetime.now(),ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Table not found: prod_features_liveinternet.phone_cred_score; line 1 pos 78'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-0bc401fb4b59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mend_query\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mhc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \"\"\"\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u'Table not found: prod_features_liveinternet.phone_cred_score; line 1 pos 78'"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    end_query = '''\n",
    "    create external table prod_features_liveinternet.phone_cred_scor_tmp\n",
    "    (\n",
    "      `phone_num` string,\n",
    "      `score` float,  \n",
    "      `ymd` string\n",
    "      )\n",
    "    ROW FORMAT DELIMITED\n",
    "       FIELDS TERMINATED BY ','\n",
    "        LOCATION\n",
    "          '/prod_features/liveinternet/phone_cred_scor_tmp'\n",
    "    ;\n",
    "\n",
    "    insert overwrite table prod_lookalike.phone_x_segment partition (ymd)\n",
    "    select \n",
    "      phone_num,\n",
    "      score,\n",
    "      current_timestamp() as load_dttm,\n",
    "      'cred_score_1' as segment_nm,\n",
    "      ymd\n",
    "    from prod_features_liveinternet.phone_cred_scor_tmp;\n",
    "    '''\n",
    "\n",
    "    for q in end_query.split(';'):\n",
    "        hc.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Finished loading parameters\n",
      "[LightGBM] [Info] Finished loading 500 models\n",
      "[LightGBM] [Info] Finished initializing prediction\n",
      "[LightGBM] [Info] Finished prediction\n"
     ]
    }
   ],
   "source": [
    "for v in categorical_features:\n",
    "    df_batch.loc[:,v]  = df_batch.loc[:,v].map(lambda f: labelencoder[v].get(f,-1))\n",
    "    df_batch.loc[:,v + '_encoded'] = df_batch[v].map(target_encoded[v])\n",
    "    \n",
    "df_batch['pred'] = clf.predict_proba(df_batch[feats])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_batch[['phone_num','pred','last_calc_ymd']].to_csv('data/phone_cred_scor_tmp.txt'.format(ind),index = False,header = False,quote_char = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 1\n",
    "os.popen('hadoop fs -put data/phone_cred_scor_tmp.txt /prod_features/liveinternet/phone_cred_scor_tmp/phone_cred_scor_tmp_{}.txt'.format(ind)).read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.popen('hadoop fs -rm -r /prod_features/liveinternet/phone_cred_scor_tmp/*').read()\n",
    "! hadoop fs -du -h /prod_features/liveinternet/phone_cred_scor_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_query = '''\n",
    "create external table prod_features_liveinternet.phone_cred_scor_tmp\n",
    "(\n",
    "  `phone_num` string,\n",
    "  `score` float,  \n",
    "  `ymd` string\n",
    "  )\n",
    "ROW FORMAT DELIMITED\n",
    "   FIELDS TERMINATED BY ','\n",
    "    LOCATION\n",
    "      '/prod_features/liveinternet/phone_cred_scor_tmp'\n",
    ";\n",
    "\n",
    "insert overwrite table prod_features_liveinternet.phone_cred_score partition (ymd)\n",
    "select * from prod_features_liveinternet.phone_cred_scor_tmp;\n",
    "'''\n",
    "\n",
    "for q in end_query.split(';'):\n",
    "    hc.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create table prod_features_liveinternet.phone_cred_scor_tmp\n",
    "(\n",
    "  `phone_num` string,\n",
    "  `score` float,\n",
    "  `first_calc_ymd` string, \n",
    "  `last_calc_ymd` string\n",
    "  );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
