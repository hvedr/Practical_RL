{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Холодные звонки\n",
    "\n",
    "Эффективность замены доменов на кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().set(\"spark.executor.instances\", 32).set(\"spark.driver.maxResultSize\", \"16g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_query = '''\n",
    "select \n",
    "    ymd, \n",
    "    ccall_approve_label as label, \n",
    "    score_ccall_total, \n",
    "    score_ccall_approve_total, \n",
    "    score_ccall_not_approve_total, \n",
    "    score_ccall_approve_ccall_not_approve, \n",
    "    score_ccall_approve_total_weekly\n",
    "from user_kposminin.ccalls_scores_2\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_clusters_query = '''\n",
    "select \n",
    "    ymd, \n",
    "    full_app as label, \n",
    "    features\n",
    "from user_kposminin.ccalls_visits_clusters m\n",
    "left join user_kposminin.cold_calls_havent_started neg on neg.phone_num = m.phone_num\n",
    "WHERE neg.phone_num is Null\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_superclusters_query = '''\n",
    "select   \n",
    "  phone_num,\n",
    "  -- max(label) as label,\n",
    "  max(full_app) as label,\n",
    "  collect_set(supercluster) as features,\n",
    "  ymd\n",
    "from \n",
    "  (select\n",
    "    v.ymd,\n",
    "    v.phone_num,\n",
    "    cast(v.label as tinyint) as label,\n",
    "    cast(v.full_app as tinyint) as full_app,   \n",
    "  --  cast(v.visit_lag as tinyint) as visit_lag,\n",
    "    cast(c.supercluster as int) as supercluster\n",
    "  from user_kposminin.ccalls_visits_1 v\n",
    "  inner join user_kposminin.domain_clusters c on c.domain = split(v.urlfr,'#')[0]\n",
    "  left semi join user_kposminin.cold_calls_matched_5 m on m.phone_num = v.phone_num and m.ymd = v.ymd and m.havent_started = 0\n",
    "  ) a\n",
    "group by phone_num, ymd\n",
    "'''\n",
    "\n",
    "read_ccalls_visits_superclusters_with_count_query = '''\n",
    "select \n",
    "  ymd,\n",
    "  phone_num,\n",
    "  max(label) as label,\n",
    "  collect_list(concat(supercluster,\":\",cnt)) as features\n",
    "from\n",
    "  (select   \n",
    "    phone_num,\n",
    "    -- max(label) as label,\n",
    "    max(full_app) as label,\n",
    "    supercluster,\n",
    "    count(visit_lag) as cnt,\n",
    "    stddev(visit_lag) as std,\n",
    "    ymd\n",
    "  from \n",
    "    (select\n",
    "      v.ymd,\n",
    "      v.phone_num,\n",
    "      cast(v.label as tinyint) as label,\n",
    "      cast(v.full_app as tinyint) as full_app,  \n",
    "      cast(c.supercluster as int) as supercluster,\n",
    "      v.cnt,\n",
    "      v.visit_lag\n",
    "    from user_kposminin.ccalls_visits_1 v\n",
    "    inner join user_kposminin.domain_clusters c on concat(c.domain,'#') = v.urlfr\n",
    "    left semi join user_kposminin.cold_calls_matched_5 m on m.phone_num = v.phone_num and m.ymd = v.ymd and m.havent_started = 0\n",
    "    ) a\n",
    "  group by phone_num, ymd, supercluster\n",
    "--  order by ymd,phone_num, supercluster\n",
    "  ) b\n",
    "group by ymd, phone_num\n",
    "'''\n",
    "\n",
    "#from scipy.sparse import coo_matrix, vstack,hstack\n",
    "\n",
    "#tf = HashingTF(numFeatures = 10 ** 5)\n",
    "\n",
    "data = hc.sql(read_ccalls_visits_superclusters_with_count_query).rdd\n",
    "train_data = data \\\n",
    "    .filter(lambda row: row['ymd'] < '2016-05-16') \\\n",
    "    .map(lambda r: (r.label,r.features)) \\\n",
    "    .collect()\n",
    "test_data  = data \\\n",
    "    .filter(lambda row: row['ymd'] >= '2016-05-16') \\\n",
    "    .map(lambda r: (r.label,r.features)) \\\n",
    "    .collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/152\n",
      "1/152\n",
      "2/152\n",
      "3/152\n",
      "4/152\n",
      "5/152\n",
      "6/152\n",
      "7/152\n",
      "8/152\n",
      "9/152\n",
      "10/152\n",
      "11/152\n",
      "12/152\n",
      "13/152\n",
      "14/152\n",
      "15/152\n",
      "16/152\n",
      "17/152\n",
      "18/152\n",
      "19/152\n",
      "20/152\n",
      "21/152\n",
      "22/152\n",
      "23/152\n",
      "24/152\n",
      "25/152\n",
      "26/152\n",
      "27/152\n",
      "28/152\n",
      "29/152\n",
      "30/152\n",
      "31/152\n",
      "32/152\n",
      "33/152\n",
      "34/152\n",
      "35/152\n",
      "36/152\n",
      "37/152\n",
      "38/152\n",
      "39/152\n",
      "40/152\n",
      "41/152\n",
      "42/152\n",
      "43/152\n",
      "44/152\n",
      "45/152\n",
      "46/152\n",
      "47/152\n",
      "48/152\n",
      "49/152\n",
      "50/152\n",
      "51/152\n",
      "52/152\n",
      "53/152\n",
      "54/152\n",
      "55/152\n",
      "56/152\n",
      "57/152\n",
      "58/152\n",
      "59/152\n",
      "60/152\n",
      "61/152\n",
      "62/152\n",
      "63/152\n",
      "64/152\n",
      "65/152\n",
      "66/152\n",
      "67/152\n",
      "68/152\n",
      "69/152\n",
      "70/152\n",
      "71/152\n",
      "72/152\n",
      "73/152\n",
      "74/152\n",
      "75/152\n",
      "76/152\n",
      "77/152\n",
      "78/152\n",
      "79/152\n",
      "80/152\n",
      "81/152\n",
      "82/152\n",
      "83/152\n",
      "84/152\n",
      "85/152\n",
      "86/152\n",
      "87/152\n",
      "88/152\n",
      "89/152\n",
      "90/152\n",
      "91/152\n",
      "92/152\n",
      "93/152\n",
      "94/152\n",
      "95/152\n",
      "96/152\n",
      "97/152\n",
      "98/152\n",
      "99/152\n",
      "100/152\n",
      "101/152\n",
      "102/152\n",
      "103/152\n",
      "104/152\n",
      "105/152\n",
      "106/152\n",
      "107/152\n",
      "108/152\n",
      "109/152\n",
      "110/152\n",
      "111/152\n",
      "112/152\n",
      "113/152\n",
      "114/152\n",
      "115/152\n",
      "116/152\n",
      "117/152\n",
      "118/152\n",
      "119/152\n",
      "120/152\n",
      "121/152\n",
      "122/152\n",
      "123/152\n",
      "124/152\n",
      "125/152\n",
      "126/152\n",
      "127/152\n",
      "128/152\n",
      "129/152\n",
      "130/152\n",
      "131/152\n",
      "132/152\n",
      "133/152\n",
      "134/152\n",
      "135/152\n",
      "136/152\n",
      "137/152\n",
      "138/152\n",
      "139/152\n",
      "140/152\n",
      "141/152\n",
      "142/152\n",
      "143/152\n",
      "144/152\n",
      "145/152\n",
      "146/152\n",
      "147/152\n",
      "148/152\n",
      "149/152\n",
      "150/152\n",
      "151/152\n"
     ]
    }
   ],
   "source": [
    "#Transform into sparse matrix\n",
    "from scipy.sparse import csr_matrix, vstack,hstack\n",
    "train_labels, train_features = [0], csr_matrix((1,1000))\n",
    "i = 0\n",
    "batch_size = 3000\n",
    "for i in range(len(train_data)/batch_size + 1):\n",
    "    batch = csr_matrix((1,1000))\n",
    "    for r in train_data[batch_size*i:batch_size*(i+1)]:\n",
    "        #idx  = [int(e.split(':')[0]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "        #vals = [int(e.split(':')[1]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "        idx  = [int(e.split(':')[0]) for e in r[1]]\n",
    "        vals = [int(e.split(':')[1]) for e in r[1]]\n",
    "        train_labels.append(r[0])\n",
    "        new_row = csr_matrix(([float(v)/sum(vals) for v in vals], ([0]*len(idx),idx)), shape=(1,1000), dtype = type(1.))\n",
    "        batch = vstack([batch,new_row])\n",
    "    train_features = vstack([train_features, batch[1:]])\n",
    "    print(str(i)+'/'+str(len(train_data)/batch_size + 1))\n",
    "    \n",
    "train_labels, train_features = train_labels[1:],train_features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/34\n",
      "1/34\n",
      "2/34\n",
      "3/34\n",
      "4/34\n",
      "5/34\n",
      "6/34\n",
      "7/34\n",
      "8/34\n",
      "9/34\n",
      "10/34\n",
      "11/34\n",
      "12/34\n",
      "13/34\n",
      "14/34\n",
      "15/34\n",
      "16/34\n",
      "17/34\n",
      "18/34\n",
      "19/34\n",
      "20/34\n",
      "21/34\n",
      "22/34\n",
      "23/34\n",
      "24/34\n",
      "25/34\n",
      "26/34\n",
      "27/34\n",
      "28/34\n",
      "29/34\n",
      "30/34\n",
      "31/34\n",
      "32/34\n",
      "33/34\n"
     ]
    }
   ],
   "source": [
    "test_labels, test_features = [0], csr_matrix((1,1000))\n",
    "i = 0\n",
    "batch_size = 3000\n",
    "for i in range(len(test_data)/batch_size + 1):\n",
    "    batch = csr_matrix((1,1000))\n",
    "    for r in test_data[batch_size*i:batch_size*(i+1)]:\n",
    "        #idx  = [int(e.split(':')[0]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "        #vals = [int(e.split(':')[1]) for e in r[1] if 0 <= int(e.split(':')[0]) < 1000]\n",
    "        idx  = [int(e.split(':')[0]) for e in r[1]]\n",
    "        vals = [int(e.split(':')[1]) for e in r[1]]\n",
    "        test_labels.append(r[0])\n",
    "        new_row = csr_matrix(([float(v)/sum(vals) for v in vals], ([0]*len(idx),idx)), shape=(1,1000), dtype = type(1.))\n",
    "        batch = vstack([batch,new_row])\n",
    "    test_features = vstack([test_features, batch[1:]])\n",
    "    print(str(i)+'/'+str(len(test_data)/batch_size + 1))\n",
    "\n",
    "test_labels, test_features = test_labels[1:],test_features[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([train_features,train_labels,test_features,test_labels],open('train_test_data_superclust_w_count_v2.pck','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cf3ca8b90126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 0.01)\n",
    "mLR.fit(X = train_features,y = train_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegr AUCROC: 0.584243534631\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 1,class_weight = 'auto')\n",
    "mLR.fit(X = train_features,y = train_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.5, 'l1'), 0.59098475338660805], [(0.5, 'l2'), 0.5917140064965497], [(1, 'l1'), 0.59257132119265288], [(1, 'l2'), 0.5923741801180924], [(2, 'l1'), 0.59118285580880081], [(2, 'l2'), 0.59219354829882509]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "params = [\n",
    "    # C\n",
    "    [0.5,1,2],\n",
    "    # penalty\n",
    "    ['l1','l2'],    \n",
    "]\n",
    "\n",
    "#n_iter_search = 10\n",
    "aucroc = []\n",
    "for C,penalty in itertools.product(*params):\n",
    "    clf = sklearn.linear_model.LogisticRegression(penalty = penalty,C = C,class_weight = 'auto')\n",
    "    clf.fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    aucroc.append([(C,penalty),sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in clf.predict_proba(test_features)]\n",
    "    )])\n",
    "\n",
    "print(aucroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_index = [i for i in range(len(train_labels)) if train_labels[i] == 1 or np.random.randint(0,4) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sampled_labels = [train_labels[e] for e in sample_index]\n",
    "train_sampled_features = train_features[sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegr AUCROC: 0.592571391043\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "mLR = sklearn.linear_model.LogisticRegression(penalty = 'l1',C = 1, class_weight = 'auto')\n",
    "mLR.fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "\n",
    "print('{0} AUCROC: {1}'.format('LogRegr',sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in mLR.predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars AUCROC:    0.593023390056\n",
      "LinRegr AUCROC:    0.590445436645\n",
      "Ridge AUCROC:    0.591382633208\n",
      "ElasticNet AUCROC:    0.5\n",
      "BayesianRidge AUCROC:    0.593445548406\n",
      "Lasso AUCROC:    0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lin_models = {\n",
    "    'Ridge': linear_model.Ridge (alpha = .5),\n",
    "    'Lasso': linear_model.Lasso(),\n",
    "    'ElasticNet': linear_model.ElasticNet(),\n",
    "    'Lars': linear_model.Lars(),\n",
    "    'LinRegr': linear_model.LinearRegression(),\n",
    "    'BayesianRidge':  linear_model.BayesianRidge()\n",
    "}\n",
    "for m in lin_models:\n",
    "    lin_models[m].fit(X = train_features.toarray(),y = train_labels)\n",
    "    print('{0} AUCROC:    {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true =  test_labels , \n",
    "                y_score = lin_models[m].predict(test_features.toarray()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest AUCROC: 0.585710649773\n",
      "SVC AUCROC: 0.497074656601\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "#from sklearn import svm\n",
    "clf = {\n",
    "    'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'RandomForest': sklearn.ensemble.RandomForestClassifier(max_depth = 4,n_estimators = 200),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features)]\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [\n",
    "    # max_depth\n",
    "    [2,5,10,20,50,100],\n",
    "    # trees num\n",
    "    [20,50,100,200,400],\n",
    "    # criterion\n",
    "    ['gini','entropy'],\n",
    "    #max_features\n",
    "    [5,10,20,50,100]\n",
    "]\n",
    "\n",
    "n_iter_search = 20\n",
    "aucroc = []\n",
    "for max_depth, tree_num, criter,max_feat in [[np.random.choice(part) for part in params] for _ in range(n_iter_search)]:\n",
    "    clf = sklearn.ensemble.RandomForestClassifier(\n",
    "        max_depth = max_depth,\n",
    "        n_estimators = tree_num, \n",
    "        criterion = criter,\n",
    "        max_features = max_feat,\n",
    "        class_weight = 'auto')\n",
    "    clf.fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    aucroc.append([(C,penalty),sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels , \n",
    "                y_score = [e[1] for e in clf.predict_proba(test_features)]\n",
    "    )])\n",
    "\n",
    "print('\\n'.join([str(e) for e in aucroc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 100, 'gini', '10'], [100, 20, 'gini', '10']]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [\n",
    "    # max_depth\n",
    "    [2,5,10,20,50,100],\n",
    "    # trees num\n",
    "    [20,50,100,200,400],\n",
    "    # criterion\n",
    "    ['gini','entropy'],\n",
    "    #max_features\n",
    "    [5,10,20,50,'auto']\n",
    "]\n",
    "\n",
    "[[np.random.choice(part) for part in params] for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD AUCROC: 0.540228960601\n",
      "Nearest centriod AUCROC: 0.522471749342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.neighbors\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'SGD': sklearn.linear_model.SGDClassifier(penalty = 'elasticnet',loss = 'log',class_weight = 'auto'),\n",
    "    'Nearest centriod': sklearn.neighbors.NearestCentroid(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_features,y = train_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = clf[m].predict(test_features)\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Multinpomial AUCROC: 0.583131827909\n",
      "NaiveBayes Bernoulli AUCROC: 0.572231601103\n",
      "NaiveBayes Gaussian AUCROC: 0.517012349753\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes\n",
    "\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'NaiveBayes Multinomial': sklearn.naive_bayes.MultinomialNB(alpha = 0.001),\n",
    "    'NaiveBayes Bernoulli': sklearn.naive_bayes.BernoulliNB(alpha = 0.01),\n",
    "    'NaiveBayes Gaussian': sklearn.naive_bayes.GaussianNB(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features.toarray(),y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features.toarray())]\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BernoulliRBM' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-325a8c2df076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n\u001b[0;32m     15\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     )))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BernoulliRBM' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "\n",
    "clf = {\n",
    "   # 'SVC': sklearn.svm.SVC(probability = True,max_iter = 100),\n",
    "   # 'LogRegr': sklearn.linear_model.LogisticRegression(),\n",
    "    'neural_network.BernoulliRBM': sklearn.neural_network.BernoulliRBM(),\n",
    "   # 'NaiveBayes Bernoulli': sklearn.naive_bayes.BernoulliNB(alpha = 0.01),\n",
    "   # 'NaiveBayes Gaussian': sklearn.naive_bayes.GaussianNB(),\n",
    "   # 'GBM': sklearn.ensemble.GradientBoostingClassifier(n_estimators = 400)\n",
    "    \n",
    "}\n",
    "for m in clf:\n",
    "    clf[m].fit(X = train_sampled_features,y = train_sampled_labels)\n",
    "    print('{0} AUCROC: {1}'.format(m,sklearn.metrics.roc_auc_score(\n",
    "                y_true = test_labels, \n",
    "                y_score = [e[1] for e in clf[m].predict_proba(test_features.toarray())]\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
