{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Тест сегмента лукалайк на лукалайк la_apppr_ccall_2_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 2)\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set('spark.driver.memory','4g')\n",
    "        .set(\"spark.executor.memory\", '2g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)\n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n",
    "\n",
    "for q in hive_config_query.split(';'):\n",
    "    try:\n",
    "        hc.sql(q)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive_queries = '''\n",
    "\n",
    "-- Positive class train\n",
    "create table user_kposminin.la_la_apppr_segment_pos_id as \n",
    "select distinct v.id,v.ymd\n",
    "from prod_odd.visit_feature v\n",
    "inner join dds_dic.max_coeff_url_fragment_score ufc on ufc.url_fragment = v.url_fragment\n",
    "where\n",
    "  ufc.segment_nm = 'la_apppr_ccall_2_1'\n",
    "  and ufc.coeff > -7.2\n",
    "  and v.ymd between '2017-04-17' and '2017-04-30'\n",
    ";\n",
    "\n",
    "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
    "\n",
    "-- Score urlfr\n",
    "WITH t AS \n",
    " (\n",
    "   SELECT\n",
    "   v.url_fragment AS urlfr\n",
    "   ,count(distinct if(ta.ymd between v.ymd and date_add(v.ymd,3),ta.id,Null)) as cnt_positive\n",
    "   ,count(distinct v.id) as cnt_total\n",
    "  FROM\n",
    "   prod_odd.visit_feature v\n",
    "   left join user_kposminin.la_la_apppr_segment_pos_id ta on v.id = ta.id\n",
    "  WHERE\n",
    "   v.ymd between '2017-04-17' and '2017-04-27'\n",
    "   and v.load_src = 'LI.02'\n",
    "  GROUP BY \n",
    "   v.url_fragment\n",
    "   ) \n",
    "INSERT OVERWRITE TABLE \n",
    "  user_kposminin.urlfr_tgt_cnt PARTITION (ymd='2017-04-27', target='lala_apppr') \n",
    "SELECT \n",
    " urlfr AS urlfr\n",
    " ,nvl(cnt_positive, 0) as cnt_positive\n",
    " ,cnt_total\n",
    " ,log((cnt_positive + 0.1)/(cnt_total - cnt_positive + 0.1)) as score\n",
    "FROM t \n",
    ";\n",
    "\n",
    "\n",
    "-- la_apppr_ccall2. Url coefs comparison\n",
    "create table user_kposminin.lala_apppr_test as \n",
    "with \n",
    "pos_id as\n",
    " (\n",
    "    select distinct v.id,v.ymd\n",
    "    from prod_odd.visit_feature v\n",
    "    inner join dds_dic.max_coeff_url_fragment_score ufc on ufc.url_fragment = v.url_fragment\n",
    "    where\n",
    "        ufc.segment_nm = 'la_apppr_ccall_2_1'\n",
    "        and ufc.coeff > -7.2\n",
    "        and v.ymd between '2017-05-16' and '2017-05-19'\n",
    " \n",
    " ),\n",
    "domains_to_exclude as (\n",
    "   select distinct split(url_fragment,'#')[0] as domain    \n",
    "   from dds_dic.max_coeff_url_fragment_score ufc\n",
    "   where\n",
    "        ufc.segment_nm = 'la_apppr_ccall_2_1'\n",
    "        and ufc.coeff > -7.2\n",
    " )\n",
    " \n",
    "\n",
    "  select \n",
    "    v.id, \n",
    "    max(if(pid.id is Null,0,1)) as label, \n",
    "    max(score) as max_score1, \n",
    "    avg(score) as avg_score1,\n",
    "    count(score) as cnt_score1,\n",
    "    max(if(de.domain is Null,score,Null)) as max_score2, \n",
    "    avg(if(de.domain is Null,score,Null)) as avg_score2,\n",
    "    count(if(de.domain is Null,score,Null)) as cnt_score2\n",
    "  from\n",
    "    prod_odd.visit_feature v\n",
    "    left join pos_id pid on pid.id = v.id\n",
    "    left join domains_to_exclude de on split(v.url_fragment,'#')[0] = de.domain\n",
    "    left join (select urlfr as url_fragment,score from \n",
    "               user_kposminin.urlfr_tgt_cnt \n",
    "               where ymd='2017-04-27' and target='lala_apppr'\n",
    "               and ((cnt_total > 4000) or (cnt_positive > 10))\n",
    "              ) utc on utc.url_fragment = v.url_fragment   \n",
    "  where\n",
    "    v.ymd = '2017-05-16' and\n",
    "    (de.domain is Null)\n",
    "  group by\n",
    "    v.id\n",
    ";\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def metrics(y_true,y_score,lift = None, return_str = False):\n",
    "    import sklearn\n",
    "    import collections\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        res = collections.OrderedDict()\n",
    "        samp_size = len(y_true)\n",
    "        res['Sample size'] = samp_size\n",
    "        res['Posit share'] = sum(y_true) * 1./ samp_size\n",
    "        res['Sample size'] = len(y_true)\n",
    "        res['AUC ROC'] = sklearn.metrics.roc_auc_score(y_true = y_true, y_score = y_score)\n",
    "        res['AUC PR'] = sklearn.metrics.average_precision_score( y_true,  y_score)\n",
    "        res['Log loss'] = sklearn.metrics.log_loss(y_true = y_true, y_pred = y_score)\n",
    "        if lift:\n",
    "            predictions_and_labels = sorted(zip(y_score,y_true), key = lambda e:-e[0])\n",
    "            for l in lift:\n",
    "                res['Lift ' + str(l)] = sum([e[1] for e in predictions_and_labels[:int(l * samp_size)]]) * 1. / int(l * samp_size) / res['Posit share']                \n",
    "        if return_str:\n",
    "            res = '\\n'.join(['{:<12}: {:.5f}'.format(k,v) for (k,v) in res.items()]) + '.'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.0283172921116\n",
      "Area under ROC = 0.838607846967\n"
     ]
    }
   ],
   "source": [
    "# ml not working\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "sdf = (hc.sql('select nvl(max_score1,-100) as score,cast(label as double) as label from user_kposminin.lala_apppr_test')\n",
    "         .rdd\n",
    "         .map(lambda row: (float(row['score']),row['label']))\n",
    "      )\n",
    "\n",
    "metrics = BinaryClassificationMetrics(sdf)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Быстрее в hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
