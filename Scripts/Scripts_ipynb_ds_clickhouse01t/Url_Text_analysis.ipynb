{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ текстов интернет-страниц\n",
    "### Задача DMP-3643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! mv /data1/share/kosm/data/data_all.csv /data1/share/kosm/data/parsed_urls_cred_scor.csv\n",
    "#df = pd.read_csv('/data1/share/kosm/data/parsed_urls_cred_scor.csv',sep = '\\t',skiprows=2,index_col=0)\n",
    "#for l in open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r'):\n",
    "first_row = open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r').readline()\n",
    "df = pd.DataFrame([e.split('\\t')[1:] for e in open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r') if not e == first_row])\n",
    "df.columns ='url init_url response_code title body'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Кто подписывает справку 2 НДФЛ - для банка, ди...\n",
       "1          Пэн: Путешествие в Нетландию / Pan / 2015 / ДБ...\n",
       "2          Белоснежка и охотник (2012) смотреть онлайн HD...\n",
       "3                                     Гадание на картах таро\n",
       "4                                             DNSLookupError\n",
       "5                          Добро пожаловать в Одноклассники!\n",
       "6          Купить смартфон Nokia XL Dual Sim RM-1030 в Мо...\n",
       "7                             ALIEXPRESS SHOPPER | ВКонтакте\n",
       "8          Рыба запеченная, 416 рецептов + фото рецепты /...\n",
       "9                              Ляйсан Драчевская | ВКонтакте\n",
       "10         ИГИЛ показало видео казни «шпионов» четырехлет...\n",
       "11                                                     embed\n",
       "12                                             404 Not Found\n",
       "13                              Сплетница района | ВКонтакте\n",
       "14         ТЭФИ 2016: Летучая, Сябитова, Меньшова и други...\n",
       "15                                         Стена | ВКонтакте\n",
       "16         Кастинги на порно категория, страница 3 - Дойк...\n",
       "17         Парни вдвоем трахают по полной программе горяч...\n",
       "18         Биатлон. Одиночная смешанная Эстафета 12.03.20...\n",
       "19         Порно видео онлайн: Домашнее+Русское+Скрытая к...\n",
       "20                                  Tatiana Miro | ВКонтакте\n",
       "21                         Добро пожаловать в Одноклассники!\n",
       "22                      Как приготовить фасоль в мультиварке\n",
       "23                                Продажа Geo (Гео) в России\n",
       "24         ГК РФ Статья 190. Определение срока / Консульт...\n",
       "25         Работа в Сочи, подбор персонала, резюме, вакан...\n",
       "26                         Добро пожаловать в Одноклассники!\n",
       "27                         Добро пожаловать в Одноклассники!\n",
       "28         Продажа Лада 2109 (ВАЗ 2109) в Ханты-Мансийско...\n",
       "29         Сварная сетка для забора по доступной цене | С...\n",
       "                                 ...                        \n",
       "2670605    Мой поиск - аудио/видео файлы без дубликатов |...\n",
       "2670606    Собаки и щенки породы Немецкая овчарка - купит...\n",
       "2670607    Ñêàçêè íà àíãëèéñêîì ÿçûêå íà BiLingual. Àíãëè...\n",
       "2670608                                        Одноклассники\n",
       "2670609                                     Вход | ВКонтакте\n",
       "2670610                                                embed\n",
       "2670611                                        Игра Копатель\n",
       "2670612                                        Одноклассники\n",
       "2670613    Население Челябинской области начнет сокращать...\n",
       "2670614                                Корпорация «ДЭНАС МС»\n",
       "2670615                                       DNSLookupError\n",
       "2670616                             Империя Клуб | ВКонтакте\n",
       "2670617    Гадание Будет ли счастье | Гадание онлайн бесп...\n",
       "2670618          Как в домашних условиях снять зубной камень\n",
       "2670619    Роспотребнадзор узнал, как российские туристы ...\n",
       "2670620    Horny tranny got a dick up her tight ass hole ...\n",
       "2670621    Скачать фильмы через торрент в хорошем качеств...\n",
       "2670622                         Товары и цены - Без названия\n",
       "2670623    Центральный офис Coca-Cola оправдался за публи...\n",
       "2670624                              Положение о ФСИН России\n",
       "2670625    Оденься сама: кройка и шитье для начинающих. |...\n",
       "2670626                                        Одноклассники\n",
       "2670627                                        Одноклассники\n",
       "2670628                                        404 Not Found\n",
       "2670629    Сериал Игра престолов 6 сезон (2016) 9 серия и...\n",
       "2670630    The one-armed gangster (однорукий бандит) | ВК...\n",
       "2670631    Английский алфавит с транскрипцией и русским п...\n",
       "2670632                           Chaqmoq.com - Супер портал\n",
       "2670633                                         TimeoutError\n",
       "2670634                                         TimeoutError\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub(u'[^а-яa-z]+',' ',s.lower())\n",
    "\n",
    "def stem_ru(s, stemmer):\n",
    "    ''' Simple string stemming.\n",
    "        Input string is splitted by space and list of basic form of word + part of speech.\n",
    "        Stemmer is pymystem3.Mystem instance.\n",
    "    '''\n",
    "    stemmed_words = [((\n",
    "                e['analysis'][0]['lex'] \n",
    "                if 'analysis' in e and len(e['analysis']) > 0 else ''\n",
    "             ) \n",
    "             + '_' + \n",
    "             (\n",
    "                re.match('^([A-Z]+)', e['analysis'][0]['gr']).group(0) \n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '')\n",
    "             )\n",
    "             for e in stemmer.analyze(s) if len(e['text'].strip()) > 0]\n",
    "    return [w for w in stemmed_words if w != '_']\n",
    "\n",
    "def stem_eng(s):\n",
    "    ''' Simple string stemming.\n",
    "        Input string is splitted by space and list of basic form of word + part of speech.\n",
    "        Stemmer is nltk.stem.snowball.SnowballStemmer instance.\n",
    "    '''\n",
    "    words = [w for w in re.sub(u'[^a-z]+',' ',s.lower()).split(' ') if len(w)>0]\n",
    "    return [wordbase + '_' + speechpart  for wordbase,speechpart in nltk.pos_tag(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir '/data1/share/kosm/data/text_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-7085de589fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mstemmer_ru\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_stemmed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstem_ru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmer_ru\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstem_eng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'init_url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'title_stemmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/data1/share/kosm/data/text_analysis/stemmed_titles.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2158\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2160\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62187)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-212-7085de589fd4>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mstemmer_ru\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_stemmed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstem_ru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmer_ru\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstem_eng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'init_url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'title_stemmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/data1/share/kosm/data/text_analysis/stemmed_titles.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-192-61223545607c>\u001b[0m in \u001b[0;36mstem_eng\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     30\u001b[0m     '''\n\u001b[0;32m     31\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'[^a-z]+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwordbase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mspeechpart\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mwordbase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspeechpart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mw_td_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_td_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_binget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_binget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_binget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import cPickle\n",
    "\n",
    "stemmer_ru = Mystem()\n",
    "df['title_stemmed'] = df['title'].apply(lambda s: stem_ru(s, stemmer_ru) + stem_eng(s))\n",
    "\n",
    "df[[u'init_url','title_stemmed']].to_csv('/data1/share/kosm/data/text_analysis/stemmed_titles.csv')\n",
    "st_list = list(chain(*stemmed.values.flat))\n",
    "counts = Counter(st_list)\n",
    "word_stat = sorted(counts.items(), key = lambda (word,cnt):-cnt)\n",
    "cPickle.dump(word_stat, open('/data1/share/kosm/data/text_analysis/stemmed_titles.pck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [кто_SPRO, подписывать_V, справка_S, для_PR, б...\n",
       "1     [пэн_S, путешествие_S, в_PR, нетландия_S, кино...\n",
       "2     [белоснежка_S, и_CONJ, охотник_S, смотреть_V, ...\n",
       "3                   [гадание_S, на_PR, карта_S, таро_S]\n",
       "4                                   [dnslookuperror_NN]\n",
       "5         [добро_S, пожаловать_V, в_PR, одноклассник_S]\n",
       "6     [купить_V, смартфон_S, в_PR, москва_S, цена_S,...\n",
       "7              [вконтакте_S, aliexpress_NN, shopper_NN]\n",
       "8     [рыба_S, запекать_V, рецепт_S, фото_S, рецепт_...\n",
       "9                 [ляйсан_S, драчевская_S, вконтакте_S]\n",
       "10    [игил_S, показывать_V, видео_S, казнь_S, шпион...\n",
       "11                                           [embed_NN]\n",
       "12                                  [not_RB, found_VBN]\n",
       "13                  [сплетница_S, район_S, вконтакте_S]\n",
       "14    [тэфи_S, летучий_A, сябитов_S, меньшов_S, и_CO...\n",
       "15                               [стена_S, вконтакте_S]\n",
       "16    [кастинг_S, на_PR, порно_S, категория_S, стран...\n",
       "17    [парень_S, вдвоем_ADV, трахать_V, по_PR, полны...\n",
       "18    [биатлон_S, одиночный_A, смешанный_A, эстафета...\n",
       "19    [порно_S, видео_S, онлайн_ADV, домашний_A, рус...\n",
       "20                   [вконтакте_S, tatiana_NN, miro_NN]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#df.loc[:20,'title'].apply(lambda s: s.lower())\n",
    "#re.sub('[^а-яa-z]',' ',s.lower())\n",
    "\n",
    "text = u'Куплю, новый смартфон * либо поюзанный | nokia   xl dual sim3243 rm- you just selected the prettiest phones with fastest reply rates'\n",
    "\n",
    "#stem_ru(text, stemmer_ru) + stem_eng(text)\n",
    "#re.sub(u'[^a-z]+',' ',text.lower()).split(' ')\n",
    "#print(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'nokia_JJ',\n",
       " u'xl_NNP',\n",
       " u'dual_JJ',\n",
       " u'sim_NN',\n",
       " u'rm_NN',\n",
       " u'you_PRP',\n",
       " u'just_RB',\n",
       " u'selected_VBD',\n",
       " u'the_DT',\n",
       " u'pretty_JJ',\n",
       " u'phones_NNS',\n",
       " u'with_IN',\n",
       " u'fastest_JJS',\n",
       " u'reply_NN',\n",
       " u'rates_NNS']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = text\n",
    "words = [w for w in re.sub(u'[^a-z]+',' ',s.lower()).split(' ') if len(w)>0]\n",
    "[wordbase + '_' + speechpart  for wordbase,speechpart in nltk.pos_tag(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'nokia', 'JJ'),\n",
       " (u'xl', 'NNP'),\n",
       " (u'dual', 'JJ'),\n",
       " (u'sim', 'NN'),\n",
       " (u'rm', 'NN'),\n",
       " (u'you', 'PRP'),\n",
       " (u'just', 'RB'),\n",
       " (u'selected', 'VBD'),\n",
       " (u'the', 'DT'),\n",
       " (u'pretty', 'JJ'),\n",
       " (u'phones', 'NNS'),\n",
       " (u'with', 'IN'),\n",
       " (u'fastest', 'JJS'),\n",
       " (u'reply', 'NN'),\n",
       " (u'rates', 'NNS')]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "    Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(re.sub(u'[^a-z]+',' ',text.lower()))\n",
    "#tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "куплю, новый смартфон * либо поюзанный | nokia   xl dual sim3243 rm-caresses flies dies mules denied died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plot\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "#singles = [stemmer.stem(plural) for plural in plurals]\n",
    "#print(' '.join(singles))  # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "print(stemmer.stem(text + ' '.join(plurals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сливы\tслива\tS\t\n",
      "маринованные\tмариновать\tV\t\n",
      "на\tна\tPR\t\n",
      "зиму\tзима\tS\t\n"
     ]
    }
   ],
   "source": [
    "#import sys, re, pymystem3\n",
    "#mystem = pymystem3.Mystem()\n",
    "#mystem.analyze(df.loc[0,'title'])\n",
    "print('\\n'.join('\\t'.join(e) for e in stem(df.loc[103,'title'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Куплю, новый смартфон * либо поюзанный | nokia   xl dual sim3243 rm-\n",
      "купить_V\n",
      "новый_A\n",
      "смартфон_S\n",
      "либо_CONJ\n",
      "поюзать_V\n"
     ]
    }
   ],
   "source": [
    "def stem(s,stemmer):\n",
    "    stemmed_words = [((\n",
    "                e['analysis'][0]['lex'] \n",
    "                if 'analysis' in e and len(e['analysis']) > 0 else ''\n",
    "             ) \n",
    "             + '_' + \n",
    "             (\n",
    "                re.match('^([A-Z]+)', e['analysis'][0]['gr']).group(0) \n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '')\n",
    "             )\n",
    "             for e in stemmer.analyze(s) if len(e['text'].strip()) > 0]\n",
    "    return [w for w in stemmed_words if w != '_']\n",
    "print(text)\n",
    "print('\\n'.join(stem(text,m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem\n",
    "\n",
    ">>> text = \"Красивая мама красиво мыла раму\"\n",
    ">>> m = Mystem()\n",
    ">>> lemmas = m.lemmatize(text)\n",
    "\n",
    ">>> print \"lemmas:\", ''.join(lemmas)\n",
    ">>> print \"full info:\", json.dumps(m.analyze(text), ensure_ascii=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-c77910493b1d>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-60-c77910493b1d>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print('Ошибка стемминга', file=sys.stderr)\u001b[0m\n\u001b[1;37m                                              \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys, re, pymystem3\n",
    "\n",
    "mystem = pymystem3.Mystem()\n",
    "\n",
    "def stem(s):\n",
    "    return [(e['text'].strip(), \\\n",
    "             e['analysis'][0]['lex'] \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '', \\\n",
    "             re.match('^([A-Z]+)', e['analysis'][0]['gr']).group(0) \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '', \\\n",
    "             ','.join(set(re.findall(r\"[\\w']+\", e['analysis'][0]['gr'])[1:])) \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '')\\\n",
    "             for e in mystem.analyze(s) if len(e['text'].strip()) > 0]\n",
    "\n",
    "line_buf = []\n",
    "word_buf = []\n",
    "\n",
    "def stemAndPrint():    \n",
    "    global word_buf, line_buf\n",
    "    res = stem(' '.join(word_buf))\n",
    "    for i, e in enumerate(res):\n",
    "        if e[0] != word_buf[i]:\n",
    "            print('Ошибка стемминга', file=sys.stderr)\n",
    "            print(word_buf, file=sys.stderr)\n",
    "            print(res, file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "        print(line_buf[i] + '\\t' + '\\t'.join(e[1:]))\n",
    "    line_buf = []\n",
    "    word_buf = []\n",
    "    \n",
    "c = 0\n",
    "header = None    \n",
    "prev_dp_id = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    c += 1\n",
    "    line = line.strip()\n",
    "    if c == 1: #header\n",
    "        header = line.split('\\t')\n",
    "        print(line + '\\tlem_word\\tpos\\tadd_morph')\n",
    "        continue\n",
    "    fields = line.split('\\t')\n",
    "    if len(fields) != len(header):\n",
    "        #TODO exception?\n",
    "        continue\n",
    "    r = dict()\n",
    "    for i, f in enumerate(fields):\n",
    "        r[header[i]] = f\n",
    "        \n",
    "    dp_id = '%s-%s' % (r['dialog_id'], r['phrase_id'])\n",
    "    if prev_dp_id != dp_id:\n",
    "        if prev_dp_id is not None:\n",
    "            stemAndPrint()\n",
    "        prev_dp_id = dp_id\n",
    "    line_buf.append(line)\n",
    "    word_buf.append(r['word'])\n",
    "    \n",
    "if prev_dp_id is not None:\n",
    "    stemAndPrint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.925\n"
     ]
    }
   ],
   "source": [
    "pi = [1,0,0,0]\n",
    "q = [3,2,1,0]\n",
    "eps = 0.05\n",
    "m = 4\n",
    "print(eps/4 * sum(q) + (1-eps)*max(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(pi,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
