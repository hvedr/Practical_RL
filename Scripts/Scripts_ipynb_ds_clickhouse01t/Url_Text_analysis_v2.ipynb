{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ текстов интернет-страниц\n",
    "### Задача DMP-3643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Config\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 4)\n",
    "        .set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "        .set('spark.driver.memory','8g')\n",
    "        .set(\"spark.executor.memory\", '4g')\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", 1048)\n",
    "        .set(\"spark.akka.frameSize\", '1024')        \n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! mv /data1/share/kosm/data/data_all.csv /data1/share/kosm/data/parsed_urls_cred_scor.csv\n",
    "#df = pd.read_csv('/data1/share/kosm/data/parsed_urls_cred_scor.csv',sep = '\\t',skiprows=2,index_col=0)\n",
    "#for l in open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r'):\n",
    "first_row = open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r').readline()\n",
    "df = pd.DataFrame([e.split('\\t')[1:] for e in open('/data1/share/kosm/data/parsed_urls_cred_scor.csv','r') if not e == first_row])\n",
    "df.columns ='url init_url response_code title body'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub(u'[^а-яa-z]+',' ',s.lower())\n",
    "\n",
    "def stem_ru(s, stemmer):\n",
    "    ''' Simple string stemming.\n",
    "        Input string is splitted by space and list of basic form of word + part of speech.\n",
    "        Stemmer is pymystem3.Mystem instance.\n",
    "    '''\n",
    "    stemmed_words = [((\n",
    "                e['analysis'][0]['lex'] \n",
    "                if 'analysis' in e and len(e['analysis']) > 0 else ''\n",
    "             ) \n",
    "             + '_' + \n",
    "             (\n",
    "                re.match('^([A-Z]+)', e['analysis'][0]['gr']).group(0) \n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '')\n",
    "             )\n",
    "             for e in stemmer.analyze(s) if len(e['text'].strip()) > 0]\n",
    "    return [w for w in stemmed_words if w != '_']\n",
    "\n",
    "def stem_eng(s):\n",
    "    ''' Simple string stemming.\n",
    "        Input string is splitted by space and list of basic form of word + part of speech.\n",
    "    '''\n",
    "    words = [w for w in re.sub(u'[^a-z]+',' ',s.lower()).split(' ') if len(w)>0]\n",
    "    return [wordbase + '_' + speechpart  for wordbase,speechpart in nltk.pos_tag(words)]\n",
    "\n",
    "def lemmatize_eng(s, stemmer):\n",
    "    ''' Simple English string lemmatizing.\n",
    "        Input string is splitted by space and list of basic form of word .\n",
    "        Stemmer is nltk.stem.snowball.SnowballStemmer instance.\n",
    "    '''\n",
    "    words = [w for w in re.sub(u'[^a-z]+',' ',s.lower()).split(' ') if len(w)>0]\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import cPickle\n",
    "\n",
    "stemmer_eng = SnowballStemmer(\"english\")\n",
    "stemmer_ru = Mystem()\n",
    "df['title_stemmed'] = df['title'].apply(lambda s: stem_ru(s, stemmer_ru) + lemmatize_eng(s, stemmer_eng))\n",
    "\n",
    "df[[u'init_url','title_stemmed']].to_csv('/data1/share/kosm/data/text_analysis/stemmed_titles.csv')\n",
    "print('titles stemmed')\n",
    "st_list = list(chain(*df['title_stemmed'].values.flat))\n",
    "counts = Counter(st_list)\n",
    "word_stat = sorted(counts.items(), key = lambda (word,cnt):-cnt)\n",
    "cPickle.dump(word_stat, open('/data1/share/kosm/data/text_analysis/stemmed_titles.pck','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print('\\n'.join(u'{}:{}'.format(*e) for e in word_stat[:300]))\n",
    "len([w for w in word_stat if w[1]>1]) # слова встречающиеся больше раза 113504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pymystem3 part of speech\n",
    "A\tприлагательное\n",
    "ADV\tнаречие\n",
    "ADVPRO\tместоименное наречие\n",
    "ANUM\tчислительное-прилагательное\n",
    "APRO\tместоимение-прилагательное\n",
    "COM\tчасть композита - сложного слова\n",
    "CONJ\tсоюз\n",
    "INTJ\tмеждометие\n",
    "NUM\tчислительное\n",
    "PART\tчастица\n",
    "PR\tпредлог\n",
    "S\tсуществительное\n",
    "SPRO\tместоимение-существительное\n",
    "V\tглагол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts_of_speech_to_remove = ['CONJ','PART','PR']\n",
    "words_to_remove = set([w[0] for w in word_stat if w[1] <= 5] + ['not','ru','the','dnslookuperror','timeouterror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('\\n'.join([u'{}:{}'.format(*w) for w in word_stat if '_PR' in w[0]][:100]))\n",
    "df['title_bow'] = df['title_stemmed'].map( lambda stem_list:\n",
    "        [s for s in stem_list if all(not p in s for p in parts_of_speech_to_remove) & (not s in words_to_remove) & (len(s) > 1)]\n",
    ")\n",
    "df.ix[:,'url_bow'] = df['init_url'].map(lambda s: [w for w in re.sub(u'[^a-z]+',' ',str(s).lower()).split(' ') if len(w)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.ix[:,['init_url','title_bow','url_bow']].to_csv('/data1/share/kosm/data/text_analysis/cred_scor_url_titles.csv',index = False)\n",
    "#print(df.iloc[3,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_spark1 = hc.createDataFrame(df.ix[:,['init_url','url_bow']])\n",
    "df_spark2 = hc.createDataFrame(df.ix[:,['init_url','title_bow']])\n",
    "hc.registerDataFrameAsTable(df_spark1, \"df_spark1\")\n",
    "hc.registerDataFrameAsTable(df_spark2, \"df_spark2\")\n",
    "hc.sql('create table user_kposminin.cred_scor_url_titles4 as select * from df_spark1')\n",
    "hc.sql('create table user_kposminin.cred_scor_url_titles5 as select * from df_spark2')\n",
    "hc.sql('''\n",
    "  create table user_kposminin.cred_scor_url_titles6 as \n",
    "  select a.init_url as url,a.url_bow,b.title_bow from \n",
    "   user_kposminin.cred_scor_url_titles4 a\n",
    "   inner join user_kposminin.cred_scor_url_titles5 b on a.init_url = b.init_url\n",
    "   ''')\n",
    "\n",
    "ya_queries = '''\n",
    "create table user_kposminin.cred_scor_url_titles7 as\n",
    "select \n",
    "  init_url,\n",
    "  split(regexp_replace(title_bow,'(u\\')|(\\')|\"|\\\\[|\\\\]| ',''),',') as title_bow\n",
    "from user_kposminin.cred_scor_url_titles5\n",
    ";\n",
    "\n",
    "drop table user_kposminin.cred_scor_url_titles6;\n",
    "  create table user_kposminin.cred_scor_url_titles6 as \n",
    "  select a.init_url as url,a.url_bow,b.title_bow from \n",
    "   user_kposminin.cred_scor_url_titles4 a\n",
    "   inner join user_kposminin.cred_scor_url_titles7 b on a.init_url = b.init_url\n",
    ";\n",
    "\n",
    "create temporary function trunc as 'brickhouse.udf.collect.TruncateArrayUDF';\n",
    "\n",
    "create table user_kposminin.cred_scor_url_titles9 as \n",
    "select url, trunc(bow,least(size(bow),100)) as bow\n",
    "from user_kposminin.cred_scor_url_titles8   \n",
    ";\n",
    "'''\n",
    "# В хдфс не хочет сохраняться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Сохраняем локально\n",
    "#df.ix[:,['init_url','title_bow','url_bow']].to_csv('/data1/share/kosm/data/text_analysis/cred_scor_url_titles.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
