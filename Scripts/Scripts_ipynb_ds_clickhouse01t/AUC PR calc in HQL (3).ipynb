{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение auc precision-recall curve запросом в hql. \n",
    "### Построение запроса сравнения коэффициентов урлфрагментов в задаче lookalike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np\n",
    "import sklearn,sklearn.metrics\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, NaiveBayes, NaiveBayesModel\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "hive_config_query = '''\n",
    "set hive.vectorized.execution.enabled=true;\n",
    "set hive.vectorized.execution.reduce.enabled = true;\n",
    "set mapreduce.map.memory.mb=4096;\n",
    "set mapreduce.map.child.java.opts=-Xmx4g;\n",
    "set mapreduce.task.io.sort.mb=1024;\n",
    "set mapreduce.reduce.child.java.opts=-Xmx4g;\n",
    "set mapreduce.reduce.memory.mb=7000;\n",
    "set mapreduce.reduce.shuffle.input.buffer.percent=0.5;\n",
    "set mapreduce.input.fileinputformat.split.minsize=536870912;\n",
    "set mapreduce.input.fileinputformat.split.maxsize=1073741824;\n",
    "set hive.optimize.ppd=true;\n",
    "set hive.merge.smallfiles.avgsize=536870912;\n",
    "set hive.merge.mapredfiles=true;\n",
    "set hive.merge.mapfiles=true;\n",
    "set hive.hadoop.supports.splittable.combineinputformat=true;\n",
    "set hive.exec.reducers.bytes.per.reducer=536870912;\n",
    "set hive.exec.parallel=true;\n",
    "set hive.exec.max.created.files=10000000;\n",
    "set hive.exec.compress.output=true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "set hive.exec.max.dynamic.partitions=1000000;\n",
    "set hive.exec.max.dynamic.partitions.pernode=100000;\n",
    "set io.seqfile.compression.type=BLOCK;\n",
    "set mapreduce.map.failures.maxpercent=5;\n",
    "'''\n",
    "try:\n",
    "    sc.stop()\n",
    "except: pass\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.executor.instances\", 2)\n",
    "        .set(\"spark.driver.maxResultSize\", \"16g\")\n",
    "        .set('spark.driver.memory','16g')\n",
    "       # .set(\"spark.executor.memory\", '8g')\n",
    "       # .set(\"spark.yarn.executor.memoryOverhead\", 2048)        \n",
    "       )\n",
    "sc = SparkContext(conf=conf)\n",
    "hc = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true = np.random.choice(2,size = 100000,p=[0.9,0.1])\n",
    "y_score = y_true + np.random.randn(y_true.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартный AUC ROC, AUC PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auc_roc = sklearn.metrics.roc_auc_score(y_true = y_true, y_score = y_score)\n",
    "auc_pr_wrong = sklearn.metrics.auc(\n",
    "                        *sklearn.metrics.precision_recall_curve(y_true = y_true, probas_pred  = y_score)[:2],\n",
    "                        reorder = True)\n",
    "auc_pr = sklearn.metrics.average_precision_score(y_true = y_true, y_score = y_score)\n",
    "logloss = sklearn.metrics.log_loss(y_true = y_true, y_pred = y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение AUC PR вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_sorted = [e[1] for e in sorted(zip(y_score,y_true),reverse = True)]\n",
    "precision = []\n",
    "pos = 0\n",
    "n = 0\n",
    "for e in y_sorted:\n",
    "    n += 1\n",
    "    if e == 1:\n",
    "        pos += 1\n",
    "        precision.append(float(pos)/n)\n",
    "auc_pr_manual = sum(precision)/len(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2869752348023623, 0.2869016252829576)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_pr_manual, auc_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет AUC PR вручную и стандартной библиотекой дают ~одинаковый результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sc.parallelize(zip([int(e) for e in y_true],[float(e) for e in y_score])).toDF(['label','score'])\n",
    "hc.registerDataFrameAsTable(df, 'label_score_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "query2 = '''\n",
    "with\n",
    "st1 as (select * from label_score_data a),\n",
    "cs1 as (\n",
    "  select \n",
    "    (1-label)*sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision\n",
    "  from st1),\n",
    "cs3 as (select sum(label) as cnt_positive from st1)\n",
    "select \n",
    "  'First' as name, \n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/max(cnt_positive) as auc_pr\n",
    "from cs1 a inner join cs3 b\n",
    "'''\n",
    "\n",
    "calc_metrics_query = '''\n",
    "with\n",
    "st1 as (select * from label_score_data a),\n",
    "cs1 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label\n",
    "  from st1)\n",
    "select \n",
    "  'First' as name, \n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr\n",
    "from cs1 a \n",
    "'''\n",
    "\n",
    "calc_metrics_query_new = '''\n",
    "with\n",
    "st1 as (select * from label_score_data a),\n",
    "cs1 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
    "    rank() OVER (ORDER BY score DESC) as rank,\n",
    "    label\n",
    "  from st1)\n",
    "select \n",
    "  'First' as name, \n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr,\n",
    "  avg(logloss) as logloss,\n",
    "  sum(if(rank < 1000,label,0))/1000 *count(*)/ sum(label) as lift_1k,\n",
    "  sum(if(rank < 20000,label,0))/20000 *count(*)/ sum(label) as lift_20k,\n",
    "  sum(label)/count(*) as pos_share\n",
    "from cs1 a \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+----------------+------------------+---------+\n",
      "| name|           auc_roc|            auc_pr|           logloss|         lift_1k|          lift_20k|pos_share|\n",
      "+-----+------------------+------------------+------------------+----------------+------------------+---------+\n",
      "|First|0.7565796607120473|0.2869752348023623|0.7654592867044155|5.37529319781079|2.4970680218921033|  0.10232|\n",
      "+-----+------------------+------------------+------------------+----------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc.sql(calc_metrics_query_new).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7565796607120473, 0.2869752348023623, 5.7761216106481195)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_roc,auc_pr_manual,logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Успех"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_query = '''\n",
    "with p as \n",
    " (\n",
    "  select \n",
    "    v.id, \n",
    "    max(if(u.id is Null,0,1)) as label, \n",
    "    max(t1.score) as score1, \n",
    "    max(t2.score) as score2 \n",
    "  from\n",
    "   (select id, url_fragment as urlfr from prod_odd.visit_feature where ymd = '2017-02-28') v\n",
    "   left join \n",
    "    (\n",
    "      select id\n",
    "      from prod_features_liveinternet.user_action \n",
    "      where ymd between '#ymd' and date_add('#ymd',3)\n",
    "      and action_type = 'tinkoff_platinum_approved_application'\n",
    "    ) u on u.id = v.id\n",
    "   left join prod_lookalike.urlfr_coeff t1 on t1.urlfr = v.urlfr and t1.segment_nm = 'la_apppr_ccall_2'\n",
    "   left join\n",
    "    (\n",
    "    select\n",
    "      urlfr,\n",
    "      score\n",
    "    from\n",
    "      prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
    "    where\n",
    "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "      and ymd = '2016-12-26'\n",
    "      and (cnt_total > 300000 or cnt_positive > 10)\n",
    "    ) t2 on t2.urlfr = v.urlfr \n",
    "   group by\n",
    "    v.id\n",
    " ),\n",
    "st1 as (select a.score1 as score, a.label as label from p a),\n",
    "st2 as (select a.score2 as score, a.label as label from p a),\n",
    "cs1 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
    "    rank() OVER (ORDER BY score DESC) as rank,\n",
    "    label\n",
    "  from st1),\n",
    "cs2 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
    "    rank() OVER (ORDER BY score DESC) as rank,\n",
    "    label\n",
    "  from st2)\n",
    "  \n",
    "select \n",
    "  'Old coeffs' as name, \n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr,\n",
    "  avg(logloss) as logloss,\n",
    "  sum(if(rank < 1000,label,0))/1000 *count(*)/ sum(label) as lift_1k,\n",
    "  sum(if(rank < 20000,label,0))/20000 *count(*)/ sum(label) as lift_20k,\n",
    "  sum(label)/count(*) as pos_share\n",
    "from cs1\n",
    "\n",
    "union all\n",
    "\n",
    "select \n",
    "  'New coeffs' as name, \n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr,\n",
    "  avg(logloss) as logloss,\n",
    "  sum(if(rank < 1000,label,0))/1000 *count(*)/ sum(label) as lift_1k,\n",
    "  sum(if(rank < 20000,label,0))/20000 *count(*)/ sum(label) as lift_20k,\n",
    "  sum(label)/count(*) as pos_share\n",
    "from cs2 a \n",
    ";\n",
    "\n",
    "'''\n",
    "\n",
    "compare_query1 = '''\n",
    "-- la_apppr_ccall2. Url coefs comparison\n",
    "with \n",
    "mymd_t as\n",
    " (\n",
    " select \n",
    "   max(ymd) as max_ymd\n",
    " from \n",
    "   prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
    " where\n",
    "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "      and ymd < date_add('#ymd',-3)\n",
    " ),\n",
    "p as \n",
    " (\n",
    "  select \n",
    "    v.id, \n",
    "    max(if(u.id is Null,0,1)) as label, \n",
    "    max(t1.score) as score1, \n",
    "    max(t2.score) as score2 \n",
    "  from\n",
    "   (select id, url_fragment as urlfr from prod_odd.visit_feature where ymd = '#ymd') v\n",
    "   left join \n",
    "    (\n",
    "      select id\n",
    "      from prod_features_liveinternet.user_action \n",
    "      where ymd between '#ymd' and date_add('#ymd',3)\n",
    "      and action_type = 'tinkoff_platinum_approved_application'\n",
    "    ) u on u.id = v.id\n",
    "   left join prod_lookalike.urlfr_coeff t1 on t1.urlfr = v.urlfr and t1.segment_nm = 'la_apppr_ccall_2'\n",
    "   left join\n",
    "    (\n",
    "    select\n",
    "      urlfr,\n",
    "      score\n",
    "    from\n",
    "      mymd_t my      \n",
    "      inner join prod_features_liveinternet.urlfr_tgt_cnt_cumulative2 t on t.ymd = my.max_ymd\n",
    "    where\n",
    "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "      and (cnt_total > 300000 or cnt_positive > 10)\n",
    "    ) t2 on t2.urlfr = v.urlfr \n",
    "   group by\n",
    "    v.id\n",
    " ),\n",
    "st1 as (select a.score1 as score, a.label as label from p a),\n",
    "st2 as (select a.score2 as score, a.label as label from p a),\n",
    "cs1 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
    "    rank() OVER (ORDER BY score DESC) as rank,\n",
    "    label\n",
    "  from st1),\n",
    "cs2 as (\n",
    "  select \n",
    "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
    "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
    "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
    "    rank() OVER (ORDER BY score DESC) as rank,\n",
    "    label\n",
    "  from st2)\n",
    "  \n",
    "select \n",
    "  'Old coeffs' as name,\n",
    "  '#ymd' as test_ymd,\n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr,\n",
    "  avg(logloss) as logloss,\n",
    "  sum(if(rank < 10000,label,0))/10000 *count(*)/ sum(label) as lift_10k,\n",
    "  sum(if(rank < 50000,label,0))/50000 *count(*)/ sum(label) as lift_50k,\n",
    "  sum(label)/count(*) as pos_share,\n",
    "  count(*) as cnt\n",
    "from cs1\n",
    "\n",
    "union all\n",
    "\n",
    "select \n",
    "  concat('New coeffs ',max(my.max_ymd)) as name, \n",
    "  '#ymd' as test_ymd,\n",
    "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
    "  sum(precision)/sum(label) as auc_pr,\n",
    "  avg(logloss) as logloss,\n",
    "  sum(if(rank < 10000,label,0))/10000 *count(*)/ sum(label) as lift_10k,\n",
    "  sum(if(rank < 50000,label,0))/50000 *count(*)/ sum(label) as lift_50k,\n",
    "  sum(label)/count(*) as pos_share,\n",
    "  count(*) as cnt\n",
    "from cs2 a \n",
    "inner join mymd_t my\n",
    ";\n",
    "\n",
    "'''\n",
    "\n",
    "update_query = '''\n",
    "-- la_apppr_ccall2. Url coefs update\n",
    "with \n",
    "mymd_t as\n",
    " (\n",
    " select \n",
    "   max(ymd) as max_ymd\n",
    " from \n",
    "   prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
    " where\n",
    "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "      and ymd < date_add('#ymd',-3)\n",
    " )\n",
    "\n",
    "insert overwrite table prod_lookalike.urlfr_coeff partition (segment_nm = 'la_apppr_ccall_2')\n",
    "select\n",
    "      urlfr,\n",
    "      score,\n",
    "      current_timestamp() as load_dttm\n",
    "from\n",
    "      mymd_t my      \n",
    "      inner join prod_features_liveinternet.urlfr_tgt_cnt_cumulative2 t on t.ymd = my.max_ymd\n",
    "where\n",
    "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
    "      and (cnt_total > 300000 or cnt_positive > 10)\n",
    ";\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- la_apppr_ccall2. Url coefs comparison\n",
      "with \n",
      "mymd_t as\n",
      " (\n",
      " select \n",
      "   max(ymd) as max_ymd\n",
      " from \n",
      "   prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
      " where\n",
      "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
      "      and ymd < date_add('2017-04-11',-3)\n",
      " ),\n",
      "p as \n",
      " (\n",
      "  select \n",
      "    v.id, \n",
      "    max(if(u.id is Null,0,1)) as label, \n",
      "    max(t1.score) as score1, \n",
      "    max(t2.score) as score2 \n",
      "  from\n",
      "   (select id, url_fragment as urlfr from prod_odd.visit_feature where ymd = '2017-04-11') v\n",
      "   left join \n",
      "    (\n",
      "      select id\n",
      "      from prod_features_liveinternet.user_action \n",
      "      where ymd between '2017-04-11' and date_add('2017-04-11',3)\n",
      "      and action_type = 'tinkoff_platinum_approved_application'\n",
      "    ) u on u.id = v.id\n",
      "   left join prod_lookalike.urlfr_coeff t1 on t1.urlfr = v.urlfr and t1.segment_nm = 'la_apppr_ccall_2'\n",
      "   left join\n",
      "    (\n",
      "    select\n",
      "      urlfr,\n",
      "      score\n",
      "    from\n",
      "      mymd_t my      \n",
      "      inner join prod_features_liveinternet.urlfr_tgt_cnt_cumulative2 t on t.ymd = my.max_ymd\n",
      "    where\n",
      "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
      "      and (cnt_total > 300000 or cnt_positive > 10)\n",
      "    ) t2 on t2.urlfr = v.urlfr \n",
      "   group by\n",
      "    v.id\n",
      " ),\n",
      "st1 as (select a.score1 as score, a.label as label from p a),\n",
      "st2 as (select a.score2 as score, a.label as label from p a),\n",
      "cs1 as (\n",
      "  select \n",
      "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
      "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
      "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
      "    rank() OVER (ORDER BY score DESC) as rank,\n",
      "    label\n",
      "  from st1),\n",
      "cs2 as (\n",
      "  select \n",
      "    (1-label) * sum(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as sl,\n",
      "    label * avg(label) OVER (ORDER BY score DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as precision,\n",
      "    label * log(1 + exp(-score)) + (1 - label) * log(1 + exp(score)) as logloss,\n",
      "    rank() OVER (ORDER BY score DESC) as rank,\n",
      "    label\n",
      "  from st2)\n",
      "  \n",
      "select \n",
      "  'Old coeffs' as name,\n",
      "  '2017-04-11' as test_ymd,\n",
      "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
      "  sum(precision)/sum(label) as auc_pr,\n",
      "  avg(logloss) as logloss,\n",
      "  sum(if(rank < 10000,label,0))/10000 *count(*)/ sum(label) as lift_10k,\n",
      "  sum(if(rank < 50000,label,0))/50000 *count(*)/ sum(label) as lift_50k,\n",
      "  sum(label)/count(*) as pos_share,\n",
      "  count(*) as cnt\n",
      "from cs1\n",
      "\n",
      "union all\n",
      "\n",
      "select \n",
      "  concat('New coeffs ',my.max(max_ymd)) as name, \n",
      "  '2017-04-11' as test_ymd,\n",
      "  sum(sl)*1.0/((count(*)-max(sl))*max(sl)) as auc_roc,\n",
      "  sum(precision)/sum(label) as auc_pr,\n",
      "  avg(logloss) as logloss,\n",
      "  sum(if(rank < 10000,label,0))/10000 *count(*)/ sum(label) as lift_10k,\n",
      "  sum(if(rank < 50000,label,0))/50000 *count(*)/ sum(label) as lift_50k,\n",
      "  sum(label)/count(*) as pos_share,\n",
      "  count(*) as cnt\n",
      "from cs2 a \n",
      "inner join mymd_t my\n",
      ";\n",
      "\n",
      "\n",
      "\n",
      "-- la_apppr_ccall2. Url coefs update\n",
      "with \n",
      "mymd_t as\n",
      " (\n",
      " select \n",
      "   max(ymd) as max_ymd\n",
      " from \n",
      "   prod_features_liveinternet.urlfr_tgt_cnt_cumulative2\n",
      " where\n",
      "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
      "      and ymd < date_add('2017-04-11',-3)\n",
      " )\n",
      "\n",
      "insert overwrite table prod_lookalike.urlfr_coeff partition (segment_nm = 'la_apppr_ccall_2')\n",
      "select\n",
      "      urlfr,\n",
      "      score,\n",
      "      current_timestamp() as load_dttm\n",
      "from\n",
      "      mymd_t my      \n",
      "      inner join prod_features_liveinternet.urlfr_tgt_cnt_cumulative2 t on t.ymd = my.max_ymd\n",
      "where\n",
      "      target = 'tinkoff_platinum_approved_application03@tinkoff_action'\n",
      "      and (cnt_total > 300000 or cnt_positive > 10)\n",
      ";\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ymd = (datetime.datetime.now().date() - datetime.timedelta(days = 7)).strftime('%Y-%m-%d')\n",
    "print(compare_query1.replace('#ymd', ymd))\n",
    "\n",
    "update = True\n",
    "if(update):\n",
    "    print(update_query.replace('#ymd', ymd))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_check_table_query = '''\n",
    "\n",
    "-- Проверка скрипта сравнения списков урл-коэффициентов\n",
    "-- la_apppr_ccall2. Url coefs comparison\n",
    "create table user_kposminin.urlfr_coef_comparison_scipt_check_tmp as\n",
    "  select \n",
    "    v.id, \n",
    "    max(if(u.id is Null,0,1)) as label, \n",
    "    max(t1.score) as score1, \n",
    "    max(t2.score) as score2 \n",
    "  from\n",
    "   (select id, url_fragment as urlfr from prod_odd.visit_feature where ymd = '2017-03-14') v\n",
    "   left join \n",
    "    (\n",
    "      select id\n",
    "      from prod_features_liveinternet.user_action \n",
    "      where ymd between '2017-03-14' and date_add('2017-03-14',3)\n",
    "      and action_type = 'tinkoff_platinum_approved_application'\n",
    "    ) u on u.id = v.id\n",
    "   left join prod_lookalike.urlfr_coeff t1 on t1.urlfr = v.urlfr and t1.segment_nm = 'la_apppr_ccall_2'\n",
    "   left join user_kposminin.urlfr_tgt_cnt t2 on t2.urlfr = v.urlfr \n",
    "   where\n",
    "     t2.ymd = '2017-02-05'\n",
    "     and t2.target = 'tinkoff_platinum_approved_application03@cumul_4months'\n",
    "   group by\n",
    "    v.id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://ds-hadoop-cs01p:10000/\" -n kposminin --incremental=true --showheader=false --outputformat=tsv2 --maxwidth=5000 --silent=true --showWarnings=false --showNestedErrs=false --verbose=false --nullemptystring=true -f /home/k.osminin/scripts/la_many_feat_20161213.hql > /data1/share/kosm/data/la_many_feat_20161103.txt\n",
    "select label,score1,score2 from user_kposminin.urlfr_coef_comparison_scipt_check_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o59.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply$mcI$sp(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.collectToPython(DataFrame.scala:1777)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cb4f7cfba9e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'select label,score1,score2 from user_kposminin.urlfr_coef_comparison_scipt_check_tmp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/k.osminin/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o59.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply$mcI$sp(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collectToPython$1.apply(DataFrame.scala:1778)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.collectToPython(DataFrame.scala:1777)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "data = hc.sql('select label,score1,score2 from user_kposminin.urlfr_coef_comparison_scipt_check_tmp').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics(y_true,y_score,lift = None, return_str = False):\n",
    "    import sklearn\n",
    "    import collections\n",
    "    \n",
    "    if True:\n",
    "        \n",
    "        res = collections.OrderedDict()\n",
    "        samp_size = len(y_true)\n",
    "        res['Sample size'] = samp_size\n",
    "        res['Posit share'] = sum(y_true) * 1./ samp_size\n",
    "        res['Sample size'] = len(y_true)\n",
    "        res['AUC ROC'] = sklearn.metrics.roc_auc_score(y_true = y_true, y_score = y_score)\n",
    "        res['AUC PR'] = sklearn.metrics.average_precision_score( y_true,  y_score)\n",
    "        res['Log loss'] = sklearn.metrics.log_loss(y_true = y_true, y_pred = y_score)\n",
    "        if lift:\n",
    "            predictions_and_labels = sorted(zip(y_score,y_true), key = lambda e:-e[0])\n",
    "            for l in lift:\n",
    "                res['Lift ' + str(l)] = sum([e[1] for e in predictions_and_labels[:int(l * samp_size)]]) * 1. / int(l * samp_size) / res['Posit share']                \n",
    "        if return_str:\n",
    "            res = '\\n'.join(['{:<12}: {:.5f}'.format(k,v) for (k,v) in res.items()]) + '.'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics(df['label'],df['score1'],lift = [10.**4/236.4/10**6,5*10.**4/236.4/10**6],return_str = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics(df['label'],df['score2'],lift = [10.**4/236.4/10**6,5*10.**4/236.4/10**6],return_str = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
