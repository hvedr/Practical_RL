[I 17:50:53.160 NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js
[W 17:50:53.201 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[W 17:50:53.201 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.
[I 17:50:53.243 NotebookApp] Serving notebooks from local directory: /home/k.p.osminin/scripts
[I 17:50:53.244 NotebookApp] 0 active kernels 
[I 17:50:53.244 NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:9990/
[I 17:50:53.245 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).

****************************************
2017-04-25 14:43:22.084344: Update of user_action started.
Calc user_action_new for 2017-02-01 - 2017-02-29
2017-04-25 15:30:23.506648: User_action successfully updated. Dates: 2017-02-01---2017-02-29

****************************************
2017-04-25 14:44:14.937218: Update of user_action started.
Calc user_action_new for 2017-04-01 - 2017-04-23
2017-04-25 15:31:27.063927: User_action successfully updated. Dates: 2017-04-01---2017-04-23

****************************************
2017-04-25 14:43:52.288127: Update of user_action started.
Calc user_action_new for 2017-01-01 - 2017-01-31
2017-04-25 15:32:18.240701: User_action successfully updated. Dates: 2017-01-01---2017-01-31

****************************************
2017-04-25 20:05:24.955669: Update of user_action started.
Calc user_action_new for 2016-11-01 - 2016-11-31
2017-04-25 20:26:29.660903: User_action successfully updated. Dates: 2016-11-01---2016-11-31

****************************************
2017-04-25 20:05:09.756503: Update of user_action started.
Calc user_action_new for 2016-12-01 - 2016-12-31
2017-04-25 20:26:30.965185: User_action successfully updated. Dates: 2016-12-01---2016-12-31
2017-04-27 11:33:50.802896: Calc cred scoring weblog.
2017-04-27 11:33:51.127551. Executing:


set mapred.job.queue.name=bigdata_long
2017-04-27 11:33:51.129411. Executing:


-- Факторы по кукам. 2017-02-15
create table user_kposminin.id_feat_1d_20170215 as
with mymd as 
 (select
   target,
   max(ymd) as max_ymd
  from
   user_kposminin.urlfr_tgt_cnt
  where 
    ymd < date_add('2017-02-15',-3)
  group by target
 )

select 
 id,
 load_src, 
 count(distinct urlfr) as cnt,
 count(*) as visits_cnt,
 sum(cnt) as hits,
 sum(if(urlfr like 'e.mail.ru%',1,0)) as emailru,
 sum(if(urlfr like 'm.%',1,0))/sum(1) as mobile_share,
 sum(if(urlfr rlike '^(m\.)?vk.com%', 1, 0))/sum(1) as vk_share,
 sum(if(urlfr like 'vk.com%' or urlfr rlike '^(m\.)?ok\.ru' or urlfr like 'm.odnoklassniki.ru%' or urlfr rlike '^(m\.)?my.mail.ru',1,0))/sum(1) as social_share,
 sum(if(avg_hour >= 9 and avg_hour <= 20,cnt,0))/sum(cnt) as work_hours_hits_share,
 sum( avg_hour * avg_hour) as avg_hour_sum_sq,
 sum(avg_hour) as avg_hour_sum,
 max(score1) as max_score1,
 max(score2) as max_score2,
 max(score3) as max_score3,
 max(score4) as max_score4,
 min(score1) as min_score1,
 min(score2) as min_score2,
 min(score3) as min_score3,
 min(score4) as min_score4,
 sum(score1) as sum_score1,
 sum(score2) as sum_score2,
 sum(score3) as sum_score3,
 sum(score4) as sum_score4, 
 count( if(score1 > 1, urlfr,Null))/sum(1) as good_urlfr_share_score1,
 count( if(score2 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score2,
 count( if(score3 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score3,
 ymd
 
from
 (select
    v.id,
    v.load_src,
    v.ymd,
    v.url_fragment as urlfr,
    unix_timestamp(v.ymd, 'yyyy-MM-dd')/60/60 + v.average_visit_hour  as time_h,
    1 as time_std,
    v.visit_count as cnt,
    v.average_visit_hour as avg_hour,
    t1.score as score1,
    t2.score as score2,
    t3.score as score3,
    t4.score as score4
  from
    prod_odd.visit_feature v
    left semi join (select 
       uid_str as id,
       property_value as phone_num,
       load_src
     from
       prod_dds.md_uid_property 
     where
       property_cd = 'PHONE'
    ) m on m.id = v.id and m.load_src = v.load_src  
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'ccall_tinkoff_approve_from_fullapp'
    ) t1 on t1.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_platinum_approved_application03@tinkoff_action' 
    ) t2 on t2.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_platinum_complete_application03@tinkoff_action'
    ) t3 on t3.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_LON_CCR_default'
    ) t4 on t4.urlfr = v.url_fragment
  where 
    v.ymd = '2017-02-15' 
 ) a 
group by
  id,load_src,ymd

Traceback (most recent call last):
  File "cred_scor_feat_table_calc.py", line 11278, in <module>
    cursor.execute(q)
  File "/opt/anaconda/lib/python2.7/site-packages/pyhive/hive.py", line 229, in execute
    req = ttypes.TExecuteStatementReq(self._connection.sessionHandle, sql.encode('utf-8'))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xd4 in position 5: ordinal not in range(128)
2017-04-27 11:36:09.611351: Calc cred scoring weblog.
2017-04-27 11:36:09.834415. Executing:


set mapred.job.queue.name=bigdata_long
2017-04-27 11:36:09.835798. Executing:


-- id_feat 2017-02-15
create table user_kposminin.id_feat_1d_20170215 as
with mymd as 
 (select
   target,
   max(ymd) as max_ymd
  from
   user_kposminin.urlfr_tgt_cnt
  where 
    ymd < date_add('2017-02-15',-3)
  group by target
 )

select 
 id,
 load_src, 
 count(distinct urlfr) as cnt,
 count(*) as visits_cnt,
 sum(cnt) as hits,
 sum(if(urlfr like 'e.mail.ru%',1,0)) as emailru,
 sum(if(urlfr like 'm.%',1,0))/sum(1) as mobile_share,
 sum(if(urlfr rlike '^(m\.)?vk.com%', 1, 0))/sum(1) as vk_share,
 sum(if(urlfr like 'vk.com%' or urlfr rlike '^(m\.)?ok\.ru' or urlfr like 'm.odnoklassniki.ru%' or urlfr rlike '^(m\.)?my.mail.ru',1,0))/sum(1) as social_share,
 sum(if(avg_hour >= 9 and avg_hour <= 20,cnt,0))/sum(cnt) as work_hours_hits_share,
 sum( avg_hour * avg_hour) as avg_hour_sum_sq,
 sum(avg_hour) as avg_hour_sum,
 max(score1) as max_score1,
 max(score2) as max_score2,
 max(score3) as max_score3,
 max(score4) as max_score4,
 min(score1) as min_score1,
 min(score2) as min_score2,
 min(score3) as min_score3,
 min(score4) as min_score4,
 sum(score1) as sum_score1,
 sum(score2) as sum_score2,
 sum(score3) as sum_score3,
 sum(score4) as sum_score4, 
 count( if(score1 > 1, urlfr,Null))/sum(1) as good_urlfr_share_score1,
 count( if(score2 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score2,
 count( if(score3 > -7, urlfr,Null))/sum(1) as good_urlfr_share_score3,
 ymd
 
from
 (select
    v.id,
    v.load_src,
    v.ymd,
    v.url_fragment as urlfr,
    unix_timestamp(v.ymd, 'yyyy-MM-dd')/60/60 + v.average_visit_hour  as time_h,
    1 as time_std,
    v.visit_count as cnt,
    v.average_visit_hour as avg_hour,
    t1.score as score1,
    t2.score as score2,
    t3.score as score3,
    t4.score as score4
  from
    prod_odd.visit_feature v
    left semi join (select 
       uid_str as id,
       property_value as phone_num,
       load_src
     from
       prod_dds.md_uid_property 
     where
       property_cd = 'PHONE'
    ) m on m.id = v.id and m.load_src = v.load_src  
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'ccall_tinkoff_approve_from_fullapp'
    ) t1 on t1.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_platinum_approved_application03@tinkoff_action' 
    ) t2 on t2.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_platinum_complete_application03@tinkoff_action'
    ) t3 on t3.urlfr = v.url_fragment
    left join (
        select urlfr,score
          from mymd td
         inner join user_kposminin.urlfr_scores t on t.target = td.target and t.ymd = td.max_ymd
         where td.target = 'tinkoff_LON_CCR_default'
    ) t4 on t4.urlfr = v.url_fragment
  where 
    v.ymd = '2017-02-15' 
 ) a 
group by
  id,load_src,ymd

Traceback (most recent call last):
  File "cred_scor_feat_table_calc.py", line 11279, in <module>
    cursor.execute(q)
  File "/opt/anaconda/lib/python2.7/site-packages/pyhive/hive.py", line 232, in execute
    _check_status(response)
  File "/opt/anaconda/lib/python2.7/site-packages/pyhive/hive.py", line 288, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table id_feat_1d_20170215 already exists)', sqlState='08S01', infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table id_feat_1d_20170215 already exists):28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:326', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:146', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:173', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:268', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:410', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:391', 'sun.reflect.GeneratedMethodAccessor58:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:497', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1671', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy21:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:245', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:509', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617', 'java.lang.Thread:run:Thread.java:745', '*org.apache.hadoop.hive.ql.metadata.HiveException:AlreadyExistsException(message:Table id_feat_1d_20170215 already exists):37:10', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:752', 'org.apache.hadoop.hive.ql.exec.DDLTask:createTable:DDLTask.java:4023', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:302', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:160', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:88', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:1636', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:1396', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1181', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1047', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1042', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:144', '*org.apache.hadoop.hive.metastore.api.AlreadyExistsException:Table id_feat_1d_20170215 already exists:57:20', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:29879', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:29865', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result:read:ThriftHiveMetastore.java:29791', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:78', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_create_table_with_environment_context:ThriftHiveMetastore.java:1071', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:create_table_with_environment_context:ThriftHiveMetastore.java:1057', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:create_table_with_environment_context:HiveMetaStoreClient.java:2034', 'org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient:create_table_with_environment_context:SessionHiveMetaStoreClient.java:97', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:670', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:658', 'sun.reflect.GeneratedMethodAccessor144:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:497', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:91', 'com.sun.proxy.$Proxy6:createTable::-1', 'sun.reflect.GeneratedMethodAccessor144:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:497', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:1966', 'com.sun.proxy.$Proxy6:createTable::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:749'], statusCode=3), operationHandle=None)

****************************************
2017-04-28 17:58:48.891911: Update of user_action started.
Calc user_action_new for 2016-09-01 - 2016-09-31
2017-04-28 18:17:49.892748: User_action successfully updated. Dates: 2016-09-01---2016-09-31

****************************************
2017-04-28 17:59:04.723698: Update of user_action started.
Calc user_action_new for 2016-06-01 - 2016-06-31
2017-04-28 18:18:46.929464: User_action successfully updated. Dates: 2016-06-01---2016-06-31

****************************************
2017-04-28 17:58:59.804231: Update of user_action started.
Calc user_action_new for 2016-07-01 - 2016-07-31
2017-04-28 18:18:56.567950: User_action successfully updated. Dates: 2016-07-01---2016-07-31

****************************************
2017-04-28 17:58:54.770295: Update of user_action started.
Calc user_action_new for 2016-08-01 - 2016-08-31
2017-04-28 18:18:57.314017: User_action successfully updated. Dates: 2016-08-01---2016-08-31

****************************************
2017-04-28 17:58:40.680247: Update of user_action started.
Calc user_action_new for 2016-10-01 - 2016-10-31
2017-04-28 18:18:58.717035: User_action successfully updated. Dates: 2016-10-01---2016-10-31
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = k.p.osminin'
                           ^
SyntaxError: EOL while scanning string literal
  File "user_action_calc.py", line 7
    HIVE_USER = kposminin'
                         ^
SyntaxError: EOL while scanning string literal

****************************************
2017-04-28 18:36:45.094346: Update of user_action started.
Calc user_action_new for 2016-05-01 - 2016-05-31
Traceback (most recent call last):
  File "user_action_calc.py", line 278, in <module>
    for q in query_list: cursor.execute(q)
  File "/opt/anaconda/lib/python2.7/site-packages/pyhive/hive.py", line 231, in execute
    response = self._connection.client.ExecuteStatement(req)
  File "/opt/anaconda/lib/python2.7/site-packages/TCLIService/TCLIService.py", line 265, in ExecuteStatement
    return self.recv_ExecuteStatement()
  File "/opt/anaconda/lib/python2.7/site-packages/TCLIService/TCLIService.py", line 276, in recv_ExecuteStatement
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File "/opt/anaconda/lib/python2.7/site-packages/thrift/protocol/TBinaryProtocol.py", line 126, in readMessageBegin
    sz = self.readI32()
  File "/opt/anaconda/lib/python2.7/site-packages/thrift/protocol/TBinaryProtocol.py", line 206, in readI32
    buff = self.trans.readAll(4)
  File "/opt/anaconda/lib/python2.7/site-packages/thrift/transport/TTransport.py", line 58, in readAll
    chunk = self.read(sz - have)
  File "/opt/anaconda/lib/python2.7/site-packages/thrift_sasl/__init__.py", line 159, in read
    self._read_frame()
  File "/opt/anaconda/lib/python2.7/site-packages/thrift_sasl/__init__.py", line 163, in _read_frame
    header = read_all_compat(self._trans, 4)
  File "/opt/anaconda/lib/python2.7/site-packages/thrift_sasl/six.py", line 31, in <lambda>
    read_all_compat = lambda trans, sz: trans.readAll(sz)
  File "/opt/anaconda/lib/python2.7/site-packages/thrift/transport/TTransport.py", line 58, in readAll
    chunk = self.read(sz - have)
  File "/opt/anaconda/lib/python2.7/site-packages/thrift/transport/TSocket.py", line 105, in read
    buff = self.handle.recv(sz)
KeyboardInterrupt

****************************************
2017-04-28 18:39:36.093960: Update of user_action started.
Calc user_action_new for 2015-09-01 - 2015-09-31
2017-04-28 19:05:07.803184: User_action successfully updated. Dates: 2015-09-01---2015-09-31

****************************************
2017-04-28 18:39:29.840580: Update of user_action started.
Calc user_action_new for 2015-10-01 - 2015-10-31
2017-04-28 19:05:25.543129: User_action successfully updated. Dates: 2015-10-01---2015-10-31

****************************************
2017-04-28 18:38:40.829094: Update of user_action started.
Calc user_action_new for 2016-05-01 - 2016-05-31
2017-04-28 19:06:19.847060: User_action successfully updated. Dates: 2016-05-01---2016-05-31

****************************************
2017-04-28 18:39:22.743457: Update of user_action started.
Calc user_action_new for 2015-11-01 - 2015-11-31
2017-04-28 19:07:18.982908: User_action successfully updated. Dates: 2015-11-01---2015-11-31

****************************************
2017-04-28 18:38:56.030962: Update of user_action started.
Calc user_action_new for 2016-03-01 - 2016-03-31
2017-04-28 19:07:20.149732: User_action successfully updated. Dates: 2016-03-01---2016-03-31

****************************************
2017-04-28 18:38:48.477498: Update of user_action started.
Calc user_action_new for 2016-04-01 - 2016-04-31
2017-04-28 19:07:46.193481: User_action successfully updated. Dates: 2016-04-01---2016-04-31

****************************************
2017-04-28 18:39:09.790247: Update of user_action started.
Calc user_action_new for 2016-01-01 - 2016-01-31
2017-04-28 19:08:39.764912: User_action successfully updated. Dates: 2016-01-01---2016-01-31

****************************************
2017-04-28 18:39:02.756593: Update of user_action started.
Calc user_action_new for 2016-02-01 - 2016-02-31
2017-04-28 19:08:40.353712: User_action successfully updated. Dates: 2016-02-01---2016-02-31

****************************************
2017-04-28 18:39:16.095756: Update of user_action started.
Calc user_action_new for 2015-12-01 - 2015-12-31
2017-04-28 19:08:41.202929: User_action successfully updated. Dates: 2015-12-01---2015-12-31
