{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple q-learning agent with experience replay\n",
    "\n",
    "We re-write q-learning algorithm using _agentnet_ - a helper for lasagne that implements some RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "* Here we simply load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-18 22:24:12,185] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7598e459b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "dense_1 = DenseLayer(observation_layer,num_units=128,\n",
    "                                   nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                   name = \"hidden_dense_layer_1\")\n",
    "dense_2 = DenseLayer(dense_1,num_units=256,\n",
    "                                   nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                   name = \"hidden_dense_layer_1\")\n",
    "dense_3 = DenseLayer(dense_2,num_units=128,\n",
    "                                   nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                   name = \"hidden_dense_layer_1\")\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(dense_3,num_units=n_actions,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hidden_dense_layer_1.W,\n",
       " hidden_dense_layer_1.b,\n",
       " hidden_dense_layer_1.W,\n",
       " hidden_dense_layer_1.b,\n",
       " hidden_dense_layer_1.W,\n",
       " hidden_dense_layer_1.b,\n",
       " q-values.W,\n",
       " q-values.b]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-18 22:47:26,679] Making new env: Acrobot-v1\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "[2017-09-18 22:47:34,183] install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: [[1 1 2 1 0]]\n",
      "rewards: [[-1. -1. -1. -1.  0.]]\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-4)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "target_score = -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "for i in trange(10000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        rewards[epoch_counter] = np.mean(pool.evaluate(n_games=3,record_video=False))\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(3,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework part I (5+ pts)\n",
    "\n",
    "Train a neural network for [`LunarLander-v2`](https://gym.openai.com/envs/LunarLander-v2).\n",
    "* Getting average reward of at least +0 gets you 5 points\n",
    "* Higher reward = more points\n",
    "\n",
    "\n",
    "## Bonus I\n",
    "* Try getting the same [or better] results on Acrobot __(+2 pts)__ or __LunarLander (+3 pts)__ using on-policy methods\n",
    "* You can get n-step q-learning by messing with ```n_steps``` param in the q-learning code above\n",
    "* Note that using large experience replay buffer will slow down on-policy algorithms to almost zero, so it's probably a good idea to use small experience replay buffer with several parallel agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
